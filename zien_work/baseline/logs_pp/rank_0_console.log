[2025-12-23 14:38:17,340] m-LoRA: NVIDIA CUDA initialized successfully.
[2025-12-23 14:38:17,340] m-LoRA: Total 8 GPU(s) detected.
[2025-12-23 14:38:17,561] m-LoRA: Pipeline parallelism, rank is 0 and balance is [8, 9, 9, 9].
[2025-12-23 14:38:17,563] m-LoRA: Loading model with precision - bf16
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.46s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.74it/s]
[2025-12-23 14:38:19,197] m-LoRA: Some parameters are on the meta device because they were offloaded to the disk.
[2025-12-23 14:38:19,198] m-LoRA: loading llama compatible model - llama
[2025-12-23 14:38:19,343] m-LoRA: Adapter lora_gsm8k_0 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_1 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_2 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_3 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_4 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_5 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_6 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_7 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_8 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_9 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_10 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_11 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_12 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_13 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_14 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_15 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_16 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_17 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_18 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_19 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_20 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_21 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_22 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_23 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_24 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_25 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_26 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_27 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_28 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_29 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_30 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_31 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_32 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_33 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_34 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_35 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_36 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_37 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_38 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_39 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_40 without lr_scheduler.
[2025-12-23 14:38:19,344] m-LoRA: Adapter lora_gsm8k_41 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_42 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_43 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_44 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_45 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_46 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_47 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_48 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_49 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_50 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_51 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_52 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_53 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_54 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_55 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_56 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_57 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_58 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_59 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_60 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_61 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_62 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_63 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_64 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_65 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_66 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_67 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_68 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_69 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_70 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_71 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_72 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_73 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_74 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_75 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_76 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_77 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_78 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_79 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_80 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_81 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_82 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_83 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_84 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_85 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_86 without lr_scheduler.
[2025-12-23 14:38:19,345] m-LoRA: Adapter lora_gsm8k_87 without lr_scheduler.
[2025-12-23 14:38:19,346] m-LoRA: Adapter lora_gsm8k_88 without lr_scheduler.
[2025-12-23 14:38:19,346] m-LoRA: Adapter lora_gsm8k_89 without lr_scheduler.
[2025-12-23 14:38:19,352] m-LoRA: DeviceSwapQueue - input_data_queue start.
[2025-12-23 14:38:19,352] m-LoRA: RANK-0 in device cuda:0 to load module layers from 0 to 8.
[2025-12-23 14:38:19,355] m-LoRA: DeviceSwapQueue - ACTIVATIONS_send start.
[2025-12-23 14:38:19,355] m-LoRA: DeviceSwapQueue - GRADIENTS_send start.
[2025-12-23 14:38:19,355] m-LoRA: DeviceSwapQueue - ACTIVATIONS_recv start.
[2025-12-23 14:38:19,356] m-LoRA: DeviceSwapQueue - GRADIENTS_recv start.
/scr/dataset/yuke/zien/mlora_env/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:653: UserWarning: You are using a Backend <class 'torch.distributed.distributed_c10d.ProcessGroupGloo'> as a ProcessGroup. This usage is deprecated since PyTorch 2.0. Please use a public API of PyTorch Distributed instead.
  warnings.warn(
[2025-12-23 14:38:23,153] m-LoRA: Init rpc with rank 0 world_size: 4
[2025-12-23 14:38:23,153] m-LoRA: Init train : task_gsm8k_0 task with adapters: ['lora_gsm8k_0']
[2025-12-23 14:38:23,851] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:38:24,136] m-LoRA: Adapter lora_gsm8k_0 data size: 128
DEBUG: Real Model Length is 35
0it [00:00, ?it/s]128it [00:00, 5071.81it/s]
[2025-12-23 14:38:24,170] m-LoRA: Init train : task_gsm8k_1 task with adapters: ['lora_gsm8k_1']
[2025-12-23 14:38:24,653] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:38:24,907] m-LoRA: Adapter lora_gsm8k_1 data size: 128
0it [00:00, ?it/s]128it [00:00, 30196.91it/s]
[2025-12-23 14:38:24,912] m-LoRA: Init train : task_gsm8k_2 task with adapters: ['lora_gsm8k_2']
[2025-12-23 14:38:25,015] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:38:25,280] m-LoRA: Adapter lora_gsm8k_2 data size: 128
0it [00:00, ?it/s]128it [00:00, 5002.24it/s]
[2025-12-23 14:38:25,307] m-LoRA: Init train : task_gsm8k_3 task with adapters: ['lora_gsm8k_3']
[2025-12-23 14:38:26,477] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:38:26,737] m-LoRA: Adapter lora_gsm8k_3 data size: 128
0it [00:00, ?it/s]128it [00:00, 6110.67it/s]
[2025-12-23 14:38:26,759] m-LoRA: Init train : task_gsm8k_4 task with adapters: ['lora_gsm8k_4']
[2025-12-23 14:38:28,402] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:38:28,665] m-LoRA: Adapter lora_gsm8k_4 data size: 128
0it [00:00, ?it/s]128it [00:00, 13502.45it/s]
[2025-12-23 14:38:28,675] m-LoRA: Init train : task_gsm8k_5 task with adapters: ['lora_gsm8k_5']
[2025-12-23 14:38:30,134] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:38:30,392] m-LoRA: Adapter lora_gsm8k_5 data size: 128
0it [00:00, ?it/s]128it [00:00, 26288.85it/s]
[2025-12-23 14:38:30,397] m-LoRA: Init train : task_gsm8k_6 task with adapters: ['lora_gsm8k_6']
[2025-12-23 14:38:31,920] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:38:32,187] m-LoRA: Adapter lora_gsm8k_6 data size: 128
0it [00:00, ?it/s]128it [00:00, 28874.89it/s]
[2025-12-23 14:38:32,192] m-LoRA: Init train : task_gsm8k_7 task with adapters: ['lora_gsm8k_7']
[2025-12-23 14:38:32,407] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:38:32,665] m-LoRA: Adapter lora_gsm8k_7 data size: 128
0it [00:00, ?it/s]128it [00:00, 28134.94it/s]
[2025-12-23 14:38:32,670] m-LoRA: Init train : task_gsm8k_8 task with adapters: ['lora_gsm8k_8']
[2025-12-23 14:38:33,846] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:38:34,126] m-LoRA: Adapter lora_gsm8k_8 data size: 128
0it [00:00, ?it/s]128it [00:00, 3128.20it/s]
[2025-12-23 14:38:34,168] m-LoRA: Init train : task_gsm8k_9 task with adapters: ['lora_gsm8k_9']
[2025-12-23 14:38:34,850] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:38:35,106] m-LoRA: Adapter lora_gsm8k_9 data size: 128
0it [00:00, ?it/s]128it [00:00, 33691.30it/s]
[2025-12-23 14:38:35,110] m-LoRA: Init train : task_gsm8k_10 task with adapters: ['lora_gsm8k_10']
[2025-12-23 14:38:37,069] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:38:37,336] m-LoRA: Adapter lora_gsm8k_10 data size: 128
0it [00:00, ?it/s]128it [00:00, 16984.21it/s]
[2025-12-23 14:38:37,344] m-LoRA: Init train : task_gsm8k_11 task with adapters: ['lora_gsm8k_11']
[2025-12-23 14:38:38,384] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:38:38,651] m-LoRA: Adapter lora_gsm8k_11 data size: 128
0it [00:00, ?it/s]128it [00:00, 23845.03it/s]
[2025-12-23 14:38:38,657] m-LoRA: Init train : task_gsm8k_12 task with adapters: ['lora_gsm8k_12']
[2025-12-23 14:38:39,961] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:38:40,230] m-LoRA: Adapter lora_gsm8k_12 data size: 128
0it [00:00, ?it/s]128it [00:00, 26126.38it/s]
[2025-12-23 14:38:40,235] m-LoRA: Init train : task_gsm8k_13 task with adapters: ['lora_gsm8k_13']
[2025-12-23 14:38:40,347] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:38:40,599] m-LoRA: Adapter lora_gsm8k_13 data size: 128
0it [00:00, ?it/s]128it [00:00, 28783.56it/s]
[2025-12-23 14:38:40,604] m-LoRA: Init train : task_gsm8k_14 task with adapters: ['lora_gsm8k_14']
[2025-12-23 14:38:42,396] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:38:42,662] m-LoRA: Adapter lora_gsm8k_14 data size: 128
0it [00:00, ?it/s]128it [00:00, 27831.57it/s]
[2025-12-23 14:38:42,667] m-LoRA: Init train : task_gsm8k_15 task with adapters: ['lora_gsm8k_15']
[2025-12-23 14:38:43,005] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:38:43,263] m-LoRA: Adapter lora_gsm8k_15 data size: 128
0it [00:00, ?it/s]128it [00:00, 24929.00it/s]
[2025-12-23 14:38:43,269] m-LoRA: Init train : task_gsm8k_16 task with adapters: ['lora_gsm8k_16']
[2025-12-23 14:38:44,882] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:38:45,139] m-LoRA: Adapter lora_gsm8k_16 data size: 128
0it [00:00, ?it/s]128it [00:00, 28327.93it/s]
[2025-12-23 14:38:45,144] m-LoRA: Init train : task_gsm8k_17 task with adapters: ['lora_gsm8k_17']
[2025-12-23 14:38:45,222] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:38:45,497] m-LoRA: Adapter lora_gsm8k_17 data size: 128
0it [00:00, ?it/s]128it [00:00, 26843.55it/s]
[2025-12-23 14:38:45,502] m-LoRA: Init train : task_gsm8k_18 task with adapters: ['lora_gsm8k_18']
[2025-12-23 14:38:46,561] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:38:46,832] m-LoRA: Adapter lora_gsm8k_18 data size: 128
0it [00:00, ?it/s]128it [00:00, 24824.10it/s]
[2025-12-23 14:38:46,838] m-LoRA: Init train : task_gsm8k_19 task with adapters: ['lora_gsm8k_19']
[2025-12-23 14:38:48,163] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:38:48,424] m-LoRA: Adapter lora_gsm8k_19 data size: 128
0it [00:00, ?it/s]128it [00:00, 35068.97it/s]
[2025-12-23 14:38:48,429] m-LoRA: Init train : task_gsm8k_20 task with adapters: ['lora_gsm8k_20']
[2025-12-23 14:38:49,995] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:38:50,255] m-LoRA: Adapter lora_gsm8k_20 data size: 128
0it [00:00, ?it/s]128it [00:00, 24329.13it/s]
[2025-12-23 14:38:50,261] m-LoRA: Init train : task_gsm8k_21 task with adapters: ['lora_gsm8k_21']
[2025-12-23 14:38:50,611] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:38:50,879] m-LoRA: Adapter lora_gsm8k_21 data size: 128
0it [00:00, ?it/s]128it [00:00, 24970.74it/s]
[2025-12-23 14:38:50,885] m-LoRA: Init train : task_gsm8k_22 task with adapters: ['lora_gsm8k_22']
[2025-12-23 14:38:52,316] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:38:52,594] m-LoRA: Adapter lora_gsm8k_22 data size: 128
0it [00:00, ?it/s]128it [00:00, 14192.05it/s]
[2025-12-23 14:38:52,604] m-LoRA: Init train : task_gsm8k_23 task with adapters: ['lora_gsm8k_23']
[2025-12-23 14:38:54,515] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:38:54,762] m-LoRA: Adapter lora_gsm8k_23 data size: 128
0it [00:00, ?it/s]128it [00:00, 14741.10it/s]
[2025-12-23 14:38:54,771] m-LoRA: Init train : task_gsm8k_24 task with adapters: ['lora_gsm8k_24']
[2025-12-23 14:38:56,434] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:38:56,688] m-LoRA: Adapter lora_gsm8k_24 data size: 128
0it [00:00, ?it/s]128it [00:00, 25993.56it/s]
[2025-12-23 14:38:56,694] m-LoRA: Init train : task_gsm8k_25 task with adapters: ['lora_gsm8k_25']
[2025-12-23 14:38:58,596] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:38:58,855] m-LoRA: Adapter lora_gsm8k_25 data size: 128
0it [00:00, ?it/s]128it [00:00, 26379.27it/s]
[2025-12-23 14:38:58,860] m-LoRA: Init train : task_gsm8k_26 task with adapters: ['lora_gsm8k_26']
[2025-12-23 14:39:00,505] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:00,773] m-LoRA: Adapter lora_gsm8k_26 data size: 128
0it [00:00, ?it/s]128it [00:00, 25352.80it/s]
[2025-12-23 14:39:00,779] m-LoRA: Init train : task_gsm8k_27 task with adapters: ['lora_gsm8k_27']
[2025-12-23 14:39:02,473] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:02,719] m-LoRA: Adapter lora_gsm8k_27 data size: 128
0it [00:00, ?it/s]128it [00:00, 17495.63it/s]
[2025-12-23 14:39:02,727] m-LoRA: Init train : task_gsm8k_28 task with adapters: ['lora_gsm8k_28']
[2025-12-23 14:39:02,809] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:03,067] m-LoRA: Adapter lora_gsm8k_28 data size: 128
0it [00:00, ?it/s]128it [00:00, 25130.88it/s]
[2025-12-23 14:39:03,073] m-LoRA: Init train : task_gsm8k_29 task with adapters: ['lora_gsm8k_29']
[2025-12-23 14:39:03,928] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:04,184] m-LoRA: Adapter lora_gsm8k_29 data size: 128
0it [00:00, ?it/s]128it [00:00, 23115.08it/s]
[2025-12-23 14:39:04,190] m-LoRA: Init train : task_gsm8k_30 task with adapters: ['lora_gsm8k_30']
[2025-12-23 14:39:05,776] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:06,045] m-LoRA: Adapter lora_gsm8k_30 data size: 128
0it [00:00, ?it/s]128it [00:00, 26793.98it/s]
[2025-12-23 14:39:06,051] m-LoRA: Init train : task_gsm8k_31 task with adapters: ['lora_gsm8k_31']
[2025-12-23 14:39:07,469] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:07,734] m-LoRA: Adapter lora_gsm8k_31 data size: 128
0it [00:00, ?it/s]128it [00:00, 14660.59it/s]
[2025-12-23 14:39:07,744] m-LoRA: Init train : task_gsm8k_32 task with adapters: ['lora_gsm8k_32']
[2025-12-23 14:39:09,679] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:09,938] m-LoRA: Adapter lora_gsm8k_32 data size: 128
0it [00:00, ?it/s]128it [00:00, 32640.50it/s]
[2025-12-23 14:39:09,942] m-LoRA: Init train : task_gsm8k_33 task with adapters: ['lora_gsm8k_33']
[2025-12-23 14:39:11,822] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:12,083] m-LoRA: Adapter lora_gsm8k_33 data size: 128
0it [00:00, ?it/s]128it [00:00, 34113.03it/s]
[2025-12-23 14:39:12,087] m-LoRA: Init train : task_gsm8k_34 task with adapters: ['lora_gsm8k_34']
[2025-12-23 14:39:13,835] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:14,103] m-LoRA: Adapter lora_gsm8k_34 data size: 128
0it [00:00, ?it/s]128it [00:00, 26463.79it/s]
[2025-12-23 14:39:14,108] m-LoRA: Init train : task_gsm8k_35 task with adapters: ['lora_gsm8k_35']
[2025-12-23 14:39:15,180] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:15,452] m-LoRA: Adapter lora_gsm8k_35 data size: 128
0it [00:00, ?it/s]128it [00:00, 28732.72it/s]
[2025-12-23 14:39:15,457] m-LoRA: Init train : task_gsm8k_36 task with adapters: ['lora_gsm8k_36']
[2025-12-23 14:39:16,182] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:16,450] m-LoRA: Adapter lora_gsm8k_36 data size: 128
0it [00:00, ?it/s]128it [00:00, 27966.40it/s]
[2025-12-23 14:39:16,456] m-LoRA: Init train : task_gsm8k_37 task with adapters: ['lora_gsm8k_37']
[2025-12-23 14:39:18,068] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:18,325] m-LoRA: Adapter lora_gsm8k_37 data size: 128
0it [00:00, ?it/s]128it [00:00, 29231.78it/s]
[2025-12-23 14:39:18,330] m-LoRA: Init train : task_gsm8k_38 task with adapters: ['lora_gsm8k_38']
[2025-12-23 14:39:18,421] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:18,667] m-LoRA: Adapter lora_gsm8k_38 data size: 128
0it [00:00, ?it/s]128it [00:00, 31375.78it/s]
[2025-12-23 14:39:18,671] m-LoRA: Init train : task_gsm8k_39 task with adapters: ['lora_gsm8k_39']
[2025-12-23 14:39:20,649] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:20,899] m-LoRA: Adapter lora_gsm8k_39 data size: 128
0it [00:00, ?it/s]128it [00:00, 30872.39it/s]
[2025-12-23 14:39:20,904] m-LoRA: Init train : task_gsm8k_40 task with adapters: ['lora_gsm8k_40']
[2025-12-23 14:39:22,370] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:22,640] m-LoRA: Adapter lora_gsm8k_40 data size: 128
0it [00:00, ?it/s]128it [00:00, 28765.05it/s]
[2025-12-23 14:39:22,645] m-LoRA: Init train : task_gsm8k_41 task with adapters: ['lora_gsm8k_41']
[2025-12-23 14:39:23,185] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:23,455] m-LoRA: Adapter lora_gsm8k_41 data size: 128
0it [00:00, ?it/s]128it [00:00, 12622.75it/s]
[2025-12-23 14:39:23,466] m-LoRA: Init train : task_gsm8k_42 task with adapters: ['lora_gsm8k_42']
[2025-12-23 14:39:24,735] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:25,022] m-LoRA: Adapter lora_gsm8k_42 data size: 128
0it [00:00, ?it/s]128it [00:00, 27828.68it/s]
[2025-12-23 14:39:25,027] m-LoRA: Init train : task_gsm8k_43 task with adapters: ['lora_gsm8k_43']
[2025-12-23 14:39:26,592] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:26,835] m-LoRA: Adapter lora_gsm8k_43 data size: 128
0it [00:00, ?it/s]128it [00:00, 28405.87it/s]
[2025-12-23 14:39:26,840] m-LoRA: Init train : task_gsm8k_44 task with adapters: ['lora_gsm8k_44']
[2025-12-23 14:39:28,628] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:28,893] m-LoRA: Adapter lora_gsm8k_44 data size: 128
0it [00:00, ?it/s]128it [00:00, 27905.34it/s]
[2025-12-23 14:39:28,898] m-LoRA: Init train : task_gsm8k_45 task with adapters: ['lora_gsm8k_45']
[2025-12-23 14:39:30,716] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:30,982] m-LoRA: Adapter lora_gsm8k_45 data size: 128
0it [00:00, ?it/s]128it [00:00, 26680.79it/s]
[2025-12-23 14:39:30,987] m-LoRA: Init train : task_gsm8k_46 task with adapters: ['lora_gsm8k_46']
[2025-12-23 14:39:32,641] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:32,902] m-LoRA: Adapter lora_gsm8k_46 data size: 128
0it [00:00, ?it/s]128it [00:00, 28685.13it/s]
[2025-12-23 14:39:32,907] m-LoRA: Init train : task_gsm8k_47 task with adapters: ['lora_gsm8k_47']
[2025-12-23 14:39:34,697] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:34,969] m-LoRA: Adapter lora_gsm8k_47 data size: 128
0it [00:00, ?it/s]128it [00:00, 26869.07it/s]
[2025-12-23 14:39:34,974] m-LoRA: Init train : task_gsm8k_48 task with adapters: ['lora_gsm8k_48']
[2025-12-23 14:39:36,627] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:36,891] m-LoRA: Adapter lora_gsm8k_48 data size: 128
0it [00:00, ?it/s]128it [00:00, 28946.51it/s]
[2025-12-23 14:39:36,896] m-LoRA: Init train : task_gsm8k_49 task with adapters: ['lora_gsm8k_49']
[2025-12-23 14:39:37,995] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:38,258] m-LoRA: Adapter lora_gsm8k_49 data size: 128
0it [00:00, ?it/s]128it [00:00, 20815.40it/s]
[2025-12-23 14:39:38,264] m-LoRA: Init train : task_gsm8k_50 task with adapters: ['lora_gsm8k_50']
[2025-12-23 14:39:39,601] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:39,861] m-LoRA: Adapter lora_gsm8k_50 data size: 128
0it [00:00, ?it/s]128it [00:00, 13791.03it/s]
[2025-12-23 14:39:39,871] m-LoRA: Init train : task_gsm8k_51 task with adapters: ['lora_gsm8k_51']
[2025-12-23 14:39:41,261] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:41,542] m-LoRA: Adapter lora_gsm8k_51 data size: 128
0it [00:00, ?it/s]128it [00:00, 29415.97it/s]
[2025-12-23 14:39:41,547] m-LoRA: Init train : task_gsm8k_52 task with adapters: ['lora_gsm8k_52']
[2025-12-23 14:39:42,673] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:42,940] m-LoRA: Adapter lora_gsm8k_52 data size: 128
0it [00:00, ?it/s]128it [00:00, 31166.31it/s]
[2025-12-23 14:39:42,945] m-LoRA: Init train : task_gsm8k_53 task with adapters: ['lora_gsm8k_53']
[2025-12-23 14:39:43,816] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:44,107] m-LoRA: Adapter lora_gsm8k_53 data size: 128
0it [00:00, ?it/s]128it [00:00, 25062.83it/s]
[2025-12-23 14:39:44,113] m-LoRA: Init train : task_gsm8k_54 task with adapters: ['lora_gsm8k_54']
[2025-12-23 14:39:44,822] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:45,088] m-LoRA: Adapter lora_gsm8k_54 data size: 128
0it [00:00, ?it/s]128it [00:00, 28932.47it/s]
[2025-12-23 14:39:45,093] m-LoRA: Init train : task_gsm8k_55 task with adapters: ['lora_gsm8k_55']
[2025-12-23 14:39:46,548] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:46,834] m-LoRA: Adapter lora_gsm8k_55 data size: 128
0it [00:00, ?it/s]128it [00:00, 27587.02it/s]
[2025-12-23 14:39:46,840] m-LoRA: Init train : task_gsm8k_56 task with adapters: ['lora_gsm8k_56']
[2025-12-23 14:39:48,689] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:48,967] m-LoRA: Adapter lora_gsm8k_56 data size: 128
0it [00:00, ?it/s]128it [00:00, 26140.37it/s]
[2025-12-23 14:39:48,972] m-LoRA: Init train : task_gsm8k_57 task with adapters: ['lora_gsm8k_57']
[2025-12-23 14:39:50,746] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:51,007] m-LoRA: Adapter lora_gsm8k_57 data size: 128
0it [00:00, ?it/s]128it [00:00, 16348.08it/s]
[2025-12-23 14:39:51,015] m-LoRA: Init train : task_gsm8k_58 task with adapters: ['lora_gsm8k_58']
[2025-12-23 14:39:51,881] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:52,153] m-LoRA: Adapter lora_gsm8k_58 data size: 128
0it [00:00, ?it/s]128it [00:00, 28030.64it/s]
[2025-12-23 14:39:52,158] m-LoRA: Init train : task_gsm8k_59 task with adapters: ['lora_gsm8k_59']
[2025-12-23 14:39:52,851] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:53,104] m-LoRA: Adapter lora_gsm8k_59 data size: 128
0it [00:00, ?it/s]128it [00:00, 27132.51it/s]
[2025-12-23 14:39:53,109] m-LoRA: Init train : task_gsm8k_60 task with adapters: ['lora_gsm8k_60']
[2025-12-23 14:39:54,686] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:54,956] m-LoRA: Adapter lora_gsm8k_60 data size: 128
0it [00:00, ?it/s]128it [00:00, 31692.50it/s]
[2025-12-23 14:39:54,960] m-LoRA: Init train : task_gsm8k_61 task with adapters: ['lora_gsm8k_61']
[2025-12-23 14:39:55,031] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:55,280] m-LoRA: Adapter lora_gsm8k_61 data size: 128
0it [00:00, ?it/s]128it [00:00, 29128.69it/s]
[2025-12-23 14:39:55,285] m-LoRA: Init train : task_gsm8k_62 task with adapters: ['lora_gsm8k_62']
[2025-12-23 14:39:56,690] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:56,987] m-LoRA: Adapter lora_gsm8k_62 data size: 128
0it [00:00, ?it/s]128it [00:00, 7139.91it/s]
[2025-12-23 14:39:57,006] m-LoRA: Init train : task_gsm8k_63 task with adapters: ['lora_gsm8k_63']
[2025-12-23 14:39:58,638] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:39:58,909] m-LoRA: Adapter lora_gsm8k_63 data size: 128
0it [00:00, ?it/s]128it [00:00, 26931.07it/s]
[2025-12-23 14:39:58,915] m-LoRA: Init train : task_gsm8k_64 task with adapters: ['lora_gsm8k_64']
[2025-12-23 14:40:00,530] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:40:00,803] m-LoRA: Adapter lora_gsm8k_64 data size: 128
0it [00:00, ?it/s]128it [00:00, 28235.56it/s]
[2025-12-23 14:40:00,808] m-LoRA: Init train : task_gsm8k_65 task with adapters: ['lora_gsm8k_65']
[2025-12-23 14:40:02,304] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:40:02,561] m-LoRA: Adapter lora_gsm8k_65 data size: 128
0it [00:00, ?it/s]128it [00:00, 34809.76it/s]
[2025-12-23 14:40:02,565] m-LoRA: Init train : task_gsm8k_66 task with adapters: ['lora_gsm8k_66']
[2025-12-23 14:40:03,813] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:40:04,084] m-LoRA: Adapter lora_gsm8k_66 data size: 128
0it [00:00, ?it/s]128it [00:00, 12309.88it/s]
[2025-12-23 14:40:04,095] m-LoRA: Init train : task_gsm8k_67 task with adapters: ['lora_gsm8k_67']
[2025-12-23 14:40:04,610] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:40:04,883] m-LoRA: Adapter lora_gsm8k_67 data size: 128
0it [00:00, ?it/s]128it [00:00, 27578.51it/s]
[2025-12-23 14:40:04,888] m-LoRA: Init train : task_gsm8k_68 task with adapters: ['lora_gsm8k_68']
[2025-12-23 14:40:06,534] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:40:06,787] m-LoRA: Adapter lora_gsm8k_68 data size: 128
0it [00:00, ?it/s]128it [00:00, 21859.56it/s]
[2025-12-23 14:40:06,794] m-LoRA: Init train : task_gsm8k_69 task with adapters: ['lora_gsm8k_69']
[2025-12-23 14:40:08,601] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:40:08,874] m-LoRA: Adapter lora_gsm8k_69 data size: 128
0it [00:00, ?it/s]128it [00:00, 26588.30it/s]
[2025-12-23 14:40:08,880] m-LoRA: Init train : task_gsm8k_70 task with adapters: ['lora_gsm8k_70']
[2025-12-23 14:40:09,912] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:40:10,179] m-LoRA: Adapter lora_gsm8k_70 data size: 128
0it [00:00, ?it/s]128it [00:00, 28049.68it/s]
[2025-12-23 14:40:10,184] m-LoRA: Init train : task_gsm8k_71 task with adapters: ['lora_gsm8k_71']
[2025-12-23 14:40:11,937] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:40:12,194] m-LoRA: Adapter lora_gsm8k_71 data size: 128
0it [00:00, ?it/s]128it [00:00, 27148.97it/s]
[2025-12-23 14:40:12,199] m-LoRA: Init train : task_gsm8k_72 task with adapters: ['lora_gsm8k_72']
[2025-12-23 14:40:13,966] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:40:14,222] m-LoRA: Adapter lora_gsm8k_72 data size: 128
0it [00:00, ?it/s]128it [00:00, 32880.38it/s]
[2025-12-23 14:40:14,226] m-LoRA: Init train : task_gsm8k_73 task with adapters: ['lora_gsm8k_73']
[2025-12-23 14:40:14,391] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:40:14,649] m-LoRA: Adapter lora_gsm8k_73 data size: 128
0it [00:00, ?it/s]128it [00:00, 34324.59it/s]
[2025-12-23 14:40:14,653] m-LoRA: Init train : task_gsm8k_74 task with adapters: ['lora_gsm8k_74']
[2025-12-23 14:40:16,426] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:40:16,686] m-LoRA: Adapter lora_gsm8k_74 data size: 128
0it [00:00, ?it/s]128it [00:00, 26893.30it/s]
[2025-12-23 14:40:16,692] m-LoRA: Init train : task_gsm8k_75 task with adapters: ['lora_gsm8k_75']
[2025-12-23 14:40:16,770] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:40:17,041] m-LoRA: Adapter lora_gsm8k_75 data size: 128
0it [00:00, ?it/s]128it [00:00, 4498.03it/s]
[2025-12-23 14:40:17,070] m-LoRA: Init train : task_gsm8k_76 task with adapters: ['lora_gsm8k_76']
[2025-12-23 14:40:17,833] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:40:18,110] m-LoRA: Adapter lora_gsm8k_76 data size: 128
0it [00:00, ?it/s]128it [00:00, 23973.87it/s]
[2025-12-23 14:40:18,116] m-LoRA: Init train : task_gsm8k_77 task with adapters: ['lora_gsm8k_77']
[2025-12-23 14:40:19,563] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:40:19,811] m-LoRA: Adapter lora_gsm8k_77 data size: 128
0it [00:00, ?it/s]128it [00:00, 13329.47it/s]
[2025-12-23 14:40:19,821] m-LoRA: Init train : task_gsm8k_78 task with adapters: ['lora_gsm8k_78']
[2025-12-23 14:40:21,713] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:40:21,992] m-LoRA: Adapter lora_gsm8k_78 data size: 128
0it [00:00, ?it/s]128it [00:00, 25181.56it/s]
[2025-12-23 14:40:21,998] m-LoRA: Init train : task_gsm8k_79 task with adapters: ['lora_gsm8k_79']
[2025-12-23 14:40:23,587] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:40:23,853] m-LoRA: Adapter lora_gsm8k_79 data size: 128
0it [00:00, ?it/s]128it [00:00, 12685.09it/s]
[2025-12-23 14:40:23,864] m-LoRA: Init train : task_gsm8k_80 task with adapters: ['lora_gsm8k_80']
[2025-12-23 14:40:25,651] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:40:25,913] m-LoRA: Adapter lora_gsm8k_80 data size: 128
0it [00:00, ?it/s]128it [00:00, 33368.82it/s]
[2025-12-23 14:40:25,918] m-LoRA: Init train : task_gsm8k_81 task with adapters: ['lora_gsm8k_81']
[2025-12-23 14:40:27,053] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:40:27,332] m-LoRA: Adapter lora_gsm8k_81 data size: 128
0it [00:00, ?it/s]128it [00:00, 20012.34it/s]
[2025-12-23 14:40:27,339] m-LoRA: Init train : task_gsm8k_82 task with adapters: ['lora_gsm8k_82']
[2025-12-23 14:40:27,651] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:40:27,895] m-LoRA: Adapter lora_gsm8k_82 data size: 128
0it [00:00, ?it/s]128it [00:00, 29448.24it/s]
[2025-12-23 14:40:27,900] m-LoRA: Init train : task_gsm8k_83 task with adapters: ['lora_gsm8k_83']
[2025-12-23 14:40:29,920] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:40:30,174] m-LoRA: Adapter lora_gsm8k_83 data size: 128
0it [00:00, ?it/s]128it [00:00, 26787.29it/s]
[2025-12-23 14:40:30,179] m-LoRA: Init train : task_gsm8k_84 task with adapters: ['lora_gsm8k_84']
[2025-12-23 14:40:31,673] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:40:31,954] m-LoRA: Adapter lora_gsm8k_84 data size: 128
0it [00:00, ?it/s]128it [00:00, 27434.76it/s]
[2025-12-23 14:40:31,959] m-LoRA: Init train : task_gsm8k_85 task with adapters: ['lora_gsm8k_85']
[2025-12-23 14:40:32,376] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:40:32,673] m-LoRA: Adapter lora_gsm8k_85 data size: 128
0it [00:00, ?it/s]128it [00:00, 27937.29it/s]
[2025-12-23 14:40:32,678] m-LoRA: Init train : task_gsm8k_86 task with adapters: ['lora_gsm8k_86']
[2025-12-23 14:40:33,867] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:40:34,151] m-LoRA: Adapter lora_gsm8k_86 data size: 128
0it [00:00, ?it/s]128it [00:00, 27502.22it/s]
[2025-12-23 14:40:34,156] m-LoRA: Init train : task_gsm8k_87 task with adapters: ['lora_gsm8k_87']
[2025-12-23 14:40:35,886] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:40:36,157] m-LoRA: Adapter lora_gsm8k_87 data size: 128
0it [00:00, ?it/s]128it [00:00, 27835.90it/s]
[2025-12-23 14:40:36,163] m-LoRA: Init train : task_gsm8k_88 task with adapters: ['lora_gsm8k_88']
[2025-12-23 14:40:37,727] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:40:38,002] m-LoRA: Adapter lora_gsm8k_88 data size: 128
0it [00:00, ?it/s]128it [00:00, 36302.04it/s]
[2025-12-23 14:40:38,006] m-LoRA: Init train : task_gsm8k_89 task with adapters: ['lora_gsm8k_89']
[2025-12-23 14:40:39,860] m-LoRA: Task load data from /scr/dataset/yuke/zien/mLoRA/zien_work/baseline/data/gsm8k_train_subset_128.json
[2025-12-23 14:40:40,114] m-LoRA: Adapter lora_gsm8k_89 data size: 128
0it [00:00, ?it/s]128it [00:00, 28550.89it/s]
[2025-12-23 14:40:40,119] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_0']
[2025-12-23 14:40:40,170] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_1']
[2025-12-23 14:40:40,199] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_2']
[2025-12-23 14:40:40,254] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_3']
[2025-12-23 14:40:40,292] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_4']
[2025-12-23 14:40:40,376] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:40:40,697] m-LoRA: Adapter lora_gsm8k_1 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:40:40,723] m-LoRA: Adapter lora_gsm8k_2 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:40:40,863] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:40:40,892] m-LoRA: Adapter lora_gsm8k_4 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:40:42,764] m-LoRA: Adapter lora_gsm8k_0 loss: 2.1022021770477295
[2025-12-23 14:40:42,944] m-LoRA: Adapter lora_gsm8k_3 loss: 2.5428662300109863
[2025-12-23 14:40:45,239] m-LoRA: Adapter lora_gsm8k_2 loss: 2.144622802734375
[2025-12-23 14:40:45,254] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 2/128 step: 2
[2025-12-23 14:40:45,355] m-LoRA: Adapter lora_gsm8k_1 loss: 2.519087791442871
[2025-12-23 14:40:45,358] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 2/128 step: 2
[2025-12-23 14:40:46,702] m-LoRA: Adapter lora_gsm8k_4 loss: 2.501193046569824
[2025-12-23 14:40:46,715] m-LoRA: Adapter lora_gsm8k_0 loss: 1.6737902164459229
[2025-12-23 14:40:46,953] m-LoRA: Adapter lora_gsm8k_3 loss: 1.7677913904190063
[2025-12-23 14:40:54,655] m-LoRA: Adapter lora_gsm8k_2 epoch: 1/1 iteration: 16/128 step: 2
[2025-12-23 14:40:54,863] m-LoRA: Adapter lora_gsm8k_1 epoch: 1/1 iteration: 4/128 step: 2
[2025-12-23 14:40:55,026] m-LoRA: Adapter lora_gsm8k_4 epoch: 1/1 iteration: 16/128 step: 2
[2025-12-23 14:40:55,234] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 4/128 step: 3
[2025-12-23 14:40:55,347] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 4/128 step: 3
[2025-12-23 14:40:56,975] m-LoRA: Adapter lora_gsm8k_2 loss: 2.395848035812378
[2025-12-23 14:40:57,227] m-LoRA: Adapter lora_gsm8k_1 loss: 1.662606120109558
[2025-12-23 14:40:58,960] m-LoRA: Adapter lora_gsm8k_4 loss: 1.1594384908676147
[2025-12-23 14:40:59,035] m-LoRA: Adapter lora_gsm8k_0 loss: 1.6020957231521606
[2025-12-23 14:40:59,268] m-LoRA: Adapter lora_gsm8k_3 loss: 1.6658047437667847
[2025-12-23 14:40:59,686] m-LoRA: Adapter lora_gsm8k_2 epoch: 1/1 iteration: 32/128 step: 3
[2025-12-23 14:40:59,887] m-LoRA: Adapter lora_gsm8k_1 epoch: 1/1 iteration: 8/128 step: 3
[2025-12-23 14:41:03,088] m-LoRA: Adapter lora_gsm8k_2 loss: 2.257383346557617
[2025-12-23 14:41:03,348] m-LoRA: Adapter lora_gsm8k_1 loss: 1.7701897621154785
[2025-12-23 14:41:03,351] m-LoRA: Adapter lora_gsm8k_4 epoch: 1/1 iteration: 32/128 step: 3
[2025-12-23 14:41:03,569] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 6/128 step: 4
[2025-12-23 14:41:03,662] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 6/128 step: 4
[2025-12-23 14:41:06,797] m-LoRA: Adapter lora_gsm8k_4 loss: 1.2575498819351196
[2025-12-23 14:41:07,053] m-LoRA: Adapter lora_gsm8k_0 loss: 1.5938011407852173
[2025-12-23 14:41:07,055] m-LoRA: Adapter lora_gsm8k_2 epoch: 1/1 iteration: 48/128 step: 4
[2025-12-23 14:41:07,239] m-LoRA: Adapter lora_gsm8k_3 loss: 1.2941161394119263
[2025-12-23 14:41:07,242] m-LoRA: Adapter lora_gsm8k_1 epoch: 1/1 iteration: 12/128 step: 4
[2025-12-23 14:41:10,340] m-LoRA: Adapter lora_gsm8k_2 loss: 2.212636947631836
[2025-12-23 14:41:10,455] m-LoRA: Adapter lora_gsm8k_4 epoch: 1/1 iteration: 48/128 step: 4
[2025-12-23 14:41:10,789] m-LoRA: Adapter lora_gsm8k_1 loss: 1.5237054824829102
[2025-12-23 14:41:10,793] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 8/128 step: 5
[2025-12-23 14:41:10,923] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 8/128 step: 5
[2025-12-23 14:41:13,890] m-LoRA: Adapter lora_gsm8k_4 loss: 1.2344707250595093
[2025-12-23 14:41:14,162] m-LoRA: Adapter lora_gsm8k_3 loss: 2.0345568656921387
[2025-12-23 14:41:14,164] m-LoRA: Adapter lora_gsm8k_2 epoch: 1/1 iteration: 64/128 step: 5
[2025-12-23 14:41:14,382] m-LoRA: Adapter lora_gsm8k_0 loss: 1.4427459239959717
[2025-12-23 14:41:14,385] m-LoRA: Adapter lora_gsm8k_1 epoch: 1/1 iteration: 16/128 step: 5
[2025-12-23 14:41:17,688] m-LoRA: Adapter lora_gsm8k_2 loss: 1.8429805040359497
[2025-12-23 14:41:17,817] m-LoRA: Adapter lora_gsm8k_4 epoch: 1/1 iteration: 64/128 step: 5
[2025-12-23 14:41:18,198] m-LoRA: Adapter lora_gsm8k_1 loss: 1.482514500617981
[2025-12-23 14:41:18,203] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 10/128 step: 6
[2025-12-23 14:41:18,408] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 10/128 step: 6
[2025-12-23 14:41:23,666] m-LoRA: Adapter lora_gsm8k_4 loss: 1.3274765014648438
[2025-12-23 14:41:23,967] m-LoRA: Adapter lora_gsm8k_3 loss: 1.3384196758270264
[2025-12-23 14:41:23,982] m-LoRA: Adapter lora_gsm8k_2 epoch: 1/1 iteration: 80/128 step: 6
[2025-12-23 14:41:24,197] m-LoRA: Adapter lora_gsm8k_0 loss: 1.300154685974121
[2025-12-23 14:41:24,199] m-LoRA: Adapter lora_gsm8k_1 epoch: 1/1 iteration: 20/128 step: 6
[2025-12-23 14:41:27,364] m-LoRA: Adapter lora_gsm8k_2 loss: 1.5771719217300415
[2025-12-23 14:41:27,479] m-LoRA: Adapter lora_gsm8k_4 epoch: 1/1 iteration: 80/128 step: 6
[2025-12-23 14:41:27,787] m-LoRA: Adapter lora_gsm8k_1 loss: 1.3775177001953125
[2025-12-23 14:41:27,792] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 12/128 step: 7
[2025-12-23 14:41:27,929] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 12/128 step: 7
[2025-12-23 14:41:30,991] m-LoRA: Adapter lora_gsm8k_4 loss: 1.2543832063674927
[2025-12-23 14:41:31,247] m-LoRA: Adapter lora_gsm8k_3 loss: 1.5756412744522095
[2025-12-23 14:41:31,282] m-LoRA: Adapter lora_gsm8k_2 epoch: 1/1 iteration: 96/128 step: 7
[2025-12-23 14:41:31,488] m-LoRA: Adapter lora_gsm8k_0 loss: 1.4626940488815308
[2025-12-23 14:41:31,490] m-LoRA: Adapter lora_gsm8k_1 epoch: 1/1 iteration: 24/128 step: 7
[2025-12-23 14:41:34,819] m-LoRA: Adapter lora_gsm8k_2 loss: 1.2895958423614502
[2025-12-23 14:41:34,931] m-LoRA: Adapter lora_gsm8k_4 epoch: 1/1 iteration: 96/128 step: 7
[2025-12-23 14:41:35,295] m-LoRA: Adapter lora_gsm8k_1 loss: 1.5264954566955566
[2025-12-23 14:41:35,298] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 14/128 step: 8
[2025-12-23 14:41:35,405] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 14/128 step: 8
[2025-12-23 14:41:36,991] m-LoRA: Adapter lora_gsm8k_3 loss: 2.0053718090057373
[2025-12-23 14:41:38,957] m-LoRA: Adapter lora_gsm8k_4 loss: 0.9860901236534119
[2025-12-23 14:41:39,241] m-LoRA: Adapter lora_gsm8k_0 loss: 0.9241806268692017
[2025-12-23 14:41:39,250] m-LoRA: Adapter lora_gsm8k_2 epoch: 1/1 iteration: 112/128 step: 8
[2025-12-23 14:41:39,429] m-LoRA: Adapter lora_gsm8k_1 epoch: 1/1 iteration: 28/128 step: 8
[2025-12-23 14:41:39,535] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 16/128 step: 9
[2025-12-23 14:41:42,604] m-LoRA: Adapter lora_gsm8k_2 loss: 1.5578712224960327
[2025-12-23 14:41:42,832] m-LoRA: Adapter lora_gsm8k_4 epoch: 1/1 iteration: 112/128 step: 8
[2025-12-23 14:41:43,150] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 16/128 step: 9
[2025-12-23 14:41:43,206] m-LoRA: Adapter lora_gsm8k_1 loss: 0.9740343689918518
[2025-12-23 14:41:43,299] m-LoRA: Adapter lora_gsm8k_3 loss: 1.5302138328552246
[2025-12-23 14:41:46,099] m-LoRA: Adapter lora_gsm8k_4 loss: 1.046534776687622
[2025-12-23 14:41:46,230] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_2']
[2025-12-23 14:41:46,845] m-LoRA: Adapter lora_gsm8k_0 loss: 0.8530082106590271
[2025-12-23 14:41:46,847] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_5']
[2025-12-23 14:41:47,051] m-LoRA: Adapter lora_gsm8k_5 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:41:47,119] m-LoRA: Adapter lora_gsm8k_1 epoch: 1/1 iteration: 32/128 step: 9
[2025-12-23 14:41:47,234] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 18/128 step: 10
[2025-12-23 14:41:48,371] m-LoRA: Adapter lora_gsm8k_5 loss: 2.1267282962799072
[2025-12-23 14:41:48,726] m-LoRA: Adapter lora_gsm8k_1 loss: 1.2105175256729126
[2025-12-23 14:41:48,838] m-LoRA: Adapter lora_gsm8k_3 loss: 1.6731631755828857
[2025-12-23 14:41:48,985] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 18/128 step: 10
[2025-12-23 14:41:49,123] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_4']
[2025-12-23 14:41:49,483] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_6']
[2025-12-23 14:41:49,645] m-LoRA: Adapter lora_gsm8k_6 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:41:49,852] m-LoRA: Adapter lora_gsm8k_5 epoch: 1/1 iteration: 4/128 step: 2
[2025-12-23 14:41:49,973] m-LoRA: Adapter lora_gsm8k_0 loss: 1.007803201675415
[2025-12-23 14:41:50,190] m-LoRA: Adapter lora_gsm8k_1 epoch: 1/1 iteration: 36/128 step: 10
[2025-12-23 14:41:50,294] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 20/128 step: 11
[2025-12-23 14:41:50,398] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 20/128 step: 11
[2025-12-23 14:41:52,567] m-LoRA: Adapter lora_gsm8k_6 loss: 2.3308773040771484
[2025-12-23 14:41:52,602] m-LoRA: Adapter lora_gsm8k_5 loss: 1.5244678258895874
[2025-12-23 14:41:53,001] m-LoRA: Adapter lora_gsm8k_1 loss: 1.402143955230713
[2025-12-23 14:41:53,093] m-LoRA: Adapter lora_gsm8k_3 loss: 1.1688984632492065
[2025-12-23 14:41:53,226] m-LoRA: Adapter lora_gsm8k_0 loss: 0.6378664374351501
[2025-12-23 14:41:56,171] m-LoRA: Adapter lora_gsm8k_6 epoch: 1/1 iteration: 16/128 step: 2
[2025-12-23 14:41:56,433] m-LoRA: Adapter lora_gsm8k_5 epoch: 1/1 iteration: 8/128 step: 3
[2025-12-23 14:41:56,534] m-LoRA: Adapter lora_gsm8k_1 epoch: 1/1 iteration: 40/128 step: 11
[2025-12-23 14:41:56,630] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 22/128 step: 12
[2025-12-23 14:41:56,701] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 22/128 step: 12
[2025-12-23 14:41:58,355] m-LoRA: Adapter lora_gsm8k_6 loss: 1.487040638923645
[2025-12-23 14:41:58,685] m-LoRA: Adapter lora_gsm8k_5 loss: 1.041035532951355
[2025-12-23 14:41:58,878] m-LoRA: Adapter lora_gsm8k_1 loss: 1.7073019742965698
[2025-12-23 14:41:59,054] m-LoRA: Adapter lora_gsm8k_3 loss: 1.0728340148925781
[2025-12-23 14:41:59,178] m-LoRA: Adapter lora_gsm8k_0 loss: 1.0424067974090576
[2025-12-23 14:42:03,268] m-LoRA: Adapter lora_gsm8k_6 epoch: 1/1 iteration: 32/128 step: 3
[2025-12-23 14:42:03,526] m-LoRA: Adapter lora_gsm8k_5 epoch: 1/1 iteration: 12/128 step: 4
[2025-12-23 14:42:03,607] m-LoRA: Adapter lora_gsm8k_1 epoch: 1/1 iteration: 44/128 step: 12
[2025-12-23 14:42:03,673] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 24/128 step: 13
[2025-12-23 14:42:03,731] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 24/128 step: 13
[2025-12-23 14:42:05,794] m-LoRA: Adapter lora_gsm8k_6 loss: 1.2268235683441162
[2025-12-23 14:42:05,830] m-LoRA: Adapter lora_gsm8k_5 loss: 1.4804397821426392
[2025-12-23 14:42:06,233] m-LoRA: Adapter lora_gsm8k_1 loss: 1.3039170503616333
[2025-12-23 14:42:06,332] m-LoRA: Adapter lora_gsm8k_3 loss: 1.0305554866790771
[2025-12-23 14:42:06,504] m-LoRA: Adapter lora_gsm8k_0 loss: 0.7501599192619324
[2025-12-23 14:42:08,722] m-LoRA: Adapter lora_gsm8k_6 epoch: 1/1 iteration: 48/128 step: 4
[2025-12-23 14:42:08,943] m-LoRA: Adapter lora_gsm8k_5 epoch: 1/1 iteration: 16/128 step: 5
[2025-12-23 14:42:09,014] m-LoRA: Adapter lora_gsm8k_1 epoch: 1/1 iteration: 48/128 step: 13
[2025-12-23 14:42:09,078] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 26/128 step: 14
[2025-12-23 14:42:09,132] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 26/128 step: 14
[2025-12-23 14:42:11,120] m-LoRA: Adapter lora_gsm8k_6 loss: 1.3830703496932983
[2025-12-23 14:42:11,375] m-LoRA: Adapter lora_gsm8k_5 loss: 1.0964096784591675
[2025-12-23 14:42:11,799] m-LoRA: Adapter lora_gsm8k_1 loss: 1.2794116735458374
[2025-12-23 14:42:12,043] m-LoRA: Adapter lora_gsm8k_3 loss: 1.1604691743850708
[2025-12-23 14:42:12,176] m-LoRA: Adapter lora_gsm8k_0 loss: 0.6956745982170105
[2025-12-23 14:42:13,801] m-LoRA: Adapter lora_gsm8k_6 epoch: 1/1 iteration: 64/128 step: 5
[2025-12-23 14:42:14,014] m-LoRA: Adapter lora_gsm8k_5 epoch: 1/1 iteration: 20/128 step: 6
[2025-12-23 14:42:14,131] m-LoRA: Adapter lora_gsm8k_1 epoch: 1/1 iteration: 52/128 step: 14
[2025-12-23 14:42:14,218] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 28/128 step: 15
[2025-12-23 14:42:14,295] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 28/128 step: 15
[2025-12-23 14:42:16,080] m-LoRA: Adapter lora_gsm8k_6 loss: 1.1905652284622192
[2025-12-23 14:42:16,114] m-LoRA: Adapter lora_gsm8k_5 loss: 0.930885374546051
[2025-12-23 14:42:16,430] m-LoRA: Adapter lora_gsm8k_1 loss: 1.423568844795227
[2025-12-23 14:42:16,536] m-LoRA: Adapter lora_gsm8k_3 loss: 1.2137813568115234
[2025-12-23 14:42:16,617] m-LoRA: Adapter lora_gsm8k_0 loss: 0.6332295536994934
[2025-12-23 14:42:18,767] m-LoRA: Adapter lora_gsm8k_6 epoch: 1/1 iteration: 80/128 step: 6
[2025-12-23 14:42:19,025] m-LoRA: Adapter lora_gsm8k_5 epoch: 1/1 iteration: 24/128 step: 7
[2025-12-23 14:42:19,125] m-LoRA: Adapter lora_gsm8k_1 epoch: 1/1 iteration: 56/128 step: 15
[2025-12-23 14:42:19,208] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 30/128 step: 16
[2025-12-23 14:42:19,279] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 30/128 step: 16
[2025-12-23 14:42:21,269] m-LoRA: Adapter lora_gsm8k_6 loss: 1.0167241096496582
[2025-12-23 14:42:21,306] m-LoRA: Adapter lora_gsm8k_5 loss: 0.7315717935562134
[2025-12-23 14:42:21,922] m-LoRA: Adapter lora_gsm8k_1 loss: 1.3147341012954712
[2025-12-23 14:42:22,209] m-LoRA: Adapter lora_gsm8k_3 loss: 0.9718601703643799
[2025-12-23 14:42:22,444] m-LoRA: Adapter lora_gsm8k_0 loss: 0.6473264694213867
[2025-12-23 14:42:24,296] m-LoRA: Adapter lora_gsm8k_6 epoch: 1/1 iteration: 96/128 step: 7
[2025-12-23 14:42:24,576] m-LoRA: Adapter lora_gsm8k_5 epoch: 1/1 iteration: 28/128 step: 8
[2025-12-23 14:42:24,684] m-LoRA: Adapter lora_gsm8k_1 epoch: 1/1 iteration: 60/128 step: 16
[2025-12-23 14:42:24,789] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 32/128 step: 17
[2025-12-23 14:42:24,872] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 32/128 step: 17
[2025-12-23 14:42:27,081] m-LoRA: Adapter lora_gsm8k_6 loss: 0.8154930472373962
[2025-12-23 14:42:27,125] m-LoRA: Adapter lora_gsm8k_5 loss: 0.6132086515426636
[2025-12-23 14:42:27,503] m-LoRA: Adapter lora_gsm8k_1 loss: 1.0579843521118164
[2025-12-23 14:42:27,616] m-LoRA: Adapter lora_gsm8k_3 loss: 1.0089579820632935
[2025-12-23 14:42:27,736] m-LoRA: Adapter lora_gsm8k_0 loss: 0.6807419061660767
[2025-12-23 14:42:30,373] m-LoRA: Adapter lora_gsm8k_6 epoch: 1/1 iteration: 112/128 step: 8
[2025-12-23 14:42:30,626] m-LoRA: Adapter lora_gsm8k_5 epoch: 1/1 iteration: 32/128 step: 9
[2025-12-23 14:42:30,723] m-LoRA: Adapter lora_gsm8k_1 epoch: 1/1 iteration: 64/128 step: 17
[2025-12-23 14:42:30,813] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 34/128 step: 18
[2025-12-23 14:42:30,902] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 34/128 step: 18
[2025-12-23 14:42:32,870] m-LoRA: Adapter lora_gsm8k_6 loss: 0.9043380618095398
[2025-12-23 14:42:32,902] m-LoRA: Adapter lora_gsm8k_5 loss: 0.6490975022315979
[2025-12-23 14:42:33,496] m-LoRA: Adapter lora_gsm8k_1 loss: 1.236039638519287
[2025-12-23 14:42:33,735] m-LoRA: Adapter lora_gsm8k_3 loss: 0.8567232489585876
[2025-12-23 14:42:33,970] m-LoRA: Adapter lora_gsm8k_0 loss: 0.5475693345069885
[2025-12-23 14:42:35,870] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_6']
[2025-12-23 14:42:36,755] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_7']
[2025-12-23 14:42:36,895] m-LoRA: Adapter lora_gsm8k_7 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:42:37,001] m-LoRA: Adapter lora_gsm8k_5 epoch: 1/1 iteration: 36/128 step: 10
[2025-12-23 14:42:37,082] m-LoRA: Adapter lora_gsm8k_1 epoch: 1/1 iteration: 68/128 step: 18
[2025-12-23 14:42:37,154] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 36/128 step: 19
[2025-12-23 14:42:37,227] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 36/128 step: 19
[2025-12-23 14:42:37,727] m-LoRA: Adapter lora_gsm8k_7 loss: 2.4208595752716064
[2025-12-23 14:42:37,988] m-LoRA: Adapter lora_gsm8k_5 loss: 0.5774043202400208
[2025-12-23 14:42:38,228] m-LoRA: Adapter lora_gsm8k_1 loss: 1.2154916524887085
[2025-12-23 14:42:38,368] m-LoRA: Adapter lora_gsm8k_3 loss: 0.8548558354377747
[2025-12-23 14:42:38,514] m-LoRA: Adapter lora_gsm8k_0 loss: 0.7327480316162109
[2025-12-23 14:42:38,681] m-LoRA: Adapter lora_gsm8k_7 epoch: 1/1 iteration: 4/128 step: 2
[2025-12-23 14:42:38,983] m-LoRA: Adapter lora_gsm8k_5 epoch: 1/1 iteration: 40/128 step: 11
[2025-12-23 14:42:39,094] m-LoRA: Adapter lora_gsm8k_1 epoch: 1/1 iteration: 72/128 step: 19
[2025-12-23 14:42:39,500] m-LoRA: Adapter lora_gsm8k_7 loss: 1.7930057048797607
[2025-12-23 14:42:39,504] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 38/128 step: 20
[2025-12-23 14:42:39,827] m-LoRA: Adapter lora_gsm8k_5 loss: 0.6136784553527832
[2025-12-23 14:42:39,832] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 38/128 step: 20
[2025-12-23 14:42:40,224] m-LoRA: Adapter lora_gsm8k_1 loss: 0.9027324914932251
[2025-12-23 14:42:40,400] m-LoRA: Adapter lora_gsm8k_3 loss: 0.7024224400520325
[2025-12-23 14:42:40,403] m-LoRA: Adapter lora_gsm8k_7 epoch: 1/1 iteration: 8/128 step: 3
[2025-12-23 14:42:40,602] m-LoRA: Adapter lora_gsm8k_0 loss: 0.5331115126609802
[2025-12-23 14:42:40,725] m-LoRA: Adapter lora_gsm8k_5 epoch: 1/1 iteration: 44/128 step: 12
[2025-12-23 14:42:41,326] m-LoRA: Adapter lora_gsm8k_7 loss: 1.4559602737426758
[2025-12-23 14:42:41,329] m-LoRA: Adapter lora_gsm8k_1 epoch: 1/1 iteration: 76/128 step: 20
[2025-12-23 14:42:41,780] m-LoRA: Adapter lora_gsm8k_5 loss: 0.5172746777534485
[2025-12-23 14:42:41,785] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 40/128 step: 21
[2025-12-23 14:42:41,944] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 40/128 step: 21
[2025-12-23 14:42:42,402] m-LoRA: Adapter lora_gsm8k_1 loss: 0.8410756587982178
[2025-12-23 14:42:42,406] m-LoRA: Adapter lora_gsm8k_7 epoch: 1/1 iteration: 12/128 step: 4
[2025-12-23 14:42:42,508] m-LoRA: Adapter lora_gsm8k_3 loss: 0.8322482705116272
[2025-12-23 14:42:42,700] m-LoRA: Adapter lora_gsm8k_0 loss: 0.7368487119674683
[2025-12-23 14:42:42,809] m-LoRA: Adapter lora_gsm8k_5 epoch: 1/1 iteration: 48/128 step: 13
[2025-12-23 14:42:43,649] m-LoRA: Adapter lora_gsm8k_7 loss: 1.0866209268569946
[2025-12-23 14:42:43,653] m-LoRA: Adapter lora_gsm8k_1 epoch: 1/1 iteration: 80/128 step: 21
[2025-12-23 14:42:43,945] m-LoRA: Adapter lora_gsm8k_5 loss: 0.5088790655136108
[2025-12-23 14:42:43,949] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 42/128 step: 22
[2025-12-23 14:42:44,037] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 42/128 step: 22
[2025-12-23 14:42:44,870] m-LoRA: Adapter lora_gsm8k_1 loss: 0.9828664064407349
[2025-12-23 14:42:44,874] m-LoRA: Adapter lora_gsm8k_7 epoch: 1/1 iteration: 16/128 step: 5
[2025-12-23 14:42:44,907] m-LoRA: Adapter lora_gsm8k_3 loss: 0.5210801362991333
[2025-12-23 14:42:45,162] m-LoRA: Adapter lora_gsm8k_0 loss: 0.7117526531219482
[2025-12-23 14:42:45,165] m-LoRA: Adapter lora_gsm8k_5 epoch: 1/1 iteration: 52/128 step: 14
[2025-12-23 14:42:45,880] m-LoRA: Adapter lora_gsm8k_1 epoch: 1/1 iteration: 84/128 step: 22
[2025-12-23 14:42:45,933] m-LoRA: Adapter lora_gsm8k_7 loss: 1.358641505241394
[2025-12-23 14:42:46,224] m-LoRA: Adapter lora_gsm8k_5 loss: 0.6112412810325623
[2025-12-23 14:42:46,228] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 44/128 step: 23
[2025-12-23 14:42:46,326] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 44/128 step: 23
[2025-12-23 14:42:46,925] m-LoRA: Adapter lora_gsm8k_1 loss: 0.9318662881851196
[2025-12-23 14:42:46,929] m-LoRA: Adapter lora_gsm8k_7 epoch: 1/1 iteration: 20/128 step: 6
[2025-12-23 14:42:47,145] m-LoRA: Adapter lora_gsm8k_3 loss: 0.5919166207313538
[2025-12-23 14:42:47,324] m-LoRA: Adapter lora_gsm8k_0 loss: 0.7870724201202393
[2025-12-23 14:42:47,326] m-LoRA: Adapter lora_gsm8k_5 epoch: 1/1 iteration: 56/128 step: 15
[2025-12-23 14:42:47,858] m-LoRA: Adapter lora_gsm8k_7 loss: 1.2819786071777344
[2025-12-23 14:42:47,862] m-LoRA: Adapter lora_gsm8k_1 epoch: 1/1 iteration: 88/128 step: 23
[2025-12-23 14:42:48,014] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 46/128 step: 24
[2025-12-23 14:42:48,273] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 46/128 step: 24
[2025-12-23 14:42:48,311] m-LoRA: Adapter lora_gsm8k_5 loss: 0.5598441958427429
[2025-12-23 14:42:48,591] m-LoRA: Adapter lora_gsm8k_3 loss: 0.6877331733703613
[2025-12-23 14:42:48,593] m-LoRA: Adapter lora_gsm8k_7 epoch: 1/1 iteration: 24/128 step: 7
[2025-12-23 14:42:48,939] m-LoRA: Adapter lora_gsm8k_1 loss: 0.9273910522460938
[2025-12-23 14:42:49,081] m-LoRA: Adapter lora_gsm8k_0 loss: 0.5319472551345825
[2025-12-23 14:42:49,521] m-LoRA: Adapter lora_gsm8k_7 loss: 0.9088613390922546
[2025-12-23 14:42:49,526] m-LoRA: Adapter lora_gsm8k_5 epoch: 1/1 iteration: 60/128 step: 16
[2025-12-23 14:42:49,728] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 48/128 step: 25
[2025-12-23 14:42:49,823] m-LoRA: Adapter lora_gsm8k_1 epoch: 1/1 iteration: 92/128 step: 24
[2025-12-23 14:42:49,926] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 48/128 step: 25
[2025-12-23 14:42:50,726] m-LoRA: Adapter lora_gsm8k_7 epoch: 1/1 iteration: 28/128 step: 8
[2025-12-23 14:42:50,798] m-LoRA: Adapter lora_gsm8k_5 loss: 0.5091238617897034
[2025-12-23 14:42:50,819] m-LoRA: Adapter lora_gsm8k_3 loss: 0.49135729670524597
[2025-12-23 14:42:51,291] m-LoRA: Adapter lora_gsm8k_1 loss: 0.8417746424674988
[2025-12-23 14:42:51,403] m-LoRA: Adapter lora_gsm8k_0 loss: 0.6113353967666626
[2025-12-23 14:42:51,868] m-LoRA: Adapter lora_gsm8k_7 loss: 0.7916821241378784
[2025-12-23 14:42:52,023] m-LoRA: Adapter lora_gsm8k_5 epoch: 1/1 iteration: 64/128 step: 17
[2025-12-23 14:42:52,121] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 50/128 step: 26
[2025-12-23 14:42:52,376] m-LoRA: Adapter lora_gsm8k_1 epoch: 1/1 iteration: 96/128 step: 25
[2025-12-23 14:42:52,482] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 50/128 step: 26
[2025-12-23 14:42:53,173] m-LoRA: Adapter lora_gsm8k_5 loss: 0.47853076457977295
[2025-12-23 14:42:53,427] m-LoRA: Adapter lora_gsm8k_3 loss: 0.7279178500175476
[2025-12-23 14:42:53,430] m-LoRA: Adapter lora_gsm8k_7 epoch: 1/1 iteration: 32/128 step: 9
[2025-12-23 14:42:53,685] m-LoRA: Adapter lora_gsm8k_1 loss: 0.7074927091598511
[2025-12-23 14:42:53,874] m-LoRA: Adapter lora_gsm8k_0 loss: 0.4029330611228943
[2025-12-23 14:42:54,352] m-LoRA: Adapter lora_gsm8k_7 loss: 0.8518876433372498
[2025-12-23 14:42:54,357] m-LoRA: Adapter lora_gsm8k_5 epoch: 1/1 iteration: 68/128 step: 18
[2025-12-23 14:42:54,513] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 52/128 step: 27
[2025-12-23 14:42:54,600] m-LoRA: Adapter lora_gsm8k_1 epoch: 1/1 iteration: 100/128 step: 26
[2025-12-23 14:42:54,683] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 52/128 step: 27
[2025-12-23 14:42:55,446] m-LoRA: Adapter lora_gsm8k_5 loss: 0.40819865465164185
[2025-12-23 14:42:55,450] m-LoRA: Adapter lora_gsm8k_7 epoch: 1/1 iteration: 36/128 step: 10
[2025-12-23 14:42:55,569] m-LoRA: Adapter lora_gsm8k_3 loss: 0.6884982585906982
[2025-12-23 14:42:55,881] m-LoRA: Adapter lora_gsm8k_1 loss: 0.7136362195014954
[2025-12-23 14:42:56,019] m-LoRA: Adapter lora_gsm8k_0 loss: 0.6953353881835938
[2025-12-23 14:42:56,577] m-LoRA: Adapter lora_gsm8k_7 loss: 1.0402021408081055
[2025-12-23 14:42:56,580] m-LoRA: Adapter lora_gsm8k_5 epoch: 1/1 iteration: 72/128 step: 19
[2025-12-23 14:42:56,736] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 54/128 step: 28
[2025-12-23 14:42:56,875] m-LoRA: Adapter lora_gsm8k_1 epoch: 1/1 iteration: 104/128 step: 27
[2025-12-23 14:42:56,977] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 54/128 step: 28
[2025-12-23 14:42:57,574] m-LoRA: Adapter lora_gsm8k_7 epoch: 1/1 iteration: 40/128 step: 11
[2025-12-23 14:42:57,619] m-LoRA: Adapter lora_gsm8k_5 loss: 0.6118611693382263
[2025-12-23 14:42:57,656] m-LoRA: Adapter lora_gsm8k_3 loss: 0.7919220924377441
[2025-12-23 14:42:57,997] m-LoRA: Adapter lora_gsm8k_1 loss: 0.8641452789306641
[2025-12-23 14:42:58,140] m-LoRA: Adapter lora_gsm8k_0 loss: 0.6490904688835144
[2025-12-23 14:42:58,506] m-LoRA: Adapter lora_gsm8k_7 loss: 0.7781347632408142
[2025-12-23 14:42:58,509] m-LoRA: Adapter lora_gsm8k_5 epoch: 1/1 iteration: 76/128 step: 20
[2025-12-23 14:42:58,663] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 56/128 step: 29
[2025-12-23 14:42:58,785] m-LoRA: Adapter lora_gsm8k_1 epoch: 1/1 iteration: 108/128 step: 28
[2025-12-23 14:42:58,881] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 56/128 step: 29
[2025-12-23 14:42:59,552] m-LoRA: Adapter lora_gsm8k_7 epoch: 1/1 iteration: 44/128 step: 12
[2025-12-23 14:42:59,593] m-LoRA: Adapter lora_gsm8k_5 loss: 0.5255787968635559
[2025-12-23 14:42:59,695] m-LoRA: Adapter lora_gsm8k_3 loss: 0.7032098770141602
[2025-12-23 14:43:00,021] m-LoRA: Adapter lora_gsm8k_1 loss: 0.6289983987808228
[2025-12-23 14:43:00,215] m-LoRA: Adapter lora_gsm8k_0 loss: 0.642979085445404
[2025-12-23 14:43:00,617] m-LoRA: Adapter lora_gsm8k_7 loss: 0.6583626866340637
[2025-12-23 14:43:00,621] m-LoRA: Adapter lora_gsm8k_5 epoch: 1/1 iteration: 80/128 step: 21
[2025-12-23 14:43:00,774] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 58/128 step: 30
[2025-12-23 14:43:00,877] m-LoRA: Adapter lora_gsm8k_1 epoch: 1/1 iteration: 112/128 step: 29
[2025-12-23 14:43:00,997] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 58/128 step: 30
[2025-12-23 14:43:01,661] m-LoRA: Adapter lora_gsm8k_7 epoch: 1/1 iteration: 48/128 step: 13
[2025-12-23 14:43:01,712] m-LoRA: Adapter lora_gsm8k_5 loss: 0.574732780456543
[2025-12-23 14:43:01,745] m-LoRA: Adapter lora_gsm8k_3 loss: 0.6506859660148621
[2025-12-23 14:43:02,255] m-LoRA: Adapter lora_gsm8k_1 loss: 0.6262005567550659
[2025-12-23 14:43:02,443] m-LoRA: Adapter lora_gsm8k_0 loss: 0.8236231207847595
[2025-12-23 14:43:02,886] m-LoRA: Adapter lora_gsm8k_7 loss: 0.617870569229126
[2025-12-23 14:43:02,891] m-LoRA: Adapter lora_gsm8k_5 epoch: 1/1 iteration: 84/128 step: 22
[2025-12-23 14:43:03,041] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 60/128 step: 31
[2025-12-23 14:43:03,293] m-LoRA: Adapter lora_gsm8k_1 epoch: 1/1 iteration: 116/128 step: 30
[2025-12-23 14:43:03,400] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 60/128 step: 31
[2025-12-23 14:43:03,920] m-LoRA: Adapter lora_gsm8k_7 epoch: 1/1 iteration: 52/128 step: 14
[2025-12-23 14:43:03,972] m-LoRA: Adapter lora_gsm8k_5 loss: 0.6488849520683289
[2025-12-23 14:43:04,042] m-LoRA: Adapter lora_gsm8k_3 loss: 0.7029864192008972
[2025-12-23 14:43:04,465] m-LoRA: Adapter lora_gsm8k_1 loss: 0.5915516018867493
[2025-12-23 14:43:04,623] m-LoRA: Adapter lora_gsm8k_0 loss: 0.6644443273544312
[2025-12-23 14:43:05,004] m-LoRA: Adapter lora_gsm8k_7 loss: 0.6490554809570312
[2025-12-23 14:43:05,009] m-LoRA: Adapter lora_gsm8k_5 epoch: 1/1 iteration: 88/128 step: 23
[2025-12-23 14:43:05,186] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 62/128 step: 32
[2025-12-23 14:43:05,300] m-LoRA: Adapter lora_gsm8k_1 epoch: 1/1 iteration: 120/128 step: 31
[2025-12-23 14:43:05,419] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 62/128 step: 32
[2025-12-23 14:43:06,095] m-LoRA: Adapter lora_gsm8k_7 epoch: 1/1 iteration: 56/128 step: 15
[2025-12-23 14:43:06,161] m-LoRA: Adapter lora_gsm8k_5 loss: 0.5421893000602722
[2025-12-23 14:43:06,167] m-LoRA: Adapter lora_gsm8k_3 loss: 0.632763147354126
[2025-12-23 14:43:06,569] m-LoRA: Adapter lora_gsm8k_1 loss: 0.6862408518791199
[2025-12-23 14:43:06,653] m-LoRA: Adapter lora_gsm8k_0 loss: 0.6483719348907471
[2025-12-23 14:43:06,977] m-LoRA: Adapter lora_gsm8k_7 loss: 0.6548843383789062
[2025-12-23 14:43:07,130] m-LoRA: Adapter lora_gsm8k_5 epoch: 1/1 iteration: 92/128 step: 24
[2025-12-23 14:43:07,243] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 64/128 step: 33
[2025-12-23 14:43:07,496] m-LoRA: Adapter lora_gsm8k_1 epoch: 1/1 iteration: 124/128 step: 32
[2025-12-23 14:43:07,594] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 64/128 step: 33
[2025-12-23 14:43:08,117] m-LoRA: Adapter lora_gsm8k_7 epoch: 1/1 iteration: 60/128 step: 16
[2025-12-23 14:43:08,169] m-LoRA: Adapter lora_gsm8k_5 loss: 0.46953558921813965
[2025-12-23 14:43:08,353] m-LoRA: Adapter lora_gsm8k_3 loss: 0.5737283825874329
[2025-12-23 14:43:08,701] m-LoRA: Adapter lora_gsm8k_1 loss: 0.7248899340629578
[2025-12-23 14:43:08,943] m-LoRA: Adapter lora_gsm8k_0 loss: 0.680052638053894
[2025-12-23 14:43:09,289] m-LoRA: Adapter lora_gsm8k_7 loss: 0.5704393982887268
[2025-12-23 14:43:09,292] m-LoRA: Adapter lora_gsm8k_5 epoch: 1/1 iteration: 96/128 step: 25
[2025-12-23 14:43:09,460] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 66/128 step: 34
[2025-12-23 14:43:09,619] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_1']
[2025-12-23 14:43:09,840] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_8']
[2025-12-23 14:43:09,908] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:43:10,001] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 66/128 step: 34
[2025-12-23 14:43:10,300] m-LoRA: Adapter lora_gsm8k_7 epoch: 1/1 iteration: 64/128 step: 17
[2025-12-23 14:43:10,571] m-LoRA: Adapter lora_gsm8k_5 loss: 0.6736833453178406
[2025-12-23 14:43:10,751] m-LoRA: Adapter lora_gsm8k_3 loss: 0.6754629015922546
[2025-12-23 14:43:10,871] m-LoRA: Adapter lora_gsm8k_0 loss: 0.7618377804756165
[2025-12-23 14:43:10,996] m-LoRA: Adapter lora_gsm8k_8 loss: 2.3810689449310303
[2025-12-23 14:43:11,387] m-LoRA: Adapter lora_gsm8k_7 loss: 0.7423815727233887
[2025-12-23 14:43:11,391] m-LoRA: Adapter lora_gsm8k_5 epoch: 1/1 iteration: 100/128 step: 26
[2025-12-23 14:43:11,576] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 68/128 step: 35
[2025-12-23 14:43:11,677] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 68/128 step: 35
[2025-12-23 14:43:11,774] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 2/128 step: 2
[2025-12-23 14:43:12,327] m-LoRA: Adapter lora_gsm8k_5 loss: 0.589855968952179
[2025-12-23 14:43:12,330] m-LoRA: Adapter lora_gsm8k_7 epoch: 1/1 iteration: 68/128 step: 18
[2025-12-23 14:43:12,415] m-LoRA: Adapter lora_gsm8k_3 loss: 0.5605134963989258
[2025-12-23 14:43:12,663] m-LoRA: Adapter lora_gsm8k_0 loss: 0.5194718837738037
[2025-12-23 14:43:12,796] m-LoRA: Adapter lora_gsm8k_8 loss: 1.7159357070922852
[2025-12-23 14:43:13,364] m-LoRA: Adapter lora_gsm8k_7 loss: 0.6352097988128662
[2025-12-23 14:43:13,368] m-LoRA: Adapter lora_gsm8k_5 epoch: 1/1 iteration: 104/128 step: 27
[2025-12-23 14:43:13,554] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 70/128 step: 36
[2025-12-23 14:43:13,660] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 70/128 step: 36
[2025-12-23 14:43:13,740] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 4/128 step: 3
[2025-12-23 14:43:14,083] m-LoRA: Adapter lora_gsm8k_3 loss: 0.6003610491752625
[2025-12-23 14:43:14,357] m-LoRA: Adapter lora_gsm8k_7 epoch: 1/1 iteration: 72/128 step: 19
[2025-12-23 14:43:14,411] m-LoRA: Adapter lora_gsm8k_5 loss: 0.5571674108505249
[2025-12-23 14:43:14,567] m-LoRA: Adapter lora_gsm8k_0 loss: 0.8758633732795715
[2025-12-23 14:43:14,836] m-LoRA: Adapter lora_gsm8k_8 loss: 1.2408798933029175
[2025-12-23 14:43:14,840] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 72/128 step: 37
[2025-12-23 14:43:15,391] m-LoRA: Adapter lora_gsm8k_7 loss: 0.47147420048713684
[2025-12-23 14:43:15,395] m-LoRA: Adapter lora_gsm8k_5 epoch: 1/1 iteration: 108/128 step: 28
[2025-12-23 14:43:15,582] m-LoRA: Adapter lora_gsm8k_3 loss: 0.5813080072402954
[2025-12-23 14:43:15,584] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 72/128 step: 37
[2025-12-23 14:43:15,690] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 6/128 step: 4
[2025-12-23 14:43:16,437] m-LoRA: Adapter lora_gsm8k_7 epoch: 1/1 iteration: 76/128 step: 20
[2025-12-23 14:43:16,507] m-LoRA: Adapter lora_gsm8k_5 loss: 0.672175943851471
[2025-12-23 14:43:16,640] m-LoRA: Adapter lora_gsm8k_0 loss: 0.6913025379180908
[2025-12-23 14:43:16,642] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 74/128 step: 38
[2025-12-23 14:43:16,748] m-LoRA: Adapter lora_gsm8k_8 loss: 1.5178251266479492
[2025-12-23 14:43:17,119] m-LoRA: Adapter lora_gsm8k_7 loss: 0.5695241093635559
[2025-12-23 14:43:17,396] m-LoRA: Adapter lora_gsm8k_3 loss: 0.873938262462616
[2025-12-23 14:43:17,495] m-LoRA: Adapter lora_gsm8k_5 epoch: 1/1 iteration: 112/128 step: 29
[2025-12-23 14:43:17,645] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 74/128 step: 38
[2025-12-23 14:43:17,800] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 8/128 step: 5
[2025-12-23 14:43:18,222] m-LoRA: Adapter lora_gsm8k_7 epoch: 1/1 iteration: 80/128 step: 21
[2025-12-23 14:43:18,338] m-LoRA: Adapter lora_gsm8k_5 loss: 0.4743582606315613
[2025-12-23 14:43:18,342] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 76/128 step: 39
[2025-12-23 14:43:18,424] m-LoRA: Adapter lora_gsm8k_0 loss: 0.5437812209129333
[2025-12-23 14:43:18,440] m-LoRA: Adapter lora_gsm8k_8 loss: 1.609785556793213
[2025-12-23 14:43:18,924] m-LoRA: Adapter lora_gsm8k_7 loss: 0.42164909839630127
[2025-12-23 14:43:19,356] m-LoRA: Adapter lora_gsm8k_3 loss: 0.5523194074630737
[2025-12-23 14:43:19,360] m-LoRA: Adapter lora_gsm8k_5 epoch: 1/1 iteration: 116/128 step: 30
[2025-12-23 14:43:19,572] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 76/128 step: 39
[2025-12-23 14:43:19,676] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 10/128 step: 6
[2025-12-23 14:43:19,801] m-LoRA: Adapter lora_gsm8k_7 epoch: 1/1 iteration: 84/128 step: 22
[2025-12-23 14:43:20,233] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 78/128 step: 40
[2025-12-23 14:43:20,253] m-LoRA: Adapter lora_gsm8k_5 loss: 0.565058171749115
[2025-12-23 14:43:20,424] m-LoRA: Adapter lora_gsm8k_0 loss: 0.517071008682251
[2025-12-23 14:43:20,572] m-LoRA: Adapter lora_gsm8k_8 loss: 1.20884108543396
[2025-12-23 14:43:20,898] m-LoRA: Adapter lora_gsm8k_7 loss: 0.6970252394676208
[2025-12-23 14:43:21,055] m-LoRA: Adapter lora_gsm8k_3 loss: 0.7844972014427185
[2025-12-23 14:43:21,162] m-LoRA: Adapter lora_gsm8k_5 epoch: 1/1 iteration: 120/128 step: 31
[2025-12-23 14:43:21,264] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 78/128 step: 40
[2025-12-23 14:43:21,360] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 12/128 step: 7
[2025-12-23 14:43:21,872] m-LoRA: Adapter lora_gsm8k_7 epoch: 1/1 iteration: 88/128 step: 23
[2025-12-23 14:43:21,930] m-LoRA: Adapter lora_gsm8k_5 loss: 0.49669957160949707
[2025-12-23 14:43:22,139] m-LoRA: Adapter lora_gsm8k_0 loss: 0.6620100736618042
[2025-12-23 14:43:22,143] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 80/128 step: 41
[2025-12-23 14:43:22,240] m-LoRA: Adapter lora_gsm8k_8 loss: 1.1910717487335205
[2025-12-23 14:43:22,877] m-LoRA: Adapter lora_gsm8k_7 loss: 0.6852347254753113
[2025-12-23 14:43:22,880] m-LoRA: Adapter lora_gsm8k_5 epoch: 1/1 iteration: 124/128 step: 32
[2025-12-23 14:43:23,101] m-LoRA: Adapter lora_gsm8k_3 loss: 0.6784071326255798
[2025-12-23 14:43:23,104] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 80/128 step: 41
[2025-12-23 14:43:23,204] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 14/128 step: 8
[2025-12-23 14:43:23,849] m-LoRA: Adapter lora_gsm8k_7 epoch: 1/1 iteration: 92/128 step: 24
[2025-12-23 14:43:23,944] m-LoRA: Adapter lora_gsm8k_5 loss: 0.4990233778953552
[2025-12-23 14:43:23,949] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 82/128 step: 42
[2025-12-23 14:43:23,989] m-LoRA: Adapter lora_gsm8k_0 loss: 0.5574586987495422
[2025-12-23 14:43:24,061] m-LoRA: Adapter lora_gsm8k_8 loss: 1.4175310134887695
[2025-12-23 14:43:24,587] m-LoRA: Adapter lora_gsm8k_7 loss: 0.6866435408592224
[2025-12-23 14:43:24,774] m-LoRA: Adapter lora_gsm8k_3 loss: 0.663330078125
[2025-12-23 14:43:24,913] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_5']
[2025-12-23 14:43:25,692] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_9']
[2025-12-23 14:43:25,845] m-LoRA: Adapter lora_gsm8k_9 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:43:26,020] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 82/128 step: 42
[2025-12-23 14:43:26,119] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 16/128 step: 9
[2025-12-23 14:43:26,200] m-LoRA: Adapter lora_gsm8k_7 epoch: 1/1 iteration: 96/128 step: 25
[2025-12-23 14:43:26,461] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 84/128 step: 43
[2025-12-23 14:43:28,183] m-LoRA: Adapter lora_gsm8k_9 loss: 2.516509771347046
[2025-12-23 14:43:28,204] m-LoRA: Adapter lora_gsm8k_0 loss: 0.5951308012008667
[2025-12-23 14:43:28,237] m-LoRA: Adapter lora_gsm8k_8 loss: 0.8605881333351135
[2025-12-23 14:43:28,527] m-LoRA: Adapter lora_gsm8k_7 loss: 0.6947695016860962
[2025-12-23 14:43:28,711] m-LoRA: Adapter lora_gsm8k_3 loss: 0.6007536053657532
[2025-12-23 14:43:30,893] m-LoRA: Adapter lora_gsm8k_9 epoch: 1/1 iteration: 16/128 step: 2
[2025-12-23 14:43:31,145] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 84/128 step: 43
[2025-12-23 14:43:31,259] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 18/128 step: 10
[2025-12-23 14:43:31,340] m-LoRA: Adapter lora_gsm8k_7 epoch: 1/1 iteration: 100/128 step: 26
[2025-12-23 14:43:31,421] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 86/128 step: 44
[2025-12-23 14:43:33,403] m-LoRA: Adapter lora_gsm8k_9 loss: 1.2173073291778564
[2025-12-23 14:43:33,421] m-LoRA: Adapter lora_gsm8k_0 loss: 0.510413408279419
[2025-12-23 14:43:33,445] m-LoRA: Adapter lora_gsm8k_8 loss: 1.1580414772033691
[2025-12-23 14:43:33,849] m-LoRA: Adapter lora_gsm8k_7 loss: 0.6549504995346069
[2025-12-23 14:43:33,986] m-LoRA: Adapter lora_gsm8k_3 loss: 0.8267518877983093
[2025-12-23 14:43:36,267] m-LoRA: Adapter lora_gsm8k_9 epoch: 1/1 iteration: 32/128 step: 3
[2025-12-23 14:43:36,485] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 86/128 step: 44
[2025-12-23 14:43:36,583] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 20/128 step: 11
[2025-12-23 14:43:36,662] m-LoRA: Adapter lora_gsm8k_7 epoch: 1/1 iteration: 104/128 step: 27
[2025-12-23 14:43:36,774] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 88/128 step: 45
[2025-12-23 14:43:38,434] m-LoRA: Adapter lora_gsm8k_9 loss: 1.3838995695114136
[2025-12-23 14:43:38,455] m-LoRA: Adapter lora_gsm8k_0 loss: 0.5401162505149841
[2025-12-23 14:43:38,487] m-LoRA: Adapter lora_gsm8k_8 loss: 0.7976822257041931
[2025-12-23 14:43:38,822] m-LoRA: Adapter lora_gsm8k_7 loss: 0.5203592777252197
[2025-12-23 14:43:38,909] m-LoRA: Adapter lora_gsm8k_3 loss: 0.5762007832527161
[2025-12-23 14:43:40,930] m-LoRA: Adapter lora_gsm8k_9 epoch: 1/1 iteration: 48/128 step: 4
[2025-12-23 14:43:41,180] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 88/128 step: 45
[2025-12-23 14:43:41,277] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 22/128 step: 12
[2025-12-23 14:43:41,368] m-LoRA: Adapter lora_gsm8k_7 epoch: 1/1 iteration: 108/128 step: 28
[2025-12-23 14:43:41,465] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 90/128 step: 46
[2025-12-23 14:43:43,408] m-LoRA: Adapter lora_gsm8k_9 loss: 1.1652406454086304
[2025-12-23 14:43:43,427] m-LoRA: Adapter lora_gsm8k_0 loss: 0.5614234805107117
[2025-12-23 14:43:43,450] m-LoRA: Adapter lora_gsm8k_8 loss: 0.8456639051437378
[2025-12-23 14:43:43,841] m-LoRA: Adapter lora_gsm8k_7 loss: 0.47974422574043274
[2025-12-23 14:43:44,008] m-LoRA: Adapter lora_gsm8k_3 loss: 0.558885395526886
[2025-12-23 14:43:46,259] m-LoRA: Adapter lora_gsm8k_9 epoch: 1/1 iteration: 64/128 step: 5
[2025-12-23 14:43:46,526] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 90/128 step: 46
[2025-12-23 14:43:46,621] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 24/128 step: 13
[2025-12-23 14:43:46,711] m-LoRA: Adapter lora_gsm8k_7 epoch: 1/1 iteration: 112/128 step: 29
[2025-12-23 14:43:46,797] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 92/128 step: 47
[2025-12-23 14:43:48,793] m-LoRA: Adapter lora_gsm8k_9 loss: 1.063839077949524
[2025-12-23 14:43:48,812] m-LoRA: Adapter lora_gsm8k_0 loss: 0.544312596321106
[2025-12-23 14:43:49,068] m-LoRA: Adapter lora_gsm8k_8 loss: 0.7352783679962158
[2025-12-23 14:43:49,385] m-LoRA: Adapter lora_gsm8k_7 loss: 0.5042551159858704
[2025-12-23 14:43:49,479] m-LoRA: Adapter lora_gsm8k_3 loss: 0.5607612133026123
[2025-12-23 14:43:51,706] m-LoRA: Adapter lora_gsm8k_9 epoch: 1/1 iteration: 80/128 step: 6
[2025-12-23 14:43:51,965] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 92/128 step: 47
[2025-12-23 14:43:52,058] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 26/128 step: 14
[2025-12-23 14:43:52,135] m-LoRA: Adapter lora_gsm8k_7 epoch: 1/1 iteration: 116/128 step: 30
[2025-12-23 14:43:52,241] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 94/128 step: 48
[2025-12-23 14:43:54,124] m-LoRA: Adapter lora_gsm8k_9 loss: 1.0821752548217773
[2025-12-23 14:43:54,152] m-LoRA: Adapter lora_gsm8k_0 loss: 0.4830998480319977
[2025-12-23 14:43:54,181] m-LoRA: Adapter lora_gsm8k_8 loss: 0.6724780201911926
[2025-12-23 14:43:54,562] m-LoRA: Adapter lora_gsm8k_7 loss: 0.5989230275154114
[2025-12-23 14:43:54,675] m-LoRA: Adapter lora_gsm8k_3 loss: 0.6022825837135315
[2025-12-23 14:43:56,877] m-LoRA: Adapter lora_gsm8k_9 epoch: 1/1 iteration: 96/128 step: 7
[2025-12-23 14:43:57,095] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 94/128 step: 48
[2025-12-23 14:43:57,191] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 28/128 step: 15
[2025-12-23 14:43:57,270] m-LoRA: Adapter lora_gsm8k_7 epoch: 1/1 iteration: 120/128 step: 31
[2025-12-23 14:43:57,358] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 96/128 step: 49
[2025-12-23 14:43:59,167] m-LoRA: Adapter lora_gsm8k_9 loss: 0.9357919692993164
[2025-12-23 14:43:59,189] m-LoRA: Adapter lora_gsm8k_0 loss: 0.6240901350975037
[2025-12-23 14:43:59,418] m-LoRA: Adapter lora_gsm8k_8 loss: 0.5658182501792908
[2025-12-23 14:43:59,737] m-LoRA: Adapter lora_gsm8k_7 loss: 0.5227187871932983
[2025-12-23 14:43:59,858] m-LoRA: Adapter lora_gsm8k_3 loss: 0.5064043402671814
[2025-12-23 14:44:01,796] m-LoRA: Adapter lora_gsm8k_9 epoch: 1/1 iteration: 112/128 step: 8
[2025-12-23 14:44:02,046] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 96/128 step: 49
[2025-12-23 14:44:02,142] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 30/128 step: 16
[2025-12-23 14:44:02,220] m-LoRA: Adapter lora_gsm8k_7 epoch: 1/1 iteration: 124/128 step: 32
[2025-12-23 14:44:02,312] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 98/128 step: 50
[2025-12-23 14:44:04,820] m-LoRA: Adapter lora_gsm8k_9 loss: 0.6908155679702759
[2025-12-23 14:44:04,851] m-LoRA: Adapter lora_gsm8k_0 loss: 0.5587025880813599
[2025-12-23 14:44:04,887] m-LoRA: Adapter lora_gsm8k_8 loss: 0.7605569958686829
[2025-12-23 14:44:05,268] m-LoRA: Adapter lora_gsm8k_7 loss: 0.4656097888946533
[2025-12-23 14:44:05,396] m-LoRA: Adapter lora_gsm8k_3 loss: 0.6446797847747803
[2025-12-23 14:44:08,358] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_9']
[2025-12-23 14:44:09,197] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_10']
[2025-12-23 14:44:09,333] m-LoRA: Adapter lora_gsm8k_10 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:44:09,529] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 98/128 step: 50
[2025-12-23 14:44:09,634] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 32/128 step: 17
[2025-12-23 14:44:09,726] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_7']
[2025-12-23 14:44:10,285] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_11']
[2025-12-23 14:44:10,311] m-LoRA: Adapter lora_gsm8k_11 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:44:10,474] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 100/128 step: 51
[2025-12-23 14:44:12,569] m-LoRA: Adapter lora_gsm8k_10 loss: 2.3621907234191895
[2025-12-23 14:44:12,588] m-LoRA: Adapter lora_gsm8k_0 loss: 0.42559146881103516
[2025-12-23 14:44:12,612] m-LoRA: Adapter lora_gsm8k_8 loss: 0.6133006811141968
[2025-12-23 14:44:14,027] m-LoRA: Adapter lora_gsm8k_11 loss: 2.519139528274536
[2025-12-23 14:44:14,042] m-LoRA: Adapter lora_gsm8k_3 loss: 0.7448880672454834
[2025-12-23 14:44:15,984] m-LoRA: Adapter lora_gsm8k_10 epoch: 1/1 iteration: 16/128 step: 2
[2025-12-23 14:44:16,240] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 100/128 step: 51
[2025-12-23 14:44:16,303] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 34/128 step: 18
[2025-12-23 14:44:17,019] m-LoRA: Adapter lora_gsm8k_11 epoch: 1/1 iteration: 16/128 step: 2
[2025-12-23 14:44:17,249] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 102/128 step: 52
[2025-12-23 14:44:18,866] m-LoRA: Adapter lora_gsm8k_10 loss: 1.2803689241409302
[2025-12-23 14:44:18,889] m-LoRA: Adapter lora_gsm8k_0 loss: 0.716400682926178
[2025-12-23 14:44:18,918] m-LoRA: Adapter lora_gsm8k_8 loss: 0.6588170528411865
[2025-12-23 14:44:21,360] m-LoRA: Adapter lora_gsm8k_11 loss: 1.5538135766983032
[2025-12-23 14:44:21,378] m-LoRA: Adapter lora_gsm8k_3 loss: 0.6089593768119812
[2025-12-23 14:44:22,039] m-LoRA: Adapter lora_gsm8k_10 epoch: 1/1 iteration: 32/128 step: 3
[2025-12-23 14:44:22,306] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 102/128 step: 52
[2025-12-23 14:44:22,369] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 36/128 step: 19
[2025-12-23 14:44:26,443] m-LoRA: Adapter lora_gsm8k_10 loss: 1.3919930458068848
[2025-12-23 14:44:26,711] m-LoRA: Adapter lora_gsm8k_0 loss: 0.5531126856803894
[2025-12-23 14:44:26,713] m-LoRA: Adapter lora_gsm8k_11 epoch: 1/1 iteration: 32/128 step: 3
[2025-12-23 14:44:26,917] m-LoRA: Adapter lora_gsm8k_8 loss: 0.49830111861228943
[2025-12-23 14:44:26,919] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 104/128 step: 53
[2025-12-23 14:44:30,124] m-LoRA: Adapter lora_gsm8k_11 loss: 1.2099522352218628
[2025-12-23 14:44:30,399] m-LoRA: Adapter lora_gsm8k_3 loss: 0.588059663772583
[2025-12-23 14:44:30,401] m-LoRA: Adapter lora_gsm8k_10 epoch: 1/1 iteration: 48/128 step: 4
[2025-12-23 14:44:30,610] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 104/128 step: 53
[2025-12-23 14:44:30,713] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 38/128 step: 20
[2025-12-23 14:44:33,957] m-LoRA: Adapter lora_gsm8k_10 loss: 1.1956942081451416
[2025-12-23 14:44:34,223] m-LoRA: Adapter lora_gsm8k_0 loss: 0.651792585849762
[2025-12-23 14:44:34,225] m-LoRA: Adapter lora_gsm8k_11 epoch: 1/1 iteration: 48/128 step: 4
[2025-12-23 14:44:34,409] m-LoRA: Adapter lora_gsm8k_8 loss: 0.764772891998291
[2025-12-23 14:44:34,412] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 106/128 step: 54
[2025-12-23 14:44:37,583] m-LoRA: Adapter lora_gsm8k_11 loss: 1.4999921321868896
[2025-12-23 14:44:37,821] m-LoRA: Adapter lora_gsm8k_3 loss: 0.6176452040672302
[2025-12-23 14:44:37,834] m-LoRA: Adapter lora_gsm8k_10 epoch: 1/1 iteration: 64/128 step: 5
[2025-12-23 14:44:38,017] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 106/128 step: 54
[2025-12-23 14:44:38,094] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 40/128 step: 21
[2025-12-23 14:44:39,809] m-LoRA: Adapter lora_gsm8k_11 epoch: 1/1 iteration: 64/128 step: 5
[2025-12-23 14:44:41,731] m-LoRA: Adapter lora_gsm8k_10 loss: 0.9644243121147156
[2025-12-23 14:44:41,747] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 108/128 step: 55
[2025-12-23 14:44:41,799] m-LoRA: Adapter lora_gsm8k_0 loss: 0.6354399919509888
[2025-12-23 14:44:41,803] m-LoRA: Adapter lora_gsm8k_8 loss: 0.6184514164924622
[2025-12-23 14:44:43,324] m-LoRA: Adapter lora_gsm8k_11 loss: 1.2330776453018188
[2025-12-23 14:44:43,338] m-LoRA: Adapter lora_gsm8k_3 loss: 0.6938081979751587
[2025-12-23 14:44:45,302] m-LoRA: Adapter lora_gsm8k_10 epoch: 1/1 iteration: 80/128 step: 6
[2025-12-23 14:44:45,550] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 108/128 step: 55
[2025-12-23 14:44:45,627] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 42/128 step: 22
[2025-12-23 14:44:48,300] m-LoRA: Adapter lora_gsm8k_10 loss: 1.2917988300323486
[2025-12-23 14:44:48,407] m-LoRA: Adapter lora_gsm8k_11 epoch: 1/1 iteration: 80/128 step: 6
[2025-12-23 14:44:48,711] m-LoRA: Adapter lora_gsm8k_0 loss: 0.5337550640106201
[2025-12-23 14:44:48,714] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 110/128 step: 56
[2025-12-23 14:44:48,761] m-LoRA: Adapter lora_gsm8k_8 loss: 0.7728321552276611
[2025-12-23 14:44:52,048] m-LoRA: Adapter lora_gsm8k_11 loss: 1.1582926511764526
[2025-12-23 14:44:52,329] m-LoRA: Adapter lora_gsm8k_3 loss: 0.5227741599082947
[2025-12-23 14:44:52,331] m-LoRA: Adapter lora_gsm8k_10 epoch: 1/1 iteration: 96/128 step: 7
[2025-12-23 14:44:52,536] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 44/128 step: 23
[2025-12-23 14:44:52,605] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 110/128 step: 56
[2025-12-23 14:44:55,940] m-LoRA: Adapter lora_gsm8k_10 loss: 1.1075719594955444
[2025-12-23 14:44:56,054] m-LoRA: Adapter lora_gsm8k_11 epoch: 1/1 iteration: 96/128 step: 7
[2025-12-23 14:44:56,357] m-LoRA: Adapter lora_gsm8k_8 loss: 0.5585041046142578
[2025-12-23 14:44:56,360] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 112/128 step: 57
[2025-12-23 14:44:56,435] m-LoRA: Adapter lora_gsm8k_0 loss: 0.6996291875839233
[2025-12-23 14:44:59,672] m-LoRA: Adapter lora_gsm8k_11 loss: 1.2902320623397827
[2025-12-23 14:44:59,933] m-LoRA: Adapter lora_gsm8k_3 loss: 0.6098150014877319
[2025-12-23 14:44:59,935] m-LoRA: Adapter lora_gsm8k_10 epoch: 1/1 iteration: 112/128 step: 8
[2025-12-23 14:45:00,141] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 46/128 step: 24
[2025-12-23 14:45:00,233] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 112/128 step: 57
[2025-12-23 14:45:03,405] m-LoRA: Adapter lora_gsm8k_10 loss: 1.0733973979949951
[2025-12-23 14:45:03,508] m-LoRA: Adapter lora_gsm8k_11 epoch: 1/1 iteration: 112/128 step: 8
[2025-12-23 14:45:03,834] m-LoRA: Adapter lora_gsm8k_8 loss: 0.6091168522834778
[2025-12-23 14:45:03,837] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 114/128 step: 58
[2025-12-23 14:45:03,891] m-LoRA: Adapter lora_gsm8k_0 loss: 0.5326882004737854
[2025-12-23 14:45:07,462] m-LoRA: Adapter lora_gsm8k_11 loss: 1.07041597366333
[2025-12-23 14:45:07,598] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_10']
[2025-12-23 14:45:08,617] m-LoRA: Adapter lora_gsm8k_3 loss: 0.464523047208786
[2025-12-23 14:45:08,620] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_12']
[2025-12-23 14:45:08,822] m-LoRA: Adapter lora_gsm8k_12 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:45:08,923] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 48/128 step: 25
[2025-12-23 14:45:08,979] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 114/128 step: 58
[2025-12-23 14:45:09,922] m-LoRA: Adapter lora_gsm8k_12 loss: 2.722233772277832
[2025-12-23 14:45:10,058] m-LoRA: Adapter lora_gsm8k_8 loss: 0.7169477939605713
[2025-12-23 14:45:10,313] m-LoRA: Adapter lora_gsm8k_0 loss: 0.5698115825653076
[2025-12-23 14:45:10,696] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_11']
[2025-12-23 14:45:10,963] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_13']
[2025-12-23 14:45:11,039] m-LoRA: Adapter lora_gsm8k_13 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:45:11,205] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 116/128 step: 59
[2025-12-23 14:45:11,301] m-LoRA: Adapter lora_gsm8k_12 epoch: 1/1 iteration: 4/128 step: 2
[2025-12-23 14:45:11,397] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 50/128 step: 26
[2025-12-23 14:45:11,478] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 116/128 step: 59
[2025-12-23 14:45:13,113] m-LoRA: Adapter lora_gsm8k_13 loss: 2.453106641769409
[2025-12-23 14:45:13,132] m-LoRA: Adapter lora_gsm8k_3 loss: 0.6372277736663818
[2025-12-23 14:45:13,508] m-LoRA: Adapter lora_gsm8k_12 loss: 1.583523154258728
[2025-12-23 14:45:13,635] m-LoRA: Adapter lora_gsm8k_8 loss: 0.5963454842567444
[2025-12-23 14:45:13,780] m-LoRA: Adapter lora_gsm8k_0 loss: 0.6859093308448792
[2025-12-23 14:45:15,635] m-LoRA: Adapter lora_gsm8k_13 epoch: 1/1 iteration: 16/128 step: 2
[2025-12-23 14:45:15,853] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 118/128 step: 60
[2025-12-23 14:45:15,954] m-LoRA: Adapter lora_gsm8k_12 epoch: 1/1 iteration: 8/128 step: 3
[2025-12-23 14:45:16,062] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 52/128 step: 27
[2025-12-23 14:45:16,132] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 118/128 step: 60
[2025-12-23 14:45:17,841] m-LoRA: Adapter lora_gsm8k_13 loss: 1.4271358251571655
[2025-12-23 14:45:17,853] m-LoRA: Adapter lora_gsm8k_3 loss: 0.43810564279556274
[2025-12-23 14:45:18,087] m-LoRA: Adapter lora_gsm8k_12 loss: 1.573748230934143
[2025-12-23 14:45:18,216] m-LoRA: Adapter lora_gsm8k_8 loss: 0.6432071328163147
[2025-12-23 14:45:18,326] m-LoRA: Adapter lora_gsm8k_0 loss: 0.6707240343093872
[2025-12-23 14:45:20,307] m-LoRA: Adapter lora_gsm8k_13 epoch: 1/1 iteration: 32/128 step: 3
[2025-12-23 14:45:20,514] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 120/128 step: 61
[2025-12-23 14:45:20,604] m-LoRA: Adapter lora_gsm8k_12 epoch: 1/1 iteration: 12/128 step: 4
[2025-12-23 14:45:20,702] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 54/128 step: 28
[2025-12-23 14:45:20,794] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 120/128 step: 61
[2025-12-23 14:45:22,905] m-LoRA: Adapter lora_gsm8k_13 loss: 1.1746959686279297
[2025-12-23 14:45:22,920] m-LoRA: Adapter lora_gsm8k_3 loss: 0.6411113142967224
[2025-12-23 14:45:23,191] m-LoRA: Adapter lora_gsm8k_12 loss: 1.136285662651062
[2025-12-23 14:45:23,204] m-LoRA: Adapter lora_gsm8k_8 loss: 0.45376166701316833
[2025-12-23 14:45:23,342] m-LoRA: Adapter lora_gsm8k_0 loss: 0.611465334892273
[2025-12-23 14:45:25,939] m-LoRA: Adapter lora_gsm8k_13 epoch: 1/1 iteration: 48/128 step: 4
[2025-12-23 14:45:26,154] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 122/128 step: 62
[2025-12-23 14:45:26,231] m-LoRA: Adapter lora_gsm8k_12 epoch: 1/1 iteration: 16/128 step: 5
[2025-12-23 14:45:26,324] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 56/128 step: 29
[2025-12-23 14:45:26,398] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 122/128 step: 62
[2025-12-23 14:45:28,416] m-LoRA: Adapter lora_gsm8k_13 loss: 1.192247748374939
[2025-12-23 14:45:28,430] m-LoRA: Adapter lora_gsm8k_3 loss: 0.5175140500068665
[2025-12-23 14:45:28,549] m-LoRA: Adapter lora_gsm8k_12 loss: 1.5296316146850586
[2025-12-23 14:45:28,700] m-LoRA: Adapter lora_gsm8k_8 loss: 0.7176992297172546
[2025-12-23 14:45:28,853] m-LoRA: Adapter lora_gsm8k_0 loss: 0.5543015599250793
[2025-12-23 14:45:31,195] m-LoRA: Adapter lora_gsm8k_13 epoch: 1/1 iteration: 64/128 step: 5
[2025-12-23 14:45:31,465] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 124/128 step: 63
[2025-12-23 14:45:31,564] m-LoRA: Adapter lora_gsm8k_12 epoch: 1/1 iteration: 20/128 step: 6
[2025-12-23 14:45:31,656] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 58/128 step: 30
[2025-12-23 14:45:31,729] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 124/128 step: 63
[2025-12-23 14:45:34,144] m-LoRA: Adapter lora_gsm8k_13 loss: 0.9468613862991333
[2025-12-23 14:45:34,160] m-LoRA: Adapter lora_gsm8k_3 loss: 0.8235116600990295
[2025-12-23 14:45:34,301] m-LoRA: Adapter lora_gsm8k_12 loss: 1.0520099401474
[2025-12-23 14:45:34,486] m-LoRA: Adapter lora_gsm8k_8 loss: 0.6671642065048218
[2025-12-23 14:45:34,588] m-LoRA: Adapter lora_gsm8k_0 loss: 0.5912622809410095
[2025-12-23 14:45:37,563] m-LoRA: Adapter lora_gsm8k_13 epoch: 1/1 iteration: 80/128 step: 6
[2025-12-23 14:45:37,796] m-LoRA: Adapter lora_gsm8k_3 epoch: 1/1 iteration: 126/128 step: 64
[2025-12-23 14:45:37,870] m-LoRA: Adapter lora_gsm8k_12 epoch: 1/1 iteration: 24/128 step: 7
[2025-12-23 14:45:37,963] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 60/128 step: 31
[2025-12-23 14:45:38,035] m-LoRA: Adapter lora_gsm8k_0 epoch: 1/1 iteration: 126/128 step: 64
[2025-12-23 14:45:40,028] m-LoRA: Adapter lora_gsm8k_13 loss: 1.1038414239883423
[2025-12-23 14:45:40,044] m-LoRA: Adapter lora_gsm8k_3 loss: 0.4972303509712219
[2025-12-23 14:45:40,380] m-LoRA: Adapter lora_gsm8k_12 loss: 1.1204923391342163
[2025-12-23 14:45:40,493] m-LoRA: Adapter lora_gsm8k_8 loss: 0.6416612267494202
[2025-12-23 14:45:40,661] m-LoRA: Adapter lora_gsm8k_0 loss: 0.6517335772514343
[2025-12-23 14:45:42,792] m-LoRA: Adapter lora_gsm8k_13 epoch: 1/1 iteration: 96/128 step: 7
[2025-12-23 14:45:43,051] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_3']
[2025-12-23 14:45:43,254] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_14']
[2025-12-23 14:45:43,342] m-LoRA: Adapter lora_gsm8k_14 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:45:43,437] m-LoRA: Adapter lora_gsm8k_12 epoch: 1/1 iteration: 28/128 step: 8
[2025-12-23 14:45:43,521] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 62/128 step: 32
[2025-12-23 14:45:43,597] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_0']
[2025-12-23 14:45:43,975] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_15']
[2025-12-23 14:45:44,122] m-LoRA: Adapter lora_gsm8k_15 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:45:45,926] m-LoRA: Adapter lora_gsm8k_13 loss: 1.0022504329681396
[2025-12-23 14:45:45,955] m-LoRA: Adapter lora_gsm8k_14 loss: 2.755882740020752
[2025-12-23 14:45:46,326] m-LoRA: Adapter lora_gsm8k_12 loss: 0.9654318690299988
[2025-12-23 14:45:46,389] m-LoRA: Adapter lora_gsm8k_8 loss: 0.6315609216690063
[2025-12-23 14:45:46,768] m-LoRA: Adapter lora_gsm8k_15 loss: 2.1775460243225098
[2025-12-23 14:45:49,054] m-LoRA: Adapter lora_gsm8k_13 epoch: 1/1 iteration: 112/128 step: 8
[2025-12-23 14:45:49,298] m-LoRA: Adapter lora_gsm8k_14 epoch: 1/1 iteration: 4/128 step: 2
[2025-12-23 14:45:49,401] m-LoRA: Adapter lora_gsm8k_12 epoch: 1/1 iteration: 32/128 step: 9
[2025-12-23 14:45:49,478] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 64/128 step: 33
[2025-12-23 14:45:49,555] m-LoRA: Adapter lora_gsm8k_15 epoch: 1/1 iteration: 4/128 step: 2
[2025-12-23 14:45:51,177] m-LoRA: Adapter lora_gsm8k_13 loss: 1.2327290773391724
[2025-12-23 14:45:51,211] m-LoRA: Adapter lora_gsm8k_14 loss: 2.422206401824951
[2025-12-23 14:45:51,582] m-LoRA: Adapter lora_gsm8k_12 loss: 1.0796537399291992
[2025-12-23 14:45:51,639] m-LoRA: Adapter lora_gsm8k_8 loss: 0.5295443534851074
[2025-12-23 14:45:52,065] m-LoRA: Adapter lora_gsm8k_15 loss: 1.1744171380996704
[2025-12-23 14:45:53,619] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_13']
[2025-12-23 14:45:54,004] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_16']
[2025-12-23 14:45:54,082] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:45:54,179] m-LoRA: Adapter lora_gsm8k_14 epoch: 1/1 iteration: 8/128 step: 3
[2025-12-23 14:45:54,248] m-LoRA: Adapter lora_gsm8k_12 epoch: 1/1 iteration: 36/128 step: 10
[2025-12-23 14:45:54,321] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 66/128 step: 34
[2025-12-23 14:45:54,402] m-LoRA: Adapter lora_gsm8k_15 epoch: 1/1 iteration: 8/128 step: 3
[2025-12-23 14:45:54,689] m-LoRA: Adapter lora_gsm8k_16 loss: 2.4213356971740723
[2025-12-23 14:45:54,926] m-LoRA: Adapter lora_gsm8k_14 loss: 2.370964765548706
[2025-12-23 14:45:55,356] m-LoRA: Adapter lora_gsm8k_12 loss: 0.8288088440895081
[2025-12-23 14:45:55,360] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 2/128 step: 2
[2025-12-23 14:45:55,408] m-LoRA: Adapter lora_gsm8k_8 loss: 0.6587369441986084
[2025-12-23 14:45:55,833] m-LoRA: Adapter lora_gsm8k_15 loss: 1.2008769512176514
[2025-12-23 14:45:55,837] m-LoRA: Adapter lora_gsm8k_14 epoch: 1/1 iteration: 12/128 step: 4
[2025-12-23 14:45:56,165] m-LoRA: Adapter lora_gsm8k_16 loss: 2.063260793685913
[2025-12-23 14:45:56,167] m-LoRA: Adapter lora_gsm8k_12 epoch: 1/1 iteration: 40/128 step: 11
[2025-12-23 14:45:56,504] m-LoRA: Adapter lora_gsm8k_14 loss: 2.2349281311035156
[2025-12-23 14:45:56,507] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 68/128 step: 35
[2025-12-23 14:45:56,936] m-LoRA: Adapter lora_gsm8k_12 loss: 0.9750158786773682
[2025-12-23 14:45:56,939] m-LoRA: Adapter lora_gsm8k_15 epoch: 1/1 iteration: 12/128 step: 4
[2025-12-23 14:45:57,102] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 4/128 step: 3
[2025-12-23 14:45:57,247] m-LoRA: Adapter lora_gsm8k_8 loss: 0.6031224727630615
[2025-12-23 14:45:57,250] m-LoRA: Adapter lora_gsm8k_14 epoch: 1/1 iteration: 16/128 step: 5
[2025-12-23 14:45:57,841] m-LoRA: Adapter lora_gsm8k_12 epoch: 1/1 iteration: 44/128 step: 12
[2025-12-23 14:45:57,937] m-LoRA: Adapter lora_gsm8k_15 loss: 1.2153328657150269
[2025-12-23 14:45:57,941] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 70/128 step: 36
[2025-12-23 14:45:57,982] m-LoRA: Adapter lora_gsm8k_16 loss: 1.8649334907531738
[2025-12-23 14:45:58,338] m-LoRA: Adapter lora_gsm8k_14 loss: 1.72810697555542
[2025-12-23 14:45:59,063] m-LoRA: Adapter lora_gsm8k_12 loss: 0.5285707116127014
[2025-12-23 14:45:59,068] m-LoRA: Adapter lora_gsm8k_15 epoch: 1/1 iteration: 16/128 step: 5
[2025-12-23 14:45:59,242] m-LoRA: Adapter lora_gsm8k_8 loss: 0.5949301719665527
[2025-12-23 14:45:59,248] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 6/128 step: 4
[2025-12-23 14:45:59,340] m-LoRA: Adapter lora_gsm8k_14 epoch: 1/1 iteration: 20/128 step: 6
[2025-12-23 14:46:00,206] m-LoRA: Adapter lora_gsm8k_12 epoch: 1/1 iteration: 48/128 step: 13
[2025-12-23 14:46:00,323] m-LoRA: Adapter lora_gsm8k_15 loss: 1.1382153034210205
[2025-12-23 14:46:00,327] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 72/128 step: 37
[2025-12-23 14:46:00,369] m-LoRA: Adapter lora_gsm8k_16 loss: 1.563313603401184
[2025-12-23 14:46:00,651] m-LoRA: Adapter lora_gsm8k_14 loss: 1.9638794660568237
[2025-12-23 14:46:00,903] m-LoRA: Adapter lora_gsm8k_12 loss: 0.8712002038955688
[2025-12-23 14:46:01,253] m-LoRA: Adapter lora_gsm8k_8 loss: 0.5182811617851257
[2025-12-23 14:46:01,257] m-LoRA: Adapter lora_gsm8k_15 epoch: 1/1 iteration: 20/128 step: 6
[2025-12-23 14:46:01,373] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 8/128 step: 5
[2025-12-23 14:46:01,611] m-LoRA: Adapter lora_gsm8k_14 epoch: 1/1 iteration: 24/128 step: 7
[2025-12-23 14:46:01,762] m-LoRA: Adapter lora_gsm8k_12 epoch: 1/1 iteration: 52/128 step: 14
[2025-12-23 14:46:02,232] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 74/128 step: 38
[2025-12-23 14:46:02,273] m-LoRA: Adapter lora_gsm8k_15 loss: 1.2113053798675537
[2025-12-23 14:46:02,432] m-LoRA: Adapter lora_gsm8k_16 loss: 1.4126108884811401
[2025-12-23 14:46:02,836] m-LoRA: Adapter lora_gsm8k_14 loss: 1.3950363397598267
[2025-12-23 14:46:03,203] m-LoRA: Adapter lora_gsm8k_12 loss: 0.6355202794075012
[2025-12-23 14:46:03,208] m-LoRA: Adapter lora_gsm8k_15 epoch: 1/1 iteration: 24/128 step: 7
[2025-12-23 14:46:03,360] m-LoRA: Adapter lora_gsm8k_8 loss: 0.5050843954086304
[2025-12-23 14:46:03,362] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 10/128 step: 6
[2025-12-23 14:46:03,779] m-LoRA: Adapter lora_gsm8k_14 epoch: 1/1 iteration: 28/128 step: 8
[2025-12-23 14:46:04,224] m-LoRA: Adapter lora_gsm8k_12 epoch: 1/1 iteration: 56/128 step: 15
[2025-12-23 14:46:04,349] m-LoRA: Adapter lora_gsm8k_15 loss: 1.0411484241485596
[2025-12-23 14:46:04,353] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 76/128 step: 39
[2025-12-23 14:46:04,398] m-LoRA: Adapter lora_gsm8k_16 loss: 1.3815826177597046
[2025-12-23 14:46:04,717] m-LoRA: Adapter lora_gsm8k_14 loss: 1.489923119544983
[2025-12-23 14:46:05,093] m-LoRA: Adapter lora_gsm8k_12 loss: 0.6429610848426819
[2025-12-23 14:46:05,237] m-LoRA: Adapter lora_gsm8k_8 loss: 0.5862189531326294
[2025-12-23 14:46:05,353] m-LoRA: Adapter lora_gsm8k_15 epoch: 1/1 iteration: 28/128 step: 8
[2025-12-23 14:46:05,460] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 12/128 step: 7
[2025-12-23 14:46:05,687] m-LoRA: Adapter lora_gsm8k_14 epoch: 1/1 iteration: 32/128 step: 9
[2025-12-23 14:46:06,167] m-LoRA: Adapter lora_gsm8k_12 epoch: 1/1 iteration: 60/128 step: 16
[2025-12-23 14:46:06,325] m-LoRA: Adapter lora_gsm8k_15 loss: 1.1130192279815674
[2025-12-23 14:46:06,329] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 78/128 step: 40
[2025-12-23 14:46:06,408] m-LoRA: Adapter lora_gsm8k_16 loss: 1.2567960023880005
[2025-12-23 14:46:06,625] m-LoRA: Adapter lora_gsm8k_14 loss: 1.7817816734313965
[2025-12-23 14:46:06,948] m-LoRA: Adapter lora_gsm8k_12 loss: 0.5140624046325684
[2025-12-23 14:46:07,177] m-LoRA: Adapter lora_gsm8k_8 loss: 0.5456827878952026
[2025-12-23 14:46:07,300] m-LoRA: Adapter lora_gsm8k_15 epoch: 1/1 iteration: 32/128 step: 9
[2025-12-23 14:46:07,405] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 14/128 step: 8
[2025-12-23 14:46:07,543] m-LoRA: Adapter lora_gsm8k_14 epoch: 1/1 iteration: 36/128 step: 10
[2025-12-23 14:46:08,068] m-LoRA: Adapter lora_gsm8k_15 loss: 0.9835085272789001
[2025-12-23 14:46:08,071] m-LoRA: Adapter lora_gsm8k_12 epoch: 1/1 iteration: 64/128 step: 17
[2025-12-23 14:46:08,518] m-LoRA: Adapter lora_gsm8k_14 loss: 1.4897183179855347
[2025-12-23 14:46:08,523] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 80/128 step: 41
[2025-12-23 14:46:08,614] m-LoRA: Adapter lora_gsm8k_16 loss: 1.2699326276779175
[2025-12-23 14:46:09,101] m-LoRA: Adapter lora_gsm8k_12 loss: 0.6341797113418579
[2025-12-23 14:46:09,106] m-LoRA: Adapter lora_gsm8k_15 epoch: 1/1 iteration: 36/128 step: 10
[2025-12-23 14:46:09,573] m-LoRA: Adapter lora_gsm8k_8 loss: 0.49871504306793213
[2025-12-23 14:46:09,575] m-LoRA: Adapter lora_gsm8k_14 epoch: 1/1 iteration: 40/128 step: 11
[2025-12-23 14:46:10,093] m-LoRA: Adapter lora_gsm8k_15 loss: 0.7088523507118225
[2025-12-23 14:46:10,098] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 16/128 step: 9
[2025-12-23 14:46:10,414] m-LoRA: Adapter lora_gsm8k_14 loss: 1.6861050128936768
[2025-12-23 14:46:10,417] m-LoRA: Adapter lora_gsm8k_12 epoch: 1/1 iteration: 68/128 step: 18
[2025-12-23 14:46:10,568] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 82/128 step: 42
[2025-12-23 14:46:10,931] m-LoRA: Adapter lora_gsm8k_16 loss: 1.2209362983703613
[2025-12-23 14:46:11,034] m-LoRA: Adapter lora_gsm8k_15 epoch: 1/1 iteration: 40/128 step: 11
[2025-12-23 14:46:11,437] m-LoRA: Adapter lora_gsm8k_12 loss: 0.7007547616958618
[2025-12-23 14:46:11,442] m-LoRA: Adapter lora_gsm8k_14 epoch: 1/1 iteration: 44/128 step: 12
[2025-12-23 14:46:11,613] m-LoRA: Adapter lora_gsm8k_8 loss: 0.631496787071228
[2025-12-23 14:46:11,615] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 18/128 step: 10
[2025-12-23 14:46:11,928] m-LoRA: Adapter lora_gsm8k_15 loss: 0.7313165068626404
[2025-12-23 14:46:12,453] m-LoRA: Adapter lora_gsm8k_14 loss: 1.5333572626113892
[2025-12-23 14:46:12,457] m-LoRA: Adapter lora_gsm8k_12 epoch: 1/1 iteration: 72/128 step: 19
[2025-12-23 14:46:12,691] m-LoRA: Adapter lora_gsm8k_16 loss: 1.0500887632369995
[2025-12-23 14:46:12,695] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 84/128 step: 43
[2025-12-23 14:46:12,807] m-LoRA: Adapter lora_gsm8k_15 epoch: 1/1 iteration: 44/128 step: 12
[2025-12-23 14:46:13,466] m-LoRA: Adapter lora_gsm8k_14 epoch: 1/1 iteration: 48/128 step: 13
[2025-12-23 14:46:13,513] m-LoRA: Adapter lora_gsm8k_12 loss: 0.5783748030662537
[2025-12-23 14:46:13,671] m-LoRA: Adapter lora_gsm8k_8 loss: 0.5809941291809082
[2025-12-23 14:46:13,674] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 20/128 step: 11
[2025-12-23 14:46:13,973] m-LoRA: Adapter lora_gsm8k_15 loss: 0.579428493976593
[2025-12-23 14:46:14,377] m-LoRA: Adapter lora_gsm8k_14 loss: 1.3318766355514526
[2025-12-23 14:46:14,559] m-LoRA: Adapter lora_gsm8k_16 loss: 1.2169296741485596
[2025-12-23 14:46:14,561] m-LoRA: Adapter lora_gsm8k_12 epoch: 1/1 iteration: 76/128 step: 20
[2025-12-23 14:46:14,680] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 86/128 step: 44
[2025-12-23 14:46:14,922] m-LoRA: Adapter lora_gsm8k_15 epoch: 1/1 iteration: 48/128 step: 13
[2025-12-23 14:46:15,471] m-LoRA: Adapter lora_gsm8k_14 epoch: 1/1 iteration: 52/128 step: 14
[2025-12-23 14:46:15,589] m-LoRA: Adapter lora_gsm8k_12 loss: 0.5625713467597961
[2025-12-23 14:46:15,593] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 22/128 step: 12
[2025-12-23 14:46:15,624] m-LoRA: Adapter lora_gsm8k_8 loss: 0.5013269186019897
[2025-12-23 14:46:15,841] m-LoRA: Adapter lora_gsm8k_15 loss: 0.5721070766448975
[2025-12-23 14:46:16,228] m-LoRA: Adapter lora_gsm8k_14 loss: 1.567135214805603
[2025-12-23 14:46:16,361] m-LoRA: Adapter lora_gsm8k_16 loss: 0.9880431890487671
[2025-12-23 14:46:16,485] m-LoRA: Adapter lora_gsm8k_12 epoch: 1/1 iteration: 80/128 step: 21
[2025-12-23 14:46:16,575] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 88/128 step: 45
[2025-12-23 14:46:16,929] m-LoRA: Adapter lora_gsm8k_15 epoch: 1/1 iteration: 52/128 step: 14
[2025-12-23 14:46:17,303] m-LoRA: Adapter lora_gsm8k_14 epoch: 1/1 iteration: 56/128 step: 15
[2025-12-23 14:46:17,423] m-LoRA: Adapter lora_gsm8k_12 loss: 0.554871141910553
[2025-12-23 14:46:17,426] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 24/128 step: 13
[2025-12-23 14:46:17,508] m-LoRA: Adapter lora_gsm8k_8 loss: 0.8599684834480286
[2025-12-23 14:46:17,652] m-LoRA: Adapter lora_gsm8k_15 loss: 0.6873772740364075
[2025-12-23 14:46:17,982] m-LoRA: Adapter lora_gsm8k_14 loss: 1.6064748764038086
[2025-12-23 14:46:18,302] m-LoRA: Adapter lora_gsm8k_16 loss: 0.8798908591270447
[2025-12-23 14:46:18,306] m-LoRA: Adapter lora_gsm8k_12 epoch: 1/1 iteration: 84/128 step: 22
[2025-12-23 14:46:18,424] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 90/128 step: 46
[2025-12-23 14:46:18,510] m-LoRA: Adapter lora_gsm8k_15 epoch: 1/1 iteration: 56/128 step: 15
[2025-12-23 14:46:19,034] m-LoRA: Adapter lora_gsm8k_14 epoch: 1/1 iteration: 60/128 step: 16
[2025-12-23 14:46:19,074] m-LoRA: Adapter lora_gsm8k_12 loss: 0.5859575271606445
[2025-12-23 14:46:19,222] m-LoRA: Adapter lora_gsm8k_8 loss: 0.6698647141456604
[2025-12-23 14:46:19,225] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 26/128 step: 14
[2025-12-23 14:46:19,509] m-LoRA: Adapter lora_gsm8k_15 loss: 0.6216480135917664
[2025-12-23 14:46:19,887] m-LoRA: Adapter lora_gsm8k_14 loss: 1.3569341897964478
[2025-12-23 14:46:20,072] m-LoRA: Adapter lora_gsm8k_16 loss: 0.7494613528251648
[2025-12-23 14:46:20,074] m-LoRA: Adapter lora_gsm8k_12 epoch: 1/1 iteration: 88/128 step: 23
[2025-12-23 14:46:20,207] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 92/128 step: 47
[2025-12-23 14:46:20,427] m-LoRA: Adapter lora_gsm8k_15 epoch: 1/1 iteration: 60/128 step: 16
[2025-12-23 14:46:20,902] m-LoRA: Adapter lora_gsm8k_14 epoch: 1/1 iteration: 64/128 step: 17
[2025-12-23 14:46:20,946] m-LoRA: Adapter lora_gsm8k_12 loss: 0.5057811141014099
[2025-12-23 14:46:21,119] m-LoRA: Adapter lora_gsm8k_8 loss: 0.6169958710670471
[2025-12-23 14:46:21,121] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 28/128 step: 15
[2025-12-23 14:46:21,394] m-LoRA: Adapter lora_gsm8k_15 loss: 0.7263507843017578
[2025-12-23 14:46:21,827] m-LoRA: Adapter lora_gsm8k_14 loss: 1.3040562868118286
[2025-12-23 14:46:21,832] m-LoRA: Adapter lora_gsm8k_12 epoch: 1/1 iteration: 92/128 step: 24
[2025-12-23 14:46:22,014] m-LoRA: Adapter lora_gsm8k_16 loss: 0.7025453448295593
[2025-12-23 14:46:22,017] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 94/128 step: 48
[2025-12-23 14:46:22,267] m-LoRA: Adapter lora_gsm8k_15 epoch: 1/1 iteration: 64/128 step: 17
[2025-12-23 14:46:22,679] m-LoRA: Adapter lora_gsm8k_12 loss: 0.41917479038238525
[2025-12-23 14:46:22,924] m-LoRA: Adapter lora_gsm8k_8 loss: 0.8010485172271729
[2025-12-23 14:46:22,927] m-LoRA: Adapter lora_gsm8k_14 epoch: 1/1 iteration: 68/128 step: 18
[2025-12-23 14:46:23,198] m-LoRA: Adapter lora_gsm8k_15 loss: 0.5150718688964844
[2025-12-23 14:46:23,202] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 30/128 step: 16
[2025-12-23 14:46:23,777] m-LoRA: Adapter lora_gsm8k_14 loss: 1.6176127195358276
[2025-12-23 14:46:23,780] m-LoRA: Adapter lora_gsm8k_12 epoch: 1/1 iteration: 96/128 step: 25
[2025-12-23 14:46:24,040] m-LoRA: Adapter lora_gsm8k_16 loss: 0.7405136227607727
[2025-12-23 14:46:24,043] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 96/128 step: 49
[2025-12-23 14:46:24,168] m-LoRA: Adapter lora_gsm8k_15 epoch: 1/1 iteration: 68/128 step: 18
[2025-12-23 14:46:24,760] m-LoRA: Adapter lora_gsm8k_12 loss: 0.5898369550704956
[2025-12-23 14:46:24,765] m-LoRA: Adapter lora_gsm8k_14 epoch: 1/1 iteration: 72/128 step: 19
[2025-12-23 14:46:24,949] m-LoRA: Adapter lora_gsm8k_8 loss: 0.7507810592651367
[2025-12-23 14:46:24,951] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 32/128 step: 17
[2025-12-23 14:46:25,138] m-LoRA: Adapter lora_gsm8k_15 loss: 0.5152400135993958
[2025-12-23 14:46:25,948] m-LoRA: Adapter lora_gsm8k_14 loss: 1.1432806253433228
[2025-12-23 14:46:25,953] m-LoRA: Adapter lora_gsm8k_12 epoch: 1/1 iteration: 100/128 step: 26
[2025-12-23 14:46:26,105] m-LoRA: Adapter lora_gsm8k_16 loss: 0.5452617406845093
[2025-12-23 14:46:26,107] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 98/128 step: 50
[2025-12-23 14:46:26,252] m-LoRA: Adapter lora_gsm8k_15 epoch: 1/1 iteration: 72/128 step: 19
[2025-12-23 14:46:26,748] m-LoRA: Adapter lora_gsm8k_12 loss: 0.5497759580612183
[2025-12-23 14:46:26,980] m-LoRA: Adapter lora_gsm8k_8 loss: 0.5716155767440796
[2025-12-23 14:46:27,441] m-LoRA: Adapter lora_gsm8k_15 loss: 0.7060251832008362
[2025-12-23 14:46:27,446] m-LoRA: Adapter lora_gsm8k_14 epoch: 1/1 iteration: 76/128 step: 20
[2025-12-23 14:46:27,626] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 34/128 step: 18
[2025-12-23 14:46:27,718] m-LoRA: Adapter lora_gsm8k_12 epoch: 1/1 iteration: 104/128 step: 27
[2025-12-23 14:46:27,818] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 100/128 step: 51
[2025-12-23 14:46:28,409] m-LoRA: Adapter lora_gsm8k_15 epoch: 1/1 iteration: 76/128 step: 20
[2025-12-23 14:46:28,463] m-LoRA: Adapter lora_gsm8k_14 loss: 1.5035160779953003
[2025-12-23 14:46:28,469] m-LoRA: Adapter lora_gsm8k_16 loss: 0.8058937191963196
[2025-12-23 14:46:28,915] m-LoRA: Adapter lora_gsm8k_12 loss: 0.6369940042495728
[2025-12-23 14:46:29,100] m-LoRA: Adapter lora_gsm8k_8 loss: 0.5516130924224854
[2025-12-23 14:46:29,469] m-LoRA: Adapter lora_gsm8k_15 loss: 0.6814029216766357
[2025-12-23 14:46:29,488] m-LoRA: Adapter lora_gsm8k_14 epoch: 1/1 iteration: 80/128 step: 21
[2025-12-23 14:46:29,600] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 36/128 step: 19
[2025-12-23 14:46:29,855] m-LoRA: Adapter lora_gsm8k_12 epoch: 1/1 iteration: 108/128 step: 28
[2025-12-23 14:46:29,979] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 102/128 step: 52
[2025-12-23 14:46:30,487] m-LoRA: Adapter lora_gsm8k_15 epoch: 1/1 iteration: 80/128 step: 21
[2025-12-23 14:46:30,552] m-LoRA: Adapter lora_gsm8k_14 loss: 1.552069902420044
[2025-12-23 14:46:30,557] m-LoRA: Adapter lora_gsm8k_16 loss: 0.7408010363578796
[2025-12-23 14:46:30,921] m-LoRA: Adapter lora_gsm8k_12 loss: 0.6156412363052368
[2025-12-23 14:46:31,119] m-LoRA: Adapter lora_gsm8k_8 loss: 0.6536420583724976
[2025-12-23 14:46:31,619] m-LoRA: Adapter lora_gsm8k_15 loss: 0.49238017201423645
[2025-12-23 14:46:31,623] m-LoRA: Adapter lora_gsm8k_14 epoch: 1/1 iteration: 84/128 step: 22
[2025-12-23 14:46:31,827] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 38/128 step: 20
[2025-12-23 14:46:31,932] m-LoRA: Adapter lora_gsm8k_12 epoch: 1/1 iteration: 112/128 step: 29
[2025-12-23 14:46:32,048] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 104/128 step: 53
[2025-12-23 14:46:32,697] m-LoRA: Adapter lora_gsm8k_15 epoch: 1/1 iteration: 84/128 step: 22
[2025-12-23 14:46:32,769] m-LoRA: Adapter lora_gsm8k_14 loss: 1.5027058124542236
[2025-12-23 14:46:32,775] m-LoRA: Adapter lora_gsm8k_16 loss: 0.6418341398239136
[2025-12-23 14:46:33,208] m-LoRA: Adapter lora_gsm8k_12 loss: 0.5583320260047913
[2025-12-23 14:46:33,370] m-LoRA: Adapter lora_gsm8k_8 loss: 0.6479019522666931
[2025-12-23 14:46:33,774] m-LoRA: Adapter lora_gsm8k_15 loss: 0.5933030247688293
[2025-12-23 14:46:33,778] m-LoRA: Adapter lora_gsm8k_14 epoch: 1/1 iteration: 88/128 step: 23
[2025-12-23 14:46:33,938] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 40/128 step: 21
[2025-12-23 14:46:34,198] m-LoRA: Adapter lora_gsm8k_12 epoch: 1/1 iteration: 116/128 step: 30
[2025-12-23 14:46:34,328] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 106/128 step: 54
[2025-12-23 14:46:34,798] m-LoRA: Adapter lora_gsm8k_15 epoch: 1/1 iteration: 88/128 step: 23
[2025-12-23 14:46:34,847] m-LoRA: Adapter lora_gsm8k_14 loss: 1.5643236637115479
[2025-12-23 14:46:34,851] m-LoRA: Adapter lora_gsm8k_16 loss: 0.764936625957489
[2025-12-23 14:46:35,143] m-LoRA: Adapter lora_gsm8k_12 loss: 0.4601261019706726
[2025-12-23 14:46:35,212] m-LoRA: Adapter lora_gsm8k_8 loss: 0.6132780909538269
[2025-12-23 14:46:35,486] m-LoRA: Adapter lora_gsm8k_15 loss: 0.6186126470565796
[2025-12-23 14:46:35,726] m-LoRA: Adapter lora_gsm8k_14 epoch: 1/1 iteration: 92/128 step: 24
[2025-12-23 14:46:35,823] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 42/128 step: 22
[2025-12-23 14:46:35,911] m-LoRA: Adapter lora_gsm8k_12 epoch: 1/1 iteration: 120/128 step: 31
[2025-12-23 14:46:36,011] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 108/128 step: 55
[2025-12-23 14:46:36,616] m-LoRA: Adapter lora_gsm8k_15 epoch: 1/1 iteration: 92/128 step: 24
[2025-12-23 14:46:36,677] m-LoRA: Adapter lora_gsm8k_14 loss: 1.3654767274856567
[2025-12-23 14:46:36,687] m-LoRA: Adapter lora_gsm8k_16 loss: 0.5997006297111511
[2025-12-23 14:46:37,070] m-LoRA: Adapter lora_gsm8k_12 loss: 0.8050435185432434
[2025-12-23 14:46:37,195] m-LoRA: Adapter lora_gsm8k_8 loss: 0.4608628451824188
[2025-12-23 14:46:37,555] m-LoRA: Adapter lora_gsm8k_15 loss: 0.55680912733078
[2025-12-23 14:46:37,692] m-LoRA: Adapter lora_gsm8k_14 epoch: 1/1 iteration: 96/128 step: 25
[2025-12-23 14:46:37,822] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 44/128 step: 23
[2025-12-23 14:46:38,044] m-LoRA: Adapter lora_gsm8k_12 epoch: 1/1 iteration: 124/128 step: 32
[2025-12-23 14:46:38,154] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 110/128 step: 56
[2025-12-23 14:46:38,674] m-LoRA: Adapter lora_gsm8k_15 epoch: 1/1 iteration: 96/128 step: 25
[2025-12-23 14:46:38,729] m-LoRA: Adapter lora_gsm8k_14 loss: 1.431099772453308
[2025-12-23 14:46:38,828] m-LoRA: Adapter lora_gsm8k_16 loss: 0.5739447474479675
[2025-12-23 14:46:39,183] m-LoRA: Adapter lora_gsm8k_12 loss: 0.5629764199256897
[2025-12-23 14:46:39,384] m-LoRA: Adapter lora_gsm8k_8 loss: 0.47555750608444214
[2025-12-23 14:46:39,727] m-LoRA: Adapter lora_gsm8k_15 loss: 0.5028260946273804
[2025-12-23 14:46:39,730] m-LoRA: Adapter lora_gsm8k_14 epoch: 1/1 iteration: 100/128 step: 26
[2025-12-23 14:46:39,882] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 46/128 step: 24
[2025-12-23 14:46:40,010] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_12']
[2025-12-23 14:46:40,746] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_17']
[2025-12-23 14:46:40,784] m-LoRA: Adapter lora_gsm8k_17 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:46:41,132] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 112/128 step: 57
[2025-12-23 14:46:41,616] m-LoRA: Adapter lora_gsm8k_15 epoch: 1/1 iteration: 100/128 step: 26
[2025-12-23 14:46:41,666] m-LoRA: Adapter lora_gsm8k_14 loss: 1.4045240879058838
[2025-12-23 14:46:41,770] m-LoRA: Adapter lora_gsm8k_16 loss: 0.6225641965866089
[2025-12-23 14:46:42,685] m-LoRA: Adapter lora_gsm8k_14 epoch: 1/1 iteration: 104/128 step: 27
[2025-12-23 14:46:42,823] m-LoRA: Adapter lora_gsm8k_17 loss: 2.159485340118408
[2025-12-23 14:46:42,831] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 48/128 step: 25
[2025-12-23 14:46:42,869] m-LoRA: Adapter lora_gsm8k_8 loss: 0.6022211313247681
[2025-12-23 14:46:42,873] m-LoRA: Adapter lora_gsm8k_15 loss: 0.6842108964920044
[2025-12-23 14:46:43,457] m-LoRA: Adapter lora_gsm8k_14 loss: 1.292791485786438
[2025-12-23 14:46:43,552] m-LoRA: Adapter lora_gsm8k_16 loss: 0.6403891444206238
[2025-12-23 14:46:45,072] m-LoRA: Adapter lora_gsm8k_17 epoch: 1/1 iteration: 8/128 step: 2
[2025-12-23 14:46:45,220] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 114/128 step: 58
[2025-12-23 14:46:45,313] m-LoRA: Adapter lora_gsm8k_15 epoch: 1/1 iteration: 104/128 step: 27
[2025-12-23 14:46:45,410] m-LoRA: Adapter lora_gsm8k_14 epoch: 1/1 iteration: 108/128 step: 28
[2025-12-23 14:46:45,510] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 50/128 step: 26
[2025-12-23 14:46:46,386] m-LoRA: Adapter lora_gsm8k_17 loss: 1.8938342332839966
[2025-12-23 14:46:46,408] m-LoRA: Adapter lora_gsm8k_8 loss: 0.6040651202201843
[2025-12-23 14:46:46,725] m-LoRA: Adapter lora_gsm8k_15 loss: 0.47791457176208496
[2025-12-23 14:46:47,067] m-LoRA: Adapter lora_gsm8k_14 loss: 1.2202339172363281
[2025-12-23 14:46:47,181] m-LoRA: Adapter lora_gsm8k_16 loss: 0.5832014679908752
[2025-12-23 14:46:47,861] m-LoRA: Adapter lora_gsm8k_17 epoch: 1/1 iteration: 16/128 step: 3
[2025-12-23 14:46:48,022] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 116/128 step: 59
[2025-12-23 14:46:48,117] m-LoRA: Adapter lora_gsm8k_15 epoch: 1/1 iteration: 108/128 step: 28
[2025-12-23 14:46:48,324] m-LoRA: Adapter lora_gsm8k_14 epoch: 1/1 iteration: 112/128 step: 29
[2025-12-23 14:46:48,414] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 52/128 step: 27
[2025-12-23 14:46:49,233] m-LoRA: Adapter lora_gsm8k_17 loss: 1.263856291770935
[2025-12-23 14:46:49,393] m-LoRA: Adapter lora_gsm8k_8 loss: 0.6479792594909668
[2025-12-23 14:46:49,873] m-LoRA: Adapter lora_gsm8k_15 loss: 0.4517149329185486
[2025-12-23 14:46:50,124] m-LoRA: Adapter lora_gsm8k_14 loss: 1.3419073820114136
[2025-12-23 14:46:50,379] m-LoRA: Adapter lora_gsm8k_16 loss: 0.5639645457267761
[2025-12-23 14:46:50,748] m-LoRA: Adapter lora_gsm8k_17 epoch: 1/1 iteration: 24/128 step: 4
[2025-12-23 14:46:50,870] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 118/128 step: 60
[2025-12-23 14:46:51,274] m-LoRA: Adapter lora_gsm8k_15 epoch: 1/1 iteration: 112/128 step: 29
[2025-12-23 14:46:51,992] m-LoRA: Adapter lora_gsm8k_14 epoch: 1/1 iteration: 116/128 step: 30
[2025-12-23 14:46:52,032] m-LoRA: Adapter lora_gsm8k_17 loss: 1.5279232263565063
[2025-12-23 14:46:52,261] m-LoRA: Adapter lora_gsm8k_8 loss: 0.7852047681808472
[2025-12-23 14:46:52,264] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 54/128 step: 28
[2025-12-23 14:46:52,439] m-LoRA: Adapter lora_gsm8k_15 loss: 0.49257373809814453
[2025-12-23 14:46:52,711] m-LoRA: Adapter lora_gsm8k_14 loss: 1.5196022987365723
[2025-12-23 14:46:53,240] m-LoRA: Adapter lora_gsm8k_16 loss: 0.5745382905006409
[2025-12-23 14:46:53,463] m-LoRA: Adapter lora_gsm8k_17 epoch: 1/1 iteration: 32/128 step: 5
[2025-12-23 14:46:53,565] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 120/128 step: 61
[2025-12-23 14:46:53,688] m-LoRA: Adapter lora_gsm8k_15 epoch: 1/1 iteration: 116/128 step: 30
[2025-12-23 14:46:53,940] m-LoRA: Adapter lora_gsm8k_14 epoch: 1/1 iteration: 120/128 step: 31
[2025-12-23 14:46:54,571] m-LoRA: Adapter lora_gsm8k_17 loss: 1.595638632774353
[2025-12-23 14:46:54,576] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 56/128 step: 29
[2025-12-23 14:46:54,613] m-LoRA: Adapter lora_gsm8k_8 loss: 0.48369014263153076
[2025-12-23 14:46:54,969] m-LoRA: Adapter lora_gsm8k_15 loss: 0.45363980531692505
[2025-12-23 14:46:55,151] m-LoRA: Adapter lora_gsm8k_14 loss: 1.6829661130905151
[2025-12-23 14:46:55,275] m-LoRA: Adapter lora_gsm8k_16 loss: 0.5924976468086243
[2025-12-23 14:46:55,826] m-LoRA: Adapter lora_gsm8k_17 epoch: 1/1 iteration: 40/128 step: 6
[2025-12-23 14:46:55,959] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 122/128 step: 62
[2025-12-23 14:46:56,052] m-LoRA: Adapter lora_gsm8k_15 epoch: 1/1 iteration: 120/128 step: 31
[2025-12-23 14:46:56,177] m-LoRA: Adapter lora_gsm8k_14 epoch: 1/1 iteration: 124/128 step: 32
[2025-12-23 14:46:56,282] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 58/128 step: 30
[2025-12-23 14:46:57,105] m-LoRA: Adapter lora_gsm8k_17 loss: 1.4551608562469482
[2025-12-23 14:46:57,120] m-LoRA: Adapter lora_gsm8k_8 loss: 0.528814435005188
[2025-12-23 14:46:57,487] m-LoRA: Adapter lora_gsm8k_15 loss: 0.5100479125976562
[2025-12-23 14:46:57,861] m-LoRA: Adapter lora_gsm8k_14 loss: 1.1047420501708984
[2025-12-23 14:46:58,037] m-LoRA: Adapter lora_gsm8k_16 loss: 0.6731690168380737
[2025-12-23 14:46:58,451] m-LoRA: Adapter lora_gsm8k_17 epoch: 1/1 iteration: 48/128 step: 7
[2025-12-23 14:46:58,559] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 124/128 step: 63
[2025-12-23 14:46:58,680] m-LoRA: Adapter lora_gsm8k_15 epoch: 1/1 iteration: 124/128 step: 32
[2025-12-23 14:46:59,591] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_14']
[2025-12-23 14:47:00,073] m-LoRA: Adapter lora_gsm8k_17 loss: 1.2709516286849976
[2025-12-23 14:47:00,129] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_18']
[2025-12-23 14:47:00,300] m-LoRA: Adapter lora_gsm8k_18 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:47:00,382] m-LoRA: Adapter lora_gsm8k_8 loss: 0.5708662867546082
[2025-12-23 14:47:00,384] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 60/128 step: 31
[2025-12-23 14:47:00,436] m-LoRA: Adapter lora_gsm8k_15 loss: 0.4404109716415405
[2025-12-23 14:47:01,928] m-LoRA: Adapter lora_gsm8k_18 loss: 2.4520041942596436
[2025-12-23 14:47:01,934] m-LoRA: Adapter lora_gsm8k_17 epoch: 1/1 iteration: 56/128 step: 8
[2025-12-23 14:47:02,076] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_15']
[2025-12-23 14:47:02,763] m-LoRA: Adapter lora_gsm8k_16 loss: 0.5642902851104736
[2025-12-23 14:47:02,765] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_19']
[2025-12-23 14:47:02,915] m-LoRA: Adapter lora_gsm8k_19 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:47:02,977] m-LoRA: Adapter lora_gsm8k_8 epoch: 1/1 iteration: 126/128 step: 64
[2025-12-23 14:47:03,896] m-LoRA: Adapter lora_gsm8k_18 epoch: 1/1 iteration: 8/128 step: 2
[2025-12-23 14:47:03,972] m-LoRA: Adapter lora_gsm8k_17 loss: 1.2823463678359985
[2025-12-23 14:47:04,185] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 62/128 step: 32
[2025-12-23 14:47:04,205] m-LoRA: Adapter lora_gsm8k_19 loss: 2.207874059677124
[2025-12-23 14:47:04,315] m-LoRA: Adapter lora_gsm8k_8 loss: 0.3882836401462555
[2025-12-23 14:47:05,082] m-LoRA: Adapter lora_gsm8k_18 loss: 1.8133572340011597
[2025-12-23 14:47:05,170] m-LoRA: Adapter lora_gsm8k_16 loss: 0.5047299861907959
[2025-12-23 14:47:05,343] m-LoRA: Adapter lora_gsm8k_17 epoch: 1/1 iteration: 64/128 step: 9
[2025-12-23 14:47:05,464] m-LoRA: Adapter lora_gsm8k_19 epoch: 1/1 iteration: 4/128 step: 2
[2025-12-23 14:47:05,528] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_8']
[2025-12-23 14:47:05,806] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_20']
[2025-12-23 14:47:05,887] m-LoRA: Adapter lora_gsm8k_20 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:47:07,028] m-LoRA: Adapter lora_gsm8k_18 epoch: 1/1 iteration: 16/128 step: 3
[2025-12-23 14:47:07,102] m-LoRA: Adapter lora_gsm8k_17 loss: 1.2914161682128906
[2025-12-23 14:47:07,173] m-LoRA: Adapter lora_gsm8k_19 loss: 2.9058730602264404
[2025-12-23 14:47:07,176] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 64/128 step: 33
[2025-12-23 14:47:07,560] m-LoRA: Adapter lora_gsm8k_20 loss: 2.3083817958831787
[2025-12-23 14:47:08,112] m-LoRA: Adapter lora_gsm8k_18 loss: 1.752875566482544
[2025-12-23 14:47:08,389] m-LoRA: Adapter lora_gsm8k_16 loss: 0.6985019445419312
[2025-12-23 14:47:08,626] m-LoRA: Adapter lora_gsm8k_17 epoch: 1/1 iteration: 72/128 step: 10
[2025-12-23 14:47:08,757] m-LoRA: Adapter lora_gsm8k_19 epoch: 1/1 iteration: 8/128 step: 3
[2025-12-23 14:47:09,023] m-LoRA: Adapter lora_gsm8k_20 epoch: 1/1 iteration: 4/128 step: 2
[2025-12-23 14:47:10,143] m-LoRA: Adapter lora_gsm8k_18 epoch: 1/1 iteration: 24/128 step: 4
[2025-12-23 14:47:10,224] m-LoRA: Adapter lora_gsm8k_17 loss: 1.2528029680252075
[2025-12-23 14:47:10,473] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 66/128 step: 34
[2025-12-23 14:47:10,493] m-LoRA: Adapter lora_gsm8k_19 loss: 2.086663246154785
[2025-12-23 14:47:10,699] m-LoRA: Adapter lora_gsm8k_20 loss: 1.9443976879119873
[2025-12-23 14:47:11,593] m-LoRA: Adapter lora_gsm8k_17 epoch: 1/1 iteration: 80/128 step: 11
[2025-12-23 14:47:12,450] m-LoRA: Adapter lora_gsm8k_19 epoch: 1/1 iteration: 12/128 step: 4
[2025-12-23 14:47:12,517] m-LoRA: Adapter lora_gsm8k_18 loss: 1.0492233037948608
[2025-12-23 14:47:12,774] m-LoRA: Adapter lora_gsm8k_16 loss: 0.5613622069358826
[2025-12-23 14:47:12,776] m-LoRA: Adapter lora_gsm8k_20 epoch: 1/1 iteration: 8/128 step: 3
[2025-12-23 14:47:13,359] m-LoRA: Adapter lora_gsm8k_17 loss: 1.1890376806259155
[2025-12-23 14:47:13,703] m-LoRA: Adapter lora_gsm8k_19 loss: 2.251133441925049
[2025-12-23 14:47:14,297] m-LoRA: Adapter lora_gsm8k_20 loss: 1.414400339126587
[2025-12-23 14:47:14,465] m-LoRA: Adapter lora_gsm8k_18 epoch: 1/1 iteration: 32/128 step: 5
[2025-12-23 14:47:14,640] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 68/128 step: 35
[2025-12-23 14:47:15,048] m-LoRA: Adapter lora_gsm8k_17 epoch: 1/1 iteration: 88/128 step: 12
[2025-12-23 14:47:15,846] m-LoRA: Adapter lora_gsm8k_19 epoch: 1/1 iteration: 16/128 step: 5
[2025-12-23 14:47:15,911] m-LoRA: Adapter lora_gsm8k_18 loss: 1.2950652837753296
[2025-12-23 14:47:16,011] m-LoRA: Adapter lora_gsm8k_16 loss: 0.5823634266853333
[2025-12-23 14:47:16,014] m-LoRA: Adapter lora_gsm8k_20 epoch: 1/1 iteration: 12/128 step: 4
[2025-12-23 14:47:16,625] m-LoRA: Adapter lora_gsm8k_17 loss: 1.394353985786438
[2025-12-23 14:47:16,803] m-LoRA: Adapter lora_gsm8k_19 loss: 2.1442928314208984
[2025-12-23 14:47:17,641] m-LoRA: Adapter lora_gsm8k_20 loss: 1.3210868835449219
[2025-12-23 14:47:17,646] m-LoRA: Adapter lora_gsm8k_18 epoch: 1/1 iteration: 40/128 step: 6
[2025-12-23 14:47:17,870] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 70/128 step: 36
[2025-12-23 14:47:18,118] m-LoRA: Adapter lora_gsm8k_17 epoch: 1/1 iteration: 96/128 step: 13
[2025-12-23 14:47:18,237] m-LoRA: Adapter lora_gsm8k_19 epoch: 1/1 iteration: 20/128 step: 6
[2025-12-23 14:47:19,345] m-LoRA: Adapter lora_gsm8k_20 epoch: 1/1 iteration: 16/128 step: 5
[2025-12-23 14:47:19,442] m-LoRA: Adapter lora_gsm8k_18 loss: 1.1667124032974243
[2025-12-23 14:47:19,451] m-LoRA: Adapter lora_gsm8k_16 loss: 0.5269913077354431
[2025-12-23 14:47:20,250] m-LoRA: Adapter lora_gsm8k_17 loss: 1.0545200109481812
[2025-12-23 14:47:20,583] m-LoRA: Adapter lora_gsm8k_19 loss: 1.7356631755828857
[2025-12-23 14:47:20,949] m-LoRA: Adapter lora_gsm8k_20 loss: 1.4086805582046509
[2025-12-23 14:47:21,095] m-LoRA: Adapter lora_gsm8k_18 epoch: 1/1 iteration: 48/128 step: 7
[2025-12-23 14:47:21,239] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 72/128 step: 37
[2025-12-23 14:47:21,859] m-LoRA: Adapter lora_gsm8k_17 epoch: 1/1 iteration: 104/128 step: 14
[2025-12-23 14:47:22,554] m-LoRA: Adapter lora_gsm8k_19 epoch: 1/1 iteration: 24/128 step: 7
[2025-12-23 14:47:22,595] m-LoRA: Adapter lora_gsm8k_18 loss: 1.3178207874298096
[2025-12-23 14:47:22,773] m-LoRA: Adapter lora_gsm8k_16 loss: 0.5582029223442078
[2025-12-23 14:47:22,776] m-LoRA: Adapter lora_gsm8k_20 epoch: 1/1 iteration: 20/128 step: 6
[2025-12-23 14:47:23,697] m-LoRA: Adapter lora_gsm8k_17 loss: 0.8516204953193665
[2025-12-23 14:47:24,050] m-LoRA: Adapter lora_gsm8k_18 epoch: 1/1 iteration: 56/128 step: 8
[2025-12-23 14:47:24,128] m-LoRA: Adapter lora_gsm8k_19 loss: 1.528310775756836
[2025-12-23 14:47:24,379] m-LoRA: Adapter lora_gsm8k_20 loss: 1.4160345792770386
[2025-12-23 14:47:24,384] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 74/128 step: 38
[2025-12-23 14:47:25,767] m-LoRA: Adapter lora_gsm8k_17 epoch: 1/1 iteration: 112/128 step: 15
[2025-12-23 14:47:25,925] m-LoRA: Adapter lora_gsm8k_18 loss: 1.4862831830978394
[2025-12-23 14:47:25,931] m-LoRA: Adapter lora_gsm8k_19 epoch: 1/1 iteration: 28/128 step: 8
[2025-12-23 14:47:26,112] m-LoRA: Adapter lora_gsm8k_16 loss: 0.7155771255493164
[2025-12-23 14:47:26,114] m-LoRA: Adapter lora_gsm8k_20 epoch: 1/1 iteration: 24/128 step: 7
[2025-12-23 14:47:26,929] m-LoRA: Adapter lora_gsm8k_17 loss: 1.087301254272461
[2025-12-23 14:47:27,349] m-LoRA: Adapter lora_gsm8k_19 loss: 1.758239507675171
[2025-12-23 14:47:27,658] m-LoRA: Adapter lora_gsm8k_20 loss: 1.3560773134231567
[2025-12-23 14:47:27,662] m-LoRA: Adapter lora_gsm8k_18 epoch: 1/1 iteration: 64/128 step: 9
[2025-12-23 14:47:27,879] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 76/128 step: 39
[2025-12-23 14:47:28,232] m-LoRA: Adapter lora_gsm8k_17 epoch: 1/1 iteration: 120/128 step: 16
[2025-12-23 14:47:29,189] m-LoRA: Adapter lora_gsm8k_19 epoch: 1/1 iteration: 32/128 step: 9
[2025-12-23 14:47:29,296] m-LoRA: Adapter lora_gsm8k_18 loss: 1.1549488306045532
[2025-12-23 14:47:29,304] m-LoRA: Adapter lora_gsm8k_20 epoch: 1/1 iteration: 28/128 step: 8
[2025-12-23 14:47:29,351] m-LoRA: Adapter lora_gsm8k_16 loss: 0.5470284223556519
[2025-12-23 14:47:29,849] m-LoRA: Adapter lora_gsm8k_17 loss: 1.1532565355300903
[2025-12-23 14:47:30,265] m-LoRA: Adapter lora_gsm8k_19 loss: 1.1954200267791748
[2025-12-23 14:47:30,553] m-LoRA: Adapter lora_gsm8k_20 loss: 1.3790901899337769
[2025-12-23 14:47:31,026] m-LoRA: Adapter lora_gsm8k_18 epoch: 1/1 iteration: 72/128 step: 10
[2025-12-23 14:47:31,226] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 78/128 step: 40
[2025-12-23 14:47:31,650] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_17']
[2025-12-23 14:47:31,891] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_21']
[2025-12-23 14:47:32,070] m-LoRA: Adapter lora_gsm8k_21 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:47:32,656] m-LoRA: Adapter lora_gsm8k_19 epoch: 1/1 iteration: 36/128 step: 10
[2025-12-23 14:47:32,819] m-LoRA: Adapter lora_gsm8k_18 loss: 1.186080813407898
[2025-12-23 14:47:32,827] m-LoRA: Adapter lora_gsm8k_20 epoch: 1/1 iteration: 32/128 step: 9
[2025-12-23 14:47:32,875] m-LoRA: Adapter lora_gsm8k_16 loss: 0.5771296620368958
[2025-12-23 14:47:33,312] m-LoRA: Adapter lora_gsm8k_21 loss: 2.443626642227173
[2025-12-23 14:47:33,572] m-LoRA: Adapter lora_gsm8k_19 loss: 1.5523625612258911
[2025-12-23 14:47:33,908] m-LoRA: Adapter lora_gsm8k_20 loss: 1.0303246974945068
[2025-12-23 14:47:34,347] m-LoRA: Adapter lora_gsm8k_18 epoch: 1/1 iteration: 80/128 step: 11
[2025-12-23 14:47:34,505] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 80/128 step: 41
[2025-12-23 14:47:34,820] m-LoRA: Adapter lora_gsm8k_21 epoch: 1/1 iteration: 4/128 step: 2
[2025-12-23 14:47:34,973] m-LoRA: Adapter lora_gsm8k_19 epoch: 1/1 iteration: 40/128 step: 11
[2025-12-23 14:47:35,696] m-LoRA: Adapter lora_gsm8k_20 epoch: 1/1 iteration: 36/128 step: 10
[2025-12-23 14:47:35,781] m-LoRA: Adapter lora_gsm8k_18 loss: 1.1113293170928955
[2025-12-23 14:47:35,789] m-LoRA: Adapter lora_gsm8k_16 loss: 0.6356930136680603
[2025-12-23 14:47:36,217] m-LoRA: Adapter lora_gsm8k_21 loss: 1.3036410808563232
[2025-12-23 14:47:36,410] m-LoRA: Adapter lora_gsm8k_19 loss: 1.9054324626922607
[2025-12-23 14:47:36,800] m-LoRA: Adapter lora_gsm8k_20 loss: 0.8307020664215088
[2025-12-23 14:47:37,207] m-LoRA: Adapter lora_gsm8k_18 epoch: 1/1 iteration: 88/128 step: 12
[2025-12-23 14:47:37,319] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 82/128 step: 42
[2025-12-23 14:47:37,532] m-LoRA: Adapter lora_gsm8k_21 epoch: 1/1 iteration: 8/128 step: 3
[2025-12-23 14:47:37,721] m-LoRA: Adapter lora_gsm8k_19 epoch: 1/1 iteration: 44/128 step: 12
[2025-12-23 14:47:38,591] m-LoRA: Adapter lora_gsm8k_20 epoch: 1/1 iteration: 40/128 step: 11
[2025-12-23 14:47:38,634] m-LoRA: Adapter lora_gsm8k_18 loss: 1.3008716106414795
[2025-12-23 14:47:38,791] m-LoRA: Adapter lora_gsm8k_16 loss: 0.5191194415092468
[2025-12-23 14:47:39,248] m-LoRA: Adapter lora_gsm8k_21 loss: 1.4075417518615723
[2025-12-23 14:47:39,528] m-LoRA: Adapter lora_gsm8k_19 loss: 1.4266455173492432
[2025-12-23 14:47:39,859] m-LoRA: Adapter lora_gsm8k_20 loss: 1.0423599481582642
[2025-12-23 14:47:39,989] m-LoRA: Adapter lora_gsm8k_18 epoch: 1/1 iteration: 96/128 step: 13
[2025-12-23 14:47:40,124] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 84/128 step: 43
[2025-12-23 14:47:40,422] m-LoRA: Adapter lora_gsm8k_21 epoch: 1/1 iteration: 12/128 step: 4
[2025-12-23 14:47:41,332] m-LoRA: Adapter lora_gsm8k_19 epoch: 1/1 iteration: 48/128 step: 13
[2025-12-23 14:47:41,497] m-LoRA: Adapter lora_gsm8k_18 loss: 1.0253851413726807
[2025-12-23 14:47:41,504] m-LoRA: Adapter lora_gsm8k_20 epoch: 1/1 iteration: 44/128 step: 12
[2025-12-23 14:47:41,549] m-LoRA: Adapter lora_gsm8k_16 loss: 0.4925350844860077
[2025-12-23 14:47:41,788] m-LoRA: Adapter lora_gsm8k_21 loss: 1.73610520362854
[2025-12-23 14:47:41,993] m-LoRA: Adapter lora_gsm8k_19 loss: 1.8870594501495361
[2025-12-23 14:47:42,245] m-LoRA: Adapter lora_gsm8k_20 loss: 0.8830060362815857
[2025-12-23 14:47:43,039] m-LoRA: Adapter lora_gsm8k_18 epoch: 1/1 iteration: 104/128 step: 14
[2025-12-23 14:47:43,151] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 86/128 step: 44
[2025-12-23 14:47:43,248] m-LoRA: Adapter lora_gsm8k_21 epoch: 1/1 iteration: 16/128 step: 5
[2025-12-23 14:47:43,348] m-LoRA: Adapter lora_gsm8k_19 epoch: 1/1 iteration: 52/128 step: 14
[2025-12-23 14:47:43,585] m-LoRA: Adapter lora_gsm8k_20 epoch: 1/1 iteration: 48/128 step: 13
[2025-12-23 14:47:44,298] m-LoRA: Adapter lora_gsm8k_18 loss: 1.1296275854110718
[2025-12-23 14:47:44,314] m-LoRA: Adapter lora_gsm8k_16 loss: 0.5974779725074768
[2025-12-23 14:47:44,706] m-LoRA: Adapter lora_gsm8k_21 loss: 1.4890570640563965
[2025-12-23 14:47:45,036] m-LoRA: Adapter lora_gsm8k_19 loss: 1.2818959951400757
[2025-12-23 14:47:45,413] m-LoRA: Adapter lora_gsm8k_20 loss: 0.7726038098335266
[2025-12-23 14:47:45,687] m-LoRA: Adapter lora_gsm8k_18 epoch: 1/1 iteration: 112/128 step: 15
[2025-12-23 14:47:45,805] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 88/128 step: 45
[2025-12-23 14:47:45,942] m-LoRA: Adapter lora_gsm8k_21 epoch: 1/1 iteration: 20/128 step: 6
[2025-12-23 14:47:46,257] m-LoRA: Adapter lora_gsm8k_19 epoch: 1/1 iteration: 56/128 step: 15
[2025-12-23 14:47:47,376] m-LoRA: Adapter lora_gsm8k_20 epoch: 1/1 iteration: 52/128 step: 14
[2025-12-23 14:47:47,459] m-LoRA: Adapter lora_gsm8k_18 loss: 1.0664011240005493
[2025-12-23 14:47:47,467] m-LoRA: Adapter lora_gsm8k_16 loss: 0.6269115209579468
[2025-12-23 14:47:47,821] m-LoRA: Adapter lora_gsm8k_21 loss: 1.3899543285369873
[2025-12-23 14:47:48,146] m-LoRA: Adapter lora_gsm8k_19 loss: 1.5168249607086182
[2025-12-23 14:47:48,589] m-LoRA: Adapter lora_gsm8k_20 loss: 0.5756872892379761
[2025-12-23 14:47:48,888] m-LoRA: Adapter lora_gsm8k_18 epoch: 1/1 iteration: 120/128 step: 16
[2025-12-23 14:47:48,973] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 90/128 step: 46
[2025-12-23 14:47:49,141] m-LoRA: Adapter lora_gsm8k_21 epoch: 1/1 iteration: 24/128 step: 7
[2025-12-23 14:47:49,387] m-LoRA: Adapter lora_gsm8k_19 epoch: 1/1 iteration: 60/128 step: 16
[2025-12-23 14:47:50,274] m-LoRA: Adapter lora_gsm8k_20 epoch: 1/1 iteration: 56/128 step: 15
[2025-12-23 14:47:50,354] m-LoRA: Adapter lora_gsm8k_18 loss: 1.1249223947525024
[2025-12-23 14:47:50,361] m-LoRA: Adapter lora_gsm8k_16 loss: 0.532261848449707
[2025-12-23 14:47:50,679] m-LoRA: Adapter lora_gsm8k_21 loss: 1.6206003427505493
[2025-12-23 14:47:50,946] m-LoRA: Adapter lora_gsm8k_19 loss: 1.6774933338165283
[2025-12-23 14:47:51,254] m-LoRA: Adapter lora_gsm8k_20 loss: 0.6259772777557373
[2025-12-23 14:47:51,628] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_18']
[2025-12-23 14:47:51,787] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_22']
[2025-12-23 14:47:51,829] m-LoRA: Adapter lora_gsm8k_22 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:47:51,940] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 92/128 step: 47
[2025-12-23 14:47:52,005] m-LoRA: Adapter lora_gsm8k_21 epoch: 1/1 iteration: 28/128 step: 8
[2025-12-23 14:47:52,143] m-LoRA: Adapter lora_gsm8k_19 epoch: 1/1 iteration: 64/128 step: 17
[2025-12-23 14:47:52,410] m-LoRA: Adapter lora_gsm8k_20 epoch: 1/1 iteration: 60/128 step: 16
[2025-12-23 14:47:52,958] m-LoRA: Adapter lora_gsm8k_22 loss: 2.097363233566284
[2025-12-23 14:47:53,063] m-LoRA: Adapter lora_gsm8k_16 loss: 0.6880903244018555
[2025-12-23 14:47:53,333] m-LoRA: Adapter lora_gsm8k_21 loss: 1.699838638305664
[2025-12-23 14:47:53,733] m-LoRA: Adapter lora_gsm8k_19 loss: 1.3742210865020752
[2025-12-23 14:47:54,172] m-LoRA: Adapter lora_gsm8k_20 loss: 0.6664032340049744
[2025-12-23 14:47:54,176] m-LoRA: Adapter lora_gsm8k_22 epoch: 1/1 iteration: 4/128 step: 2
[2025-12-23 14:47:54,313] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 94/128 step: 48
[2025-12-23 14:47:54,439] m-LoRA: Adapter lora_gsm8k_21 epoch: 1/1 iteration: 32/128 step: 9
[2025-12-23 14:47:54,715] m-LoRA: Adapter lora_gsm8k_19 epoch: 1/1 iteration: 68/128 step: 18
[2025-12-23 14:47:55,338] m-LoRA: Adapter lora_gsm8k_20 epoch: 1/1 iteration: 64/128 step: 17
[2025-12-23 14:47:55,380] m-LoRA: Adapter lora_gsm8k_22 loss: 1.9496335983276367
[2025-12-23 14:47:55,621] m-LoRA: Adapter lora_gsm8k_16 loss: 0.7419906854629517
[2025-12-23 14:47:55,880] m-LoRA: Adapter lora_gsm8k_21 loss: 1.1403623819351196
[2025-12-23 14:47:56,253] m-LoRA: Adapter lora_gsm8k_19 loss: 1.5088169574737549
[2025-12-23 14:47:56,590] m-LoRA: Adapter lora_gsm8k_20 loss: 0.5864526629447937
[2025-12-23 14:47:56,595] m-LoRA: Adapter lora_gsm8k_22 epoch: 1/1 iteration: 8/128 step: 3
[2025-12-23 14:47:56,758] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 96/128 step: 49
[2025-12-23 14:47:56,846] m-LoRA: Adapter lora_gsm8k_21 epoch: 1/1 iteration: 36/128 step: 10
[2025-12-23 14:47:57,040] m-LoRA: Adapter lora_gsm8k_19 epoch: 1/1 iteration: 72/128 step: 19
[2025-12-23 14:47:57,648] m-LoRA: Adapter lora_gsm8k_22 loss: 1.6057318449020386
[2025-12-23 14:47:57,652] m-LoRA: Adapter lora_gsm8k_20 epoch: 1/1 iteration: 68/128 step: 18
[2025-12-23 14:47:57,769] m-LoRA: Adapter lora_gsm8k_16 loss: 0.7601794004440308
[2025-12-23 14:47:58,160] m-LoRA: Adapter lora_gsm8k_21 loss: 1.128847360610962
[2025-12-23 14:47:58,503] m-LoRA: Adapter lora_gsm8k_19 loss: 1.6319667100906372
[2025-12-23 14:47:58,837] m-LoRA: Adapter lora_gsm8k_20 loss: 0.5847355723381042
[2025-12-23 14:47:58,841] m-LoRA: Adapter lora_gsm8k_22 epoch: 1/1 iteration: 12/128 step: 4
[2025-12-23 14:47:59,000] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 98/128 step: 50
[2025-12-23 14:47:59,090] m-LoRA: Adapter lora_gsm8k_21 epoch: 1/1 iteration: 40/128 step: 11
[2025-12-23 14:47:59,311] m-LoRA: Adapter lora_gsm8k_19 epoch: 1/1 iteration: 76/128 step: 20
[2025-12-23 14:47:59,848] m-LoRA: Adapter lora_gsm8k_20 epoch: 1/1 iteration: 72/128 step: 19
[2025-12-23 14:47:59,886] m-LoRA: Adapter lora_gsm8k_22 loss: 1.640376091003418
[2025-12-23 14:48:00,103] m-LoRA: Adapter lora_gsm8k_16 loss: 0.8072512149810791
[2025-12-23 14:48:00,491] m-LoRA: Adapter lora_gsm8k_21 loss: 0.9684408903121948
[2025-12-23 14:48:00,827] m-LoRA: Adapter lora_gsm8k_19 loss: 1.5367873907089233
[2025-12-23 14:48:00,831] m-LoRA: Adapter lora_gsm8k_22 epoch: 1/1 iteration: 16/128 step: 5
[2025-12-23 14:48:01,127] m-LoRA: Adapter lora_gsm8k_20 loss: 0.5795111656188965
[2025-12-23 14:48:01,130] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 100/128 step: 51
[2025-12-23 14:48:01,398] m-LoRA: Adapter lora_gsm8k_21 epoch: 1/1 iteration: 44/128 step: 12
[2025-12-23 14:48:01,948] m-LoRA: Adapter lora_gsm8k_19 epoch: 1/1 iteration: 80/128 step: 21
[2025-12-23 14:48:01,988] m-LoRA: Adapter lora_gsm8k_22 loss: 1.2953062057495117
[2025-12-23 14:48:02,189] m-LoRA: Adapter lora_gsm8k_16 loss: 0.6617236137390137
[2025-12-23 14:48:02,193] m-LoRA: Adapter lora_gsm8k_20 epoch: 1/1 iteration: 76/128 step: 20
[2025-12-23 14:48:02,551] m-LoRA: Adapter lora_gsm8k_21 loss: 0.8349730968475342
[2025-12-23 14:48:02,996] m-LoRA: Adapter lora_gsm8k_19 loss: 1.314408779144287
[2025-12-23 14:48:03,336] m-LoRA: Adapter lora_gsm8k_20 loss: 0.6122406721115112
[2025-12-23 14:48:03,340] m-LoRA: Adapter lora_gsm8k_22 epoch: 1/1 iteration: 20/128 step: 6
[2025-12-23 14:48:03,468] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 102/128 step: 52
[2025-12-23 14:48:03,626] m-LoRA: Adapter lora_gsm8k_21 epoch: 1/1 iteration: 48/128 step: 13
[2025-12-23 14:48:04,179] m-LoRA: Adapter lora_gsm8k_22 loss: 1.2645270824432373
[2025-12-23 14:48:04,184] m-LoRA: Adapter lora_gsm8k_19 epoch: 1/1 iteration: 84/128 step: 22
[2025-12-23 14:48:04,405] m-LoRA: Adapter lora_gsm8k_16 loss: 0.5219426155090332
[2025-12-23 14:48:04,408] m-LoRA: Adapter lora_gsm8k_20 epoch: 1/1 iteration: 80/128 step: 21
[2025-12-23 14:48:04,585] m-LoRA: Adapter lora_gsm8k_21 loss: 0.985862672328949
[2025-12-23 14:48:05,378] m-LoRA: Adapter lora_gsm8k_19 loss: 1.2219641208648682
[2025-12-23 14:48:05,383] m-LoRA: Adapter lora_gsm8k_22 epoch: 1/1 iteration: 24/128 step: 7
[2025-12-23 14:48:05,636] m-LoRA: Adapter lora_gsm8k_20 loss: 0.5882383584976196
[2025-12-23 14:48:05,639] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 104/128 step: 53
[2025-12-23 14:48:05,781] m-LoRA: Adapter lora_gsm8k_21 epoch: 1/1 iteration: 52/128 step: 14
[2025-12-23 14:48:06,516] m-LoRA: Adapter lora_gsm8k_22 loss: 1.4521487951278687
[2025-12-23 14:48:06,519] m-LoRA: Adapter lora_gsm8k_19 epoch: 1/1 iteration: 88/128 step: 23
[2025-12-23 14:48:06,746] m-LoRA: Adapter lora_gsm8k_16 loss: 0.5999242067337036
[2025-12-23 14:48:06,749] m-LoRA: Adapter lora_gsm8k_20 epoch: 1/1 iteration: 84/128 step: 22
[2025-12-23 14:48:07,019] m-LoRA: Adapter lora_gsm8k_21 loss: 0.8691416382789612
[2025-12-23 14:48:07,618] m-LoRA: Adapter lora_gsm8k_19 loss: 1.4699667692184448
[2025-12-23 14:48:07,623] m-LoRA: Adapter lora_gsm8k_22 epoch: 1/1 iteration: 28/128 step: 8
[2025-12-23 14:48:07,990] m-LoRA: Adapter lora_gsm8k_20 loss: 0.48541510105133057
[2025-12-23 14:48:07,994] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 106/128 step: 54
[2025-12-23 14:48:08,144] m-LoRA: Adapter lora_gsm8k_21 epoch: 1/1 iteration: 56/128 step: 15
[2025-12-23 14:48:08,739] m-LoRA: Adapter lora_gsm8k_22 loss: 1.6715598106384277
[2025-12-23 14:48:08,743] m-LoRA: Adapter lora_gsm8k_19 epoch: 1/1 iteration: 92/128 step: 24
[2025-12-23 14:48:08,828] m-LoRA: Adapter lora_gsm8k_16 loss: 0.5401854515075684
[2025-12-23 14:48:09,340] m-LoRA: Adapter lora_gsm8k_21 loss: 0.8203269243240356
[2025-12-23 14:48:09,474] m-LoRA: Adapter lora_gsm8k_20 epoch: 1/1 iteration: 88/128 step: 23
[2025-12-23 14:48:09,720] m-LoRA: Adapter lora_gsm8k_19 loss: 1.4717686176300049
[2025-12-23 14:48:09,723] m-LoRA: Adapter lora_gsm8k_22 epoch: 1/1 iteration: 32/128 step: 9
[2025-12-23 14:48:09,947] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 108/128 step: 55
[2025-12-23 14:48:10,375] m-LoRA: Adapter lora_gsm8k_20 loss: 0.5576801896095276
[2025-12-23 14:48:10,379] m-LoRA: Adapter lora_gsm8k_21 epoch: 1/1 iteration: 60/128 step: 16
[2025-12-23 14:48:10,846] m-LoRA: Adapter lora_gsm8k_22 loss: 1.3894586563110352
[2025-12-23 14:48:10,850] m-LoRA: Adapter lora_gsm8k_19 epoch: 1/1 iteration: 96/128 step: 25
[2025-12-23 14:48:11,035] m-LoRA: Adapter lora_gsm8k_16 loss: 0.5304309725761414
[2025-12-23 14:48:11,464] m-LoRA: Adapter lora_gsm8k_21 loss: 0.7133308053016663
[2025-12-23 14:48:11,469] m-LoRA: Adapter lora_gsm8k_20 epoch: 1/1 iteration: 92/128 step: 24
[2025-12-23 14:48:12,121] m-LoRA: Adapter lora_gsm8k_19 loss: 1.2116823196411133
[2025-12-23 14:48:12,125] m-LoRA: Adapter lora_gsm8k_22 epoch: 1/1 iteration: 36/128 step: 10
[2025-12-23 14:48:12,282] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 110/128 step: 56
[2025-12-23 14:48:12,722] m-LoRA: Adapter lora_gsm8k_20 loss: 0.4688451290130615
[2025-12-23 14:48:12,726] m-LoRA: Adapter lora_gsm8k_21 epoch: 1/1 iteration: 64/128 step: 17
[2025-12-23 14:48:12,981] m-LoRA: Adapter lora_gsm8k_22 loss: 1.4358549118041992
[2025-12-23 14:48:13,189] m-LoRA: Adapter lora_gsm8k_16 loss: 0.5714300274848938
[2025-12-23 14:48:13,191] m-LoRA: Adapter lora_gsm8k_19 epoch: 1/1 iteration: 100/128 step: 26
[2025-12-23 14:48:13,509] m-LoRA: Adapter lora_gsm8k_21 loss: 0.5650632977485657
[2025-12-23 14:48:13,694] m-LoRA: Adapter lora_gsm8k_20 epoch: 1/1 iteration: 96/128 step: 25
[2025-12-23 14:48:14,065] m-LoRA: Adapter lora_gsm8k_22 epoch: 1/1 iteration: 40/128 step: 11
[2025-12-23 14:48:14,156] m-LoRA: Adapter lora_gsm8k_19 loss: 1.7267649173736572
[2025-12-23 14:48:14,159] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 112/128 step: 57
[2025-12-23 14:48:14,744] m-LoRA: Adapter lora_gsm8k_20 loss: 0.6857179999351501
[2025-12-23 14:48:14,749] m-LoRA: Adapter lora_gsm8k_21 epoch: 1/1 iteration: 68/128 step: 18
[2025-12-23 14:48:15,087] m-LoRA: Adapter lora_gsm8k_22 loss: 1.5027333498001099
[2025-12-23 14:48:15,091] m-LoRA: Adapter lora_gsm8k_19 epoch: 1/1 iteration: 104/128 step: 27
[2025-12-23 14:48:15,219] m-LoRA: Adapter lora_gsm8k_16 loss: 0.4638359248638153
[2025-12-23 14:48:15,722] m-LoRA: Adapter lora_gsm8k_21 loss: 0.5003272294998169
[2025-12-23 14:48:15,725] m-LoRA: Adapter lora_gsm8k_20 epoch: 1/1 iteration: 100/128 step: 26
[2025-12-23 14:48:16,247] m-LoRA: Adapter lora_gsm8k_19 loss: 1.1970888376235962
[2025-12-23 14:48:16,251] m-LoRA: Adapter lora_gsm8k_22 epoch: 1/1 iteration: 44/128 step: 12
[2025-12-23 14:48:16,415] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 114/128 step: 58
[2025-12-23 14:48:16,620] m-LoRA: Adapter lora_gsm8k_21 epoch: 1/1 iteration: 72/128 step: 19
[2025-12-23 14:48:16,662] m-LoRA: Adapter lora_gsm8k_20 loss: 0.5770124793052673
[2025-12-23 14:48:17,399] m-LoRA: Adapter lora_gsm8k_22 loss: 1.2008455991744995
[2025-12-23 14:48:17,404] m-LoRA: Adapter lora_gsm8k_19 epoch: 1/1 iteration: 108/128 step: 28
[2025-12-23 14:48:17,614] m-LoRA: Adapter lora_gsm8k_16 loss: 0.4804704487323761
[2025-12-23 14:48:17,617] m-LoRA: Adapter lora_gsm8k_20 epoch: 1/1 iteration: 104/128 step: 27
[2025-12-23 14:48:17,968] m-LoRA: Adapter lora_gsm8k_21 loss: 0.5413652062416077
[2025-12-23 14:48:18,572] m-LoRA: Adapter lora_gsm8k_19 loss: 1.4152170419692993
[2025-12-23 14:48:18,576] m-LoRA: Adapter lora_gsm8k_22 epoch: 1/1 iteration: 48/128 step: 13
[2025-12-23 14:48:18,848] m-LoRA: Adapter lora_gsm8k_20 loss: 0.4525843858718872
[2025-12-23 14:48:18,852] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 116/128 step: 59
[2025-12-23 14:48:18,999] m-LoRA: Adapter lora_gsm8k_21 epoch: 1/1 iteration: 76/128 step: 20
[2025-12-23 14:48:19,537] m-LoRA: Adapter lora_gsm8k_22 loss: 1.3225833177566528
[2025-12-23 14:48:19,541] m-LoRA: Adapter lora_gsm8k_19 epoch: 1/1 iteration: 112/128 step: 29
[2025-12-23 14:48:19,766] m-LoRA: Adapter lora_gsm8k_16 loss: 0.5808313488960266
[2025-12-23 14:48:20,266] m-LoRA: Adapter lora_gsm8k_21 loss: 0.5423932671546936
[2025-12-23 14:48:20,270] m-LoRA: Adapter lora_gsm8k_20 epoch: 1/1 iteration: 108/128 step: 28
[2025-12-23 14:48:20,534] m-LoRA: Adapter lora_gsm8k_19 loss: 1.5762120485305786
[2025-12-23 14:48:20,537] m-LoRA: Adapter lora_gsm8k_22 epoch: 1/1 iteration: 52/128 step: 14
[2025-12-23 14:48:20,639] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 118/128 step: 60
[2025-12-23 14:48:21,523] m-LoRA: Adapter lora_gsm8k_21 epoch: 1/1 iteration: 80/128 step: 21
[2025-12-23 14:48:21,586] m-LoRA: Adapter lora_gsm8k_20 loss: 0.5099972486495972
[2025-12-23 14:48:21,886] m-LoRA: Adapter lora_gsm8k_22 loss: 1.1550785303115845
[2025-12-23 14:48:21,891] m-LoRA: Adapter lora_gsm8k_19 epoch: 1/1 iteration: 116/128 step: 30
[2025-12-23 14:48:22,038] m-LoRA: Adapter lora_gsm8k_16 loss: 0.6151435375213623
[2025-12-23 14:48:22,482] m-LoRA: Adapter lora_gsm8k_21 loss: 0.6454375386238098
[2025-12-23 14:48:22,843] m-LoRA: Adapter lora_gsm8k_19 loss: 1.396559476852417
[2025-12-23 14:48:22,847] m-LoRA: Adapter lora_gsm8k_20 epoch: 1/1 iteration: 112/128 step: 29
[2025-12-23 14:48:22,996] m-LoRA: Adapter lora_gsm8k_22 epoch: 1/1 iteration: 56/128 step: 15
[2025-12-23 14:48:23,102] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 120/128 step: 61
[2025-12-23 14:48:23,672] m-LoRA: Adapter lora_gsm8k_21 epoch: 1/1 iteration: 84/128 step: 22
[2025-12-23 14:48:23,731] m-LoRA: Adapter lora_gsm8k_20 loss: 0.6231369972229004
[2025-12-23 14:48:24,361] m-LoRA: Adapter lora_gsm8k_22 loss: 1.1237990856170654
[2025-12-23 14:48:24,364] m-LoRA: Adapter lora_gsm8k_19 epoch: 1/1 iteration: 120/128 step: 31
[2025-12-23 14:48:24,448] m-LoRA: Adapter lora_gsm8k_16 loss: 0.6266443729400635
[2025-12-23 14:48:25,059] m-LoRA: Adapter lora_gsm8k_21 loss: 0.5467708110809326
[2025-12-23 14:48:25,063] m-LoRA: Adapter lora_gsm8k_20 epoch: 1/1 iteration: 116/128 step: 30
[2025-12-23 14:48:25,427] m-LoRA: Adapter lora_gsm8k_19 loss: 1.4302345514297485
[2025-12-23 14:48:25,430] m-LoRA: Adapter lora_gsm8k_22 epoch: 1/1 iteration: 60/128 step: 16
[2025-12-23 14:48:25,573] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 122/128 step: 62
[2025-12-23 14:48:26,102] m-LoRA: Adapter lora_gsm8k_20 loss: 0.4535498321056366
[2025-12-23 14:48:26,107] m-LoRA: Adapter lora_gsm8k_21 epoch: 1/1 iteration: 88/128 step: 23
[2025-12-23 14:48:26,579] m-LoRA: Adapter lora_gsm8k_22 loss: 1.0549640655517578
[2025-12-23 14:48:26,584] m-LoRA: Adapter lora_gsm8k_19 epoch: 1/1 iteration: 124/128 step: 32
[2025-12-23 14:48:26,678] m-LoRA: Adapter lora_gsm8k_16 loss: 0.5430165529251099
[2025-12-23 14:48:27,090] m-LoRA: Adapter lora_gsm8k_21 loss: 0.7288826704025269
[2025-12-23 14:48:27,093] m-LoRA: Adapter lora_gsm8k_20 epoch: 1/1 iteration: 120/128 step: 31
[2025-12-23 14:48:27,455] m-LoRA: Adapter lora_gsm8k_19 loss: 1.4624989032745361
[2025-12-23 14:48:27,599] m-LoRA: Adapter lora_gsm8k_22 epoch: 1/1 iteration: 64/128 step: 17
[2025-12-23 14:48:27,682] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 124/128 step: 63
[2025-12-23 14:48:28,200] m-LoRA: Adapter lora_gsm8k_21 epoch: 1/1 iteration: 92/128 step: 24
[2025-12-23 14:48:28,247] m-LoRA: Adapter lora_gsm8k_20 loss: 0.5446168780326843
[2025-12-23 14:48:28,460] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_19']
[2025-12-23 14:48:28,895] m-LoRA: Adapter lora_gsm8k_22 loss: 1.0931057929992676
[2025-12-23 14:48:28,899] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_23']
[2025-12-23 14:48:29,138] m-LoRA: Adapter lora_gsm8k_23 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:48:29,167] m-LoRA: Adapter lora_gsm8k_16 loss: 0.5990745425224304
[2025-12-23 14:48:29,839] m-LoRA: Adapter lora_gsm8k_21 loss: 0.724412202835083
[2025-12-23 14:48:29,843] m-LoRA: Adapter lora_gsm8k_20 epoch: 1/1 iteration: 124/128 step: 32
[2025-12-23 14:48:30,008] m-LoRA: Adapter lora_gsm8k_22 epoch: 1/1 iteration: 68/128 step: 18
[2025-12-23 14:48:30,290] m-LoRA: Adapter lora_gsm8k_16 epoch: 1/1 iteration: 126/128 step: 64
[2025-12-23 14:48:30,331] m-LoRA: Adapter lora_gsm8k_23 loss: 2.3536078929901123
[2025-12-23 14:48:30,984] m-LoRA: Adapter lora_gsm8k_21 epoch: 1/1 iteration: 96/128 step: 25
[2025-12-23 14:48:31,015] m-LoRA: Adapter lora_gsm8k_20 loss: 0.5139842629432678
[2025-12-23 14:48:31,414] m-LoRA: Adapter lora_gsm8k_22 loss: 0.8906803131103516
[2025-12-23 14:48:31,418] m-LoRA: Adapter lora_gsm8k_23 epoch: 1/1 iteration: 4/128 step: 2
[2025-12-23 14:48:31,458] m-LoRA: Adapter lora_gsm8k_16 loss: 0.5038565993309021
[2025-12-23 14:48:32,227] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_20']
[2025-12-23 14:48:32,549] m-LoRA: Adapter lora_gsm8k_21 loss: 0.6986494660377502
[2025-12-23 14:48:32,553] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_24']
[2025-12-23 14:48:32,799] m-LoRA: Adapter lora_gsm8k_24 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:48:32,919] m-LoRA: Adapter lora_gsm8k_23 loss: 1.3713295459747314
[2025-12-23 14:48:32,923] m-LoRA: Adapter lora_gsm8k_22 epoch: 1/1 iteration: 72/128 step: 19
[2025-12-23 14:48:33,091] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_16']
[2025-12-23 14:48:33,381] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_25']
[2025-12-23 14:48:33,564] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:48:34,031] m-LoRA: Adapter lora_gsm8k_21 epoch: 1/1 iteration: 100/128 step: 26
[2025-12-23 14:48:34,082] m-LoRA: Adapter lora_gsm8k_24 loss: 2.724918842315674
[2025-12-23 14:48:34,513] m-LoRA: Adapter lora_gsm8k_23 epoch: 1/1 iteration: 8/128 step: 3
[2025-12-23 14:48:34,571] m-LoRA: Adapter lora_gsm8k_22 loss: 0.9062418341636658
[2025-12-23 14:48:34,656] m-LoRA: Adapter lora_gsm8k_25 loss: 2.231538772583008
[2025-12-23 14:48:35,001] m-LoRA: Adapter lora_gsm8k_21 loss: 0.5946395397186279
[2025-12-23 14:48:35,381] m-LoRA: Adapter lora_gsm8k_23 loss: 1.5694739818572998
[2025-12-23 14:48:35,384] m-LoRA: Adapter lora_gsm8k_24 epoch: 1/1 iteration: 8/128 step: 2
[2025-12-23 14:48:35,750] m-LoRA: Adapter lora_gsm8k_22 epoch: 1/1 iteration: 76/128 step: 20
[2025-12-23 14:48:35,844] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 2/128 step: 2
[2025-12-23 14:48:36,682] m-LoRA: Adapter lora_gsm8k_21 epoch: 1/1 iteration: 104/128 step: 27
[2025-12-23 14:48:36,736] m-LoRA: Adapter lora_gsm8k_24 loss: 2.183685064315796
[2025-12-23 14:48:37,030] m-LoRA: Adapter lora_gsm8k_23 epoch: 1/1 iteration: 12/128 step: 4
[2025-12-23 14:48:37,065] m-LoRA: Adapter lora_gsm8k_22 loss: 0.8977621793746948
[2025-12-23 14:48:37,176] m-LoRA: Adapter lora_gsm8k_25 loss: 1.6973172426223755
[2025-12-23 14:48:37,476] m-LoRA: Adapter lora_gsm8k_21 loss: 0.4134116768836975
[2025-12-23 14:48:38,303] m-LoRA: Adapter lora_gsm8k_23 loss: 1.189301609992981
[2025-12-23 14:48:38,466] m-LoRA: Adapter lora_gsm8k_24 epoch: 1/1 iteration: 16/128 step: 3
[2025-12-23 14:48:38,599] m-LoRA: Adapter lora_gsm8k_22 epoch: 1/1 iteration: 80/128 step: 21
[2025-12-23 14:48:38,694] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 4/128 step: 3
[2025-12-23 14:48:38,981] m-LoRA: Adapter lora_gsm8k_21 epoch: 1/1 iteration: 108/128 step: 28
[2025-12-23 14:48:39,921] m-LoRA: Adapter lora_gsm8k_23 epoch: 1/1 iteration: 16/128 step: 5
[2025-12-23 14:48:39,976] m-LoRA: Adapter lora_gsm8k_24 loss: 2.1935384273529053
[2025-12-23 14:48:40,314] m-LoRA: Adapter lora_gsm8k_22 loss: 0.7658471465110779
[2025-12-23 14:48:40,429] m-LoRA: Adapter lora_gsm8k_25 loss: 1.6148632764816284
[2025-12-23 14:48:40,642] m-LoRA: Adapter lora_gsm8k_21 loss: 0.6096999049186707
[2025-12-23 14:48:40,871] m-LoRA: Adapter lora_gsm8k_23 loss: 1.1950281858444214
[2025-12-23 14:48:41,713] m-LoRA: Adapter lora_gsm8k_24 epoch: 1/1 iteration: 24/128 step: 4
[2025-12-23 14:48:41,854] m-LoRA: Adapter lora_gsm8k_22 epoch: 1/1 iteration: 84/128 step: 22
[2025-12-23 14:48:41,962] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 6/128 step: 4
[2025-12-23 14:48:42,056] m-LoRA: Adapter lora_gsm8k_21 epoch: 1/1 iteration: 112/128 step: 29
[2025-12-23 14:48:42,178] m-LoRA: Adapter lora_gsm8k_23 epoch: 1/1 iteration: 20/128 step: 6
[2025-12-23 14:48:43,002] m-LoRA: Adapter lora_gsm8k_24 loss: 2.202610492706299
[2025-12-23 14:48:43,268] m-LoRA: Adapter lora_gsm8k_22 loss: 0.890678882598877
[2025-12-23 14:48:43,348] m-LoRA: Adapter lora_gsm8k_25 loss: 1.5916134119033813
[2025-12-23 14:48:43,660] m-LoRA: Adapter lora_gsm8k_21 loss: 0.5312663316726685
[2025-12-23 14:48:43,973] m-LoRA: Adapter lora_gsm8k_23 loss: 1.0311955213546753
[2025-12-23 14:48:44,385] m-LoRA: Adapter lora_gsm8k_24 epoch: 1/1 iteration: 32/128 step: 5
[2025-12-23 14:48:44,591] m-LoRA: Adapter lora_gsm8k_22 epoch: 1/1 iteration: 88/128 step: 23
[2025-12-23 14:48:44,731] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 8/128 step: 5
[2025-12-23 14:48:44,943] m-LoRA: Adapter lora_gsm8k_21 epoch: 1/1 iteration: 116/128 step: 30
[2025-12-23 14:48:45,756] m-LoRA: Adapter lora_gsm8k_23 epoch: 1/1 iteration: 24/128 step: 7
[2025-12-23 14:48:45,795] m-LoRA: Adapter lora_gsm8k_24 loss: 2.0124168395996094
[2025-12-23 14:48:46,141] m-LoRA: Adapter lora_gsm8k_22 loss: 0.8393548727035522
[2025-12-23 14:48:46,243] m-LoRA: Adapter lora_gsm8k_25 loss: 1.2328861951828003
[2025-12-23 14:48:46,565] m-LoRA: Adapter lora_gsm8k_21 loss: 0.6741039752960205
[2025-12-23 14:48:46,863] m-LoRA: Adapter lora_gsm8k_23 loss: 0.9273039102554321
[2025-12-23 14:48:47,225] m-LoRA: Adapter lora_gsm8k_24 epoch: 1/1 iteration: 40/128 step: 6
[2025-12-23 14:48:47,473] m-LoRA: Adapter lora_gsm8k_22 epoch: 1/1 iteration: 92/128 step: 24
[2025-12-23 14:48:47,582] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 10/128 step: 6
[2025-12-23 14:48:47,739] m-LoRA: Adapter lora_gsm8k_21 epoch: 1/1 iteration: 120/128 step: 31
[2025-12-23 14:48:48,156] m-LoRA: Adapter lora_gsm8k_23 epoch: 1/1 iteration: 28/128 step: 8
[2025-12-23 14:48:49,046] m-LoRA: Adapter lora_gsm8k_24 loss: 1.4780604839324951
[2025-12-23 14:48:49,311] m-LoRA: Adapter lora_gsm8k_22 loss: 0.7398760318756104
[2025-12-23 14:48:49,433] m-LoRA: Adapter lora_gsm8k_25 loss: 1.311755895614624
[2025-12-23 14:48:49,738] m-LoRA: Adapter lora_gsm8k_21 loss: 0.6792473793029785
[2025-12-23 14:48:50,250] m-LoRA: Adapter lora_gsm8k_23 loss: 0.7412863969802856
[2025-12-23 14:48:50,794] m-LoRA: Adapter lora_gsm8k_24 epoch: 1/1 iteration: 48/128 step: 7
[2025-12-23 14:48:50,947] m-LoRA: Adapter lora_gsm8k_22 epoch: 1/1 iteration: 96/128 step: 25
[2025-12-23 14:48:51,041] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 12/128 step: 7
[2025-12-23 14:48:51,243] m-LoRA: Adapter lora_gsm8k_21 epoch: 1/1 iteration: 124/128 step: 32
[2025-12-23 14:48:52,212] m-LoRA: Adapter lora_gsm8k_23 epoch: 1/1 iteration: 32/128 step: 9
[2025-12-23 14:48:52,276] m-LoRA: Adapter lora_gsm8k_24 loss: 1.7619237899780273
[2025-12-23 14:48:52,569] m-LoRA: Adapter lora_gsm8k_22 loss: 0.7971929311752319
[2025-12-23 14:48:52,659] m-LoRA: Adapter lora_gsm8k_25 loss: 0.9848960638046265
[2025-12-23 14:48:52,993] m-LoRA: Adapter lora_gsm8k_21 loss: 0.440391480922699
[2025-12-23 14:48:53,297] m-LoRA: Adapter lora_gsm8k_23 loss: 0.7343748211860657
[2025-12-23 14:48:53,586] m-LoRA: Adapter lora_gsm8k_24 epoch: 1/1 iteration: 56/128 step: 8
[2025-12-23 14:48:53,800] m-LoRA: Adapter lora_gsm8k_22 epoch: 1/1 iteration: 100/128 step: 26
[2025-12-23 14:48:53,918] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 14/128 step: 8
[2025-12-23 14:48:54,195] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_21']
[2025-12-23 14:48:54,969] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_26']
[2025-12-23 14:48:55,125] m-LoRA: Adapter lora_gsm8k_26 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:48:55,154] m-LoRA: Adapter lora_gsm8k_24 loss: 1.7161173820495605
[2025-12-23 14:48:55,636] m-LoRA: Adapter lora_gsm8k_22 loss: 0.7613195776939392
[2025-12-23 14:48:55,639] m-LoRA: Adapter lora_gsm8k_23 epoch: 1/1 iteration: 36/128 step: 10
[2025-12-23 14:48:55,775] m-LoRA: Adapter lora_gsm8k_25 loss: 0.7492783665657043
[2025-12-23 14:48:56,515] m-LoRA: Adapter lora_gsm8k_26 loss: 2.2395503520965576
[2025-12-23 14:48:56,518] m-LoRA: Adapter lora_gsm8k_24 epoch: 1/1 iteration: 64/128 step: 9
[2025-12-23 14:48:56,839] m-LoRA: Adapter lora_gsm8k_23 loss: 0.7283425331115723
[2025-12-23 14:48:56,843] m-LoRA: Adapter lora_gsm8k_22 epoch: 1/1 iteration: 104/128 step: 27
[2025-12-23 14:48:57,057] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 16/128 step: 9
[2025-12-23 14:48:57,230] m-LoRA: Adapter lora_gsm8k_26 epoch: 1/1 iteration: 4/128 step: 2
[2025-12-23 14:48:57,807] m-LoRA: Adapter lora_gsm8k_24 loss: 1.6670984029769897
[2025-12-23 14:48:58,194] m-LoRA: Adapter lora_gsm8k_23 epoch: 1/1 iteration: 40/128 step: 11
[2025-12-23 14:48:58,248] m-LoRA: Adapter lora_gsm8k_22 loss: 0.6121198534965515
[2025-12-23 14:48:58,460] m-LoRA: Adapter lora_gsm8k_25 loss: 0.8739354610443115
[2025-12-23 14:48:58,865] m-LoRA: Adapter lora_gsm8k_26 loss: 1.5567898750305176
[2025-12-23 14:48:59,345] m-LoRA: Adapter lora_gsm8k_23 loss: 0.4700179100036621
[2025-12-23 14:48:59,349] m-LoRA: Adapter lora_gsm8k_24 epoch: 1/1 iteration: 72/128 step: 10
[2025-12-23 14:48:59,578] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 18/128 step: 10
[2025-12-23 14:48:59,633] m-LoRA: Adapter lora_gsm8k_22 epoch: 1/1 iteration: 108/128 step: 28
[2025-12-23 14:48:59,702] m-LoRA: Adapter lora_gsm8k_26 epoch: 1/1 iteration: 8/128 step: 3
[2025-12-23 14:49:00,830] m-LoRA: Adapter lora_gsm8k_23 epoch: 1/1 iteration: 44/128 step: 12
[2025-12-23 14:49:00,892] m-LoRA: Adapter lora_gsm8k_24 loss: 1.4606258869171143
[2025-12-23 14:49:00,911] m-LoRA: Adapter lora_gsm8k_25 loss: 0.7590199708938599
[2025-12-23 14:49:01,311] m-LoRA: Adapter lora_gsm8k_22 loss: 0.762496829032898
[2025-12-23 14:49:01,617] m-LoRA: Adapter lora_gsm8k_26 loss: 1.4640617370605469
[2025-12-23 14:49:01,898] m-LoRA: Adapter lora_gsm8k_23 loss: 0.612329363822937
[2025-12-23 14:49:02,324] m-LoRA: Adapter lora_gsm8k_24 epoch: 1/1 iteration: 80/128 step: 11
[2025-12-23 14:49:02,443] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 20/128 step: 11
[2025-12-23 14:49:02,542] m-LoRA: Adapter lora_gsm8k_22 epoch: 1/1 iteration: 112/128 step: 29
[2025-12-23 14:49:02,792] m-LoRA: Adapter lora_gsm8k_26 epoch: 1/1 iteration: 12/128 step: 4
[2025-12-23 14:49:03,669] m-LoRA: Adapter lora_gsm8k_23 epoch: 1/1 iteration: 48/128 step: 13
[2025-12-23 14:49:03,723] m-LoRA: Adapter lora_gsm8k_24 loss: 1.5601998567581177
[2025-12-23 14:49:03,869] m-LoRA: Adapter lora_gsm8k_25 loss: 0.5612989068031311
[2025-12-23 14:49:04,248] m-LoRA: Adapter lora_gsm8k_22 loss: 0.5701514482498169
[2025-12-23 14:49:04,662] m-LoRA: Adapter lora_gsm8k_26 loss: 1.247454285621643
[2025-12-23 14:49:05,058] m-LoRA: Adapter lora_gsm8k_23 loss: 0.6202694773674011
[2025-12-23 14:49:05,061] m-LoRA: Adapter lora_gsm8k_24 epoch: 1/1 iteration: 88/128 step: 12
[2025-12-23 14:49:05,208] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 22/128 step: 12
[2025-12-23 14:49:05,419] m-LoRA: Adapter lora_gsm8k_22 epoch: 1/1 iteration: 116/128 step: 30
[2025-12-23 14:49:06,436] m-LoRA: Adapter lora_gsm8k_26 epoch: 1/1 iteration: 16/128 step: 5
[2025-12-23 14:49:06,594] m-LoRA: Adapter lora_gsm8k_24 loss: 1.3613927364349365
[2025-12-23 14:49:06,600] m-LoRA: Adapter lora_gsm8k_23 epoch: 1/1 iteration: 52/128 step: 14
[2025-12-23 14:49:06,642] m-LoRA: Adapter lora_gsm8k_25 loss: 0.8202974200248718
[2025-12-23 14:49:06,888] m-LoRA: Adapter lora_gsm8k_22 loss: 0.742653489112854
[2025-12-23 14:49:07,286] m-LoRA: Adapter lora_gsm8k_26 loss: 1.1279120445251465
[2025-12-23 14:49:07,640] m-LoRA: Adapter lora_gsm8k_23 loss: 0.7406930327415466
[2025-12-23 14:49:08,125] m-LoRA: Adapter lora_gsm8k_24 epoch: 1/1 iteration: 96/128 step: 13
[2025-12-23 14:49:08,252] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 24/128 step: 13
[2025-12-23 14:49:08,333] m-LoRA: Adapter lora_gsm8k_22 epoch: 1/1 iteration: 120/128 step: 31
[2025-12-23 14:49:08,674] m-LoRA: Adapter lora_gsm8k_26 epoch: 1/1 iteration: 20/128 step: 6
[2025-12-23 14:49:09,552] m-LoRA: Adapter lora_gsm8k_24 loss: 1.4151655435562134
[2025-12-23 14:49:09,559] m-LoRA: Adapter lora_gsm8k_23 epoch: 1/1 iteration: 56/128 step: 15
[2025-12-23 14:49:09,610] m-LoRA: Adapter lora_gsm8k_25 loss: 0.7355650067329407
[2025-12-23 14:49:09,947] m-LoRA: Adapter lora_gsm8k_22 loss: 0.6871944665908813
[2025-12-23 14:49:10,262] m-LoRA: Adapter lora_gsm8k_26 loss: 1.162970781326294
[2025-12-23 14:49:10,648] m-LoRA: Adapter lora_gsm8k_23 loss: 0.5635026693344116
[2025-12-23 14:49:11,024] m-LoRA: Adapter lora_gsm8k_24 epoch: 1/1 iteration: 104/128 step: 14
[2025-12-23 14:49:11,152] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 26/128 step: 14
[2025-12-23 14:49:11,249] m-LoRA: Adapter lora_gsm8k_22 epoch: 1/1 iteration: 124/128 step: 32
[2025-12-23 14:49:11,486] m-LoRA: Adapter lora_gsm8k_26 epoch: 1/1 iteration: 24/128 step: 7
[2025-12-23 14:49:12,401] m-LoRA: Adapter lora_gsm8k_23 epoch: 1/1 iteration: 60/128 step: 16
[2025-12-23 14:49:12,476] m-LoRA: Adapter lora_gsm8k_24 loss: 1.3774775266647339
[2025-12-23 14:49:12,486] m-LoRA: Adapter lora_gsm8k_25 loss: 0.5625307559967041
[2025-12-23 14:49:12,839] m-LoRA: Adapter lora_gsm8k_22 loss: 0.7865865230560303
[2025-12-23 14:49:13,129] m-LoRA: Adapter lora_gsm8k_26 loss: 1.1984325647354126
[2025-12-23 14:49:13,466] m-LoRA: Adapter lora_gsm8k_23 loss: 0.5857380032539368
[2025-12-23 14:49:13,887] m-LoRA: Adapter lora_gsm8k_24 epoch: 1/1 iteration: 112/128 step: 15
[2025-12-23 14:49:14,012] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 28/128 step: 15
[2025-12-23 14:49:14,125] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_22']
[2025-12-23 14:49:14,332] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_27']
[2025-12-23 14:49:14,497] m-LoRA: Adapter lora_gsm8k_27 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:49:14,626] m-LoRA: Adapter lora_gsm8k_26 epoch: 1/1 iteration: 28/128 step: 8
[2025-12-23 14:49:14,868] m-LoRA: Adapter lora_gsm8k_23 epoch: 1/1 iteration: 64/128 step: 17
[2025-12-23 14:49:15,633] m-LoRA: Adapter lora_gsm8k_24 loss: 1.4298402070999146
[2025-12-23 14:49:15,702] m-LoRA: Adapter lora_gsm8k_25 loss: 0.7470112442970276
[2025-12-23 14:49:15,988] m-LoRA: Adapter lora_gsm8k_27 loss: 2.488769769668579
[2025-12-23 14:49:16,284] m-LoRA: Adapter lora_gsm8k_26 loss: 1.1340411901474
[2025-12-23 14:49:16,552] m-LoRA: Adapter lora_gsm8k_23 loss: 0.7523021101951599
[2025-12-23 14:49:16,987] m-LoRA: Adapter lora_gsm8k_24 epoch: 1/1 iteration: 120/128 step: 16
[2025-12-23 14:49:17,131] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 30/128 step: 16
[2025-12-23 14:49:17,284] m-LoRA: Adapter lora_gsm8k_27 epoch: 1/1 iteration: 4/128 step: 2
[2025-12-23 14:49:17,460] m-LoRA: Adapter lora_gsm8k_26 epoch: 1/1 iteration: 32/128 step: 9
[2025-12-23 14:49:17,750] m-LoRA: Adapter lora_gsm8k_23 epoch: 1/1 iteration: 68/128 step: 18
[2025-12-23 14:49:18,770] m-LoRA: Adapter lora_gsm8k_24 loss: 1.0266166925430298
[2025-12-23 14:49:18,787] m-LoRA: Adapter lora_gsm8k_25 loss: 0.6564798951148987
[2025-12-23 14:49:19,130] m-LoRA: Adapter lora_gsm8k_27 loss: 1.7437973022460938
[2025-12-23 14:49:19,376] m-LoRA: Adapter lora_gsm8k_26 loss: 1.2148517370224
[2025-12-23 14:49:19,760] m-LoRA: Adapter lora_gsm8k_23 loss: 0.45613235235214233
[2025-12-23 14:49:20,678] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_24']
[2025-12-23 14:49:21,059] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_28']
[2025-12-23 14:49:21,096] m-LoRA: Adapter lora_gsm8k_28 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:49:21,191] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 32/128 step: 17
[2025-12-23 14:49:21,261] m-LoRA: Adapter lora_gsm8k_27 epoch: 1/1 iteration: 8/128 step: 3
[2025-12-23 14:49:21,360] m-LoRA: Adapter lora_gsm8k_26 epoch: 1/1 iteration: 36/128 step: 10
[2025-12-23 14:49:21,775] m-LoRA: Adapter lora_gsm8k_23 epoch: 1/1 iteration: 72/128 step: 19
[2025-12-23 14:49:21,855] m-LoRA: Adapter lora_gsm8k_28 loss: 2.824770450592041
[2025-12-23 14:49:21,861] m-LoRA: Adapter lora_gsm8k_25 loss: 0.6933711767196655
[2025-12-23 14:49:22,236] m-LoRA: Adapter lora_gsm8k_27 loss: 1.378743052482605
[2025-12-23 14:49:22,513] m-LoRA: Adapter lora_gsm8k_26 loss: 1.0892651081085205
[2025-12-23 14:49:22,883] m-LoRA: Adapter lora_gsm8k_23 loss: 0.553130030632019
[2025-12-23 14:49:22,887] m-LoRA: Adapter lora_gsm8k_28 epoch: 1/1 iteration: 4/128 step: 2
[2025-12-23 14:49:23,027] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 34/128 step: 18
[2025-12-23 14:49:23,133] m-LoRA: Adapter lora_gsm8k_27 epoch: 1/1 iteration: 12/128 step: 4
[2025-12-23 14:49:23,387] m-LoRA: Adapter lora_gsm8k_26 epoch: 1/1 iteration: 40/128 step: 11
[2025-12-23 14:49:24,000] m-LoRA: Adapter lora_gsm8k_23 epoch: 1/1 iteration: 76/128 step: 20
[2025-12-23 14:49:24,054] m-LoRA: Adapter lora_gsm8k_28 loss: 1.809463381767273
[2025-12-23 14:49:24,238] m-LoRA: Adapter lora_gsm8k_25 loss: 0.6458403468132019
[2025-12-23 14:49:24,557] m-LoRA: Adapter lora_gsm8k_27 loss: 1.3991038799285889
[2025-12-23 14:49:24,915] m-LoRA: Adapter lora_gsm8k_26 loss: 0.8751567006111145
[2025-12-23 14:49:25,262] m-LoRA: Adapter lora_gsm8k_23 loss: 0.5339197516441345
[2025-12-23 14:49:25,266] m-LoRA: Adapter lora_gsm8k_28 epoch: 1/1 iteration: 8/128 step: 3
[2025-12-23 14:49:25,393] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 36/128 step: 19
[2025-12-23 14:49:25,537] m-LoRA: Adapter lora_gsm8k_27 epoch: 1/1 iteration: 16/128 step: 5
[2025-12-23 14:49:26,052] m-LoRA: Adapter lora_gsm8k_26 epoch: 1/1 iteration: 44/128 step: 12
[2025-12-23 14:49:26,093] m-LoRA: Adapter lora_gsm8k_28 loss: 1.5115232467651367
[2025-12-23 14:49:26,185] m-LoRA: Adapter lora_gsm8k_25 loss: 0.7861283421516418
[2025-12-23 14:49:26,556] m-LoRA: Adapter lora_gsm8k_27 loss: 1.3315914869308472
[2025-12-23 14:49:26,559] m-LoRA: Adapter lora_gsm8k_23 epoch: 1/1 iteration: 80/128 step: 21
[2025-12-23 14:49:27,197] m-LoRA: Adapter lora_gsm8k_26 loss: 0.8716503381729126
[2025-12-23 14:49:27,201] m-LoRA: Adapter lora_gsm8k_28 epoch: 1/1 iteration: 12/128 step: 4
[2025-12-23 14:49:27,481] m-LoRA: Adapter lora_gsm8k_23 loss: 0.6262890100479126
[2025-12-23 14:49:27,485] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 38/128 step: 20
[2025-12-23 14:49:27,631] m-LoRA: Adapter lora_gsm8k_27 epoch: 1/1 iteration: 20/128 step: 6
[2025-12-23 14:49:28,231] m-LoRA: Adapter lora_gsm8k_26 epoch: 1/1 iteration: 48/128 step: 13
[2025-12-23 14:49:28,275] m-LoRA: Adapter lora_gsm8k_28 loss: 1.439290165901184
[2025-12-23 14:49:28,372] m-LoRA: Adapter lora_gsm8k_25 loss: 0.683427631855011
[2025-12-23 14:49:28,756] m-LoRA: Adapter lora_gsm8k_27 loss: 1.491134524345398
[2025-12-23 14:49:28,759] m-LoRA: Adapter lora_gsm8k_23 epoch: 1/1 iteration: 84/128 step: 22
[2025-12-23 14:49:29,065] m-LoRA: Adapter lora_gsm8k_26 loss: 0.8464299440383911
[2025-12-23 14:49:29,220] m-LoRA: Adapter lora_gsm8k_28 epoch: 1/1 iteration: 16/128 step: 5
[2025-12-23 14:49:29,589] m-LoRA: Adapter lora_gsm8k_23 loss: 0.5890003442764282
[2025-12-23 14:49:29,594] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 40/128 step: 21
[2025-12-23 14:49:29,756] m-LoRA: Adapter lora_gsm8k_27 epoch: 1/1 iteration: 24/128 step: 7
[2025-12-23 14:49:30,161] m-LoRA: Adapter lora_gsm8k_28 loss: 1.6447982788085938
[2025-12-23 14:49:30,165] m-LoRA: Adapter lora_gsm8k_26 epoch: 1/1 iteration: 52/128 step: 14
[2025-12-23 14:49:30,249] m-LoRA: Adapter lora_gsm8k_25 loss: 0.8833593130111694
[2025-12-23 14:49:30,689] m-LoRA: Adapter lora_gsm8k_27 loss: 1.3198500871658325
[2025-12-23 14:49:30,693] m-LoRA: Adapter lora_gsm8k_23 epoch: 1/1 iteration: 88/128 step: 23
[2025-12-23 14:49:31,145] m-LoRA: Adapter lora_gsm8k_26 loss: 0.719101071357727
[2025-12-23 14:49:31,149] m-LoRA: Adapter lora_gsm8k_28 epoch: 1/1 iteration: 20/128 step: 6
[2025-12-23 14:49:31,312] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 42/128 step: 22
[2025-12-23 14:49:31,664] m-LoRA: Adapter lora_gsm8k_23 loss: 0.5515402555465698
[2025-12-23 14:49:31,669] m-LoRA: Adapter lora_gsm8k_27 epoch: 1/1 iteration: 28/128 step: 8
[2025-12-23 14:49:32,097] m-LoRA: Adapter lora_gsm8k_26 epoch: 1/1 iteration: 56/128 step: 15
[2025-12-23 14:49:32,125] m-LoRA: Adapter lora_gsm8k_28 loss: 1.4806522130966187
[2025-12-23 14:49:32,222] m-LoRA: Adapter lora_gsm8k_25 loss: 0.6711152195930481
[2025-12-23 14:49:32,586] m-LoRA: Adapter lora_gsm8k_27 loss: 1.4062165021896362
[2025-12-23 14:49:32,589] m-LoRA: Adapter lora_gsm8k_23 epoch: 1/1 iteration: 92/128 step: 24
[2025-12-23 14:49:32,923] m-LoRA: Adapter lora_gsm8k_26 loss: 0.7178162932395935
[2025-12-23 14:49:33,057] m-LoRA: Adapter lora_gsm8k_28 epoch: 1/1 iteration: 24/128 step: 7
[2025-12-23 14:49:33,167] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 44/128 step: 23
[2025-12-23 14:49:33,689] m-LoRA: Adapter lora_gsm8k_27 epoch: 1/1 iteration: 32/128 step: 9
[2025-12-23 14:49:33,718] m-LoRA: Adapter lora_gsm8k_23 loss: 0.5398423075675964
[2025-12-23 14:49:34,058] m-LoRA: Adapter lora_gsm8k_28 loss: 1.415247917175293
[2025-12-23 14:49:34,062] m-LoRA: Adapter lora_gsm8k_26 epoch: 1/1 iteration: 60/128 step: 16
[2025-12-23 14:49:34,149] m-LoRA: Adapter lora_gsm8k_25 loss: 0.527771532535553
[2025-12-23 14:49:34,510] m-LoRA: Adapter lora_gsm8k_27 loss: 1.1512691974639893
[2025-12-23 14:49:34,960] m-LoRA: Adapter lora_gsm8k_26 loss: 0.6852011680603027
[2025-12-23 14:49:34,964] m-LoRA: Adapter lora_gsm8k_23 epoch: 1/1 iteration: 96/128 step: 25
[2025-12-23 14:49:35,117] m-LoRA: Adapter lora_gsm8k_28 epoch: 1/1 iteration: 28/128 step: 8
[2025-12-23 14:49:35,189] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 46/128 step: 24
[2025-12-23 14:49:35,377] m-LoRA: Adapter lora_gsm8k_27 epoch: 1/1 iteration: 36/128 step: 10
[2025-12-23 14:49:35,961] m-LoRA: Adapter lora_gsm8k_26 epoch: 1/1 iteration: 64/128 step: 17
[2025-12-23 14:49:36,010] m-LoRA: Adapter lora_gsm8k_23 loss: 0.4883119761943817
[2025-12-23 14:49:36,266] m-LoRA: Adapter lora_gsm8k_28 loss: 1.627107858657837
[2025-12-23 14:49:36,389] m-LoRA: Adapter lora_gsm8k_25 loss: 0.5503812432289124
[2025-12-23 14:49:36,712] m-LoRA: Adapter lora_gsm8k_27 loss: 0.904509961605072
[2025-12-23 14:49:37,094] m-LoRA: Adapter lora_gsm8k_26 loss: 0.5011443495750427
[2025-12-23 14:49:37,097] m-LoRA: Adapter lora_gsm8k_23 epoch: 1/1 iteration: 100/128 step: 26
[2025-12-23 14:49:37,250] m-LoRA: Adapter lora_gsm8k_28 epoch: 1/1 iteration: 32/128 step: 9
[2025-12-23 14:49:37,385] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 48/128 step: 25
[2025-12-23 14:49:37,932] m-LoRA: Adapter lora_gsm8k_27 epoch: 1/1 iteration: 40/128 step: 11
[2025-12-23 14:49:37,989] m-LoRA: Adapter lora_gsm8k_23 loss: 0.45694899559020996
[2025-12-23 14:49:38,225] m-LoRA: Adapter lora_gsm8k_28 loss: 1.485196828842163
[2025-12-23 14:49:38,458] m-LoRA: Adapter lora_gsm8k_25 loss: 0.9206991195678711
[2025-12-23 14:49:38,461] m-LoRA: Adapter lora_gsm8k_26 epoch: 1/1 iteration: 68/128 step: 18
[2025-12-23 14:49:38,922] m-LoRA: Adapter lora_gsm8k_27 loss: 0.8041720986366272
[2025-12-23 14:49:38,928] m-LoRA: Adapter lora_gsm8k_23 epoch: 1/1 iteration: 104/128 step: 27
[2025-12-23 14:49:39,384] m-LoRA: Adapter lora_gsm8k_26 loss: 0.6552515029907227
[2025-12-23 14:49:39,388] m-LoRA: Adapter lora_gsm8k_28 epoch: 1/1 iteration: 36/128 step: 10
[2025-12-23 14:49:39,508] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 50/128 step: 26
[2025-12-23 14:49:40,110] m-LoRA: Adapter lora_gsm8k_27 epoch: 1/1 iteration: 44/128 step: 12
[2025-12-23 14:49:40,175] m-LoRA: Adapter lora_gsm8k_23 loss: 0.5206743478775024
[2025-12-23 14:49:40,449] m-LoRA: Adapter lora_gsm8k_28 loss: 1.295244812965393
[2025-12-23 14:49:40,453] m-LoRA: Adapter lora_gsm8k_26 epoch: 1/1 iteration: 72/128 step: 19
[2025-12-23 14:49:40,541] m-LoRA: Adapter lora_gsm8k_25 loss: 0.5041782855987549
[2025-12-23 14:49:40,929] m-LoRA: Adapter lora_gsm8k_27 loss: 1.0071905851364136
[2025-12-23 14:49:41,273] m-LoRA: Adapter lora_gsm8k_23 epoch: 1/1 iteration: 108/128 step: 28
[2025-12-23 14:49:41,331] m-LoRA: Adapter lora_gsm8k_26 loss: 0.7347620129585266
[2025-12-23 14:49:41,527] m-LoRA: Adapter lora_gsm8k_28 epoch: 1/1 iteration: 40/128 step: 11
[2025-12-23 14:49:41,622] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 52/128 step: 27
[2025-12-23 14:49:41,813] m-LoRA: Adapter lora_gsm8k_27 epoch: 1/1 iteration: 48/128 step: 13
[2025-12-23 14:49:42,301] m-LoRA: Adapter lora_gsm8k_26 epoch: 1/1 iteration: 76/128 step: 20
[2025-12-23 14:49:42,342] m-LoRA: Adapter lora_gsm8k_23 loss: 0.6236060261726379
[2025-12-23 14:49:42,622] m-LoRA: Adapter lora_gsm8k_28 loss: 1.3152803182601929
[2025-12-23 14:49:42,709] m-LoRA: Adapter lora_gsm8k_25 loss: 0.542160153388977
[2025-12-23 14:49:43,023] m-LoRA: Adapter lora_gsm8k_27 loss: 0.6708780527114868
[2025-12-23 14:49:43,379] m-LoRA: Adapter lora_gsm8k_26 loss: 0.65911865234375
[2025-12-23 14:49:43,383] m-LoRA: Adapter lora_gsm8k_23 epoch: 1/1 iteration: 112/128 step: 29
[2025-12-23 14:49:43,505] m-LoRA: Adapter lora_gsm8k_28 epoch: 1/1 iteration: 44/128 step: 12
[2025-12-23 14:49:43,584] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 54/128 step: 28
[2025-12-23 14:49:43,857] m-LoRA: Adapter lora_gsm8k_27 epoch: 1/1 iteration: 52/128 step: 14
[2025-12-23 14:49:44,301] m-LoRA: Adapter lora_gsm8k_26 epoch: 1/1 iteration: 80/128 step: 21
[2025-12-23 14:49:44,349] m-LoRA: Adapter lora_gsm8k_23 loss: 0.5835704803466797
[2025-12-23 14:49:44,730] m-LoRA: Adapter lora_gsm8k_28 loss: 1.086711049079895
[2025-12-23 14:49:44,922] m-LoRA: Adapter lora_gsm8k_25 loss: 0.6196575164794922
[2025-12-23 14:49:45,319] m-LoRA: Adapter lora_gsm8k_27 loss: 0.7262255549430847
[2025-12-23 14:49:45,323] m-LoRA: Adapter lora_gsm8k_23 epoch: 1/1 iteration: 116/128 step: 30
[2025-12-23 14:49:45,629] m-LoRA: Adapter lora_gsm8k_26 loss: 0.43736955523490906
[2025-12-23 14:49:45,772] m-LoRA: Adapter lora_gsm8k_28 epoch: 1/1 iteration: 48/128 step: 13
[2025-12-23 14:49:45,889] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 56/128 step: 29
[2025-12-23 14:49:46,370] m-LoRA: Adapter lora_gsm8k_23 loss: 0.5299994349479675
[2025-12-23 14:49:46,374] m-LoRA: Adapter lora_gsm8k_27 epoch: 1/1 iteration: 56/128 step: 15
[2025-12-23 14:49:46,834] m-LoRA: Adapter lora_gsm8k_28 loss: 1.164443016052246
[2025-12-23 14:49:46,839] m-LoRA: Adapter lora_gsm8k_26 epoch: 1/1 iteration: 84/128 step: 22
[2025-12-23 14:49:47,015] m-LoRA: Adapter lora_gsm8k_25 loss: 0.45195290446281433
[2025-12-23 14:49:47,375] m-LoRA: Adapter lora_gsm8k_27 loss: 0.5456893444061279
[2025-12-23 14:49:47,380] m-LoRA: Adapter lora_gsm8k_23 epoch: 1/1 iteration: 120/128 step: 31
[2025-12-23 14:49:48,058] m-LoRA: Adapter lora_gsm8k_26 loss: 0.4411547780036926
[2025-12-23 14:49:48,064] m-LoRA: Adapter lora_gsm8k_28 epoch: 1/1 iteration: 52/128 step: 14
[2025-12-23 14:49:48,232] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 58/128 step: 30
[2025-12-23 14:49:48,513] m-LoRA: Adapter lora_gsm8k_23 loss: 0.5394381284713745
[2025-12-23 14:49:48,517] m-LoRA: Adapter lora_gsm8k_27 epoch: 1/1 iteration: 60/128 step: 16
[2025-12-23 14:49:49,282] m-LoRA: Adapter lora_gsm8k_26 epoch: 1/1 iteration: 88/128 step: 23
[2025-12-23 14:49:49,354] m-LoRA: Adapter lora_gsm8k_28 loss: 1.2189360857009888
[2025-12-23 14:49:49,369] m-LoRA: Adapter lora_gsm8k_25 loss: 0.46039217710494995
[2025-12-23 14:49:49,833] m-LoRA: Adapter lora_gsm8k_23 epoch: 1/1 iteration: 124/128 step: 32
[2025-12-23 14:49:49,899] m-LoRA: Adapter lora_gsm8k_27 loss: 0.5222046375274658
[2025-12-23 14:49:50,185] m-LoRA: Adapter lora_gsm8k_26 loss: 0.6114195585250854
[2025-12-23 14:49:50,329] m-LoRA: Adapter lora_gsm8k_28 epoch: 1/1 iteration: 56/128 step: 15
[2025-12-23 14:49:50,493] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 60/128 step: 31
[2025-12-23 14:49:50,999] m-LoRA: Adapter lora_gsm8k_23 loss: 0.5357294678688049
[2025-12-23 14:49:51,004] m-LoRA: Adapter lora_gsm8k_27 epoch: 1/1 iteration: 64/128 step: 17
[2025-12-23 14:49:51,459] m-LoRA: Adapter lora_gsm8k_28 loss: 1.414043664932251
[2025-12-23 14:49:51,463] m-LoRA: Adapter lora_gsm8k_26 epoch: 1/1 iteration: 92/128 step: 24
[2025-12-23 14:49:51,687] m-LoRA: Adapter lora_gsm8k_25 loss: 0.7997736930847168
[2025-12-23 14:49:52,042] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_23']
[2025-12-23 14:49:52,869] m-LoRA: Adapter lora_gsm8k_27 loss: 0.4827389121055603
[2025-12-23 14:49:52,872] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_29']
[2025-12-23 14:49:53,035] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:49:53,056] m-LoRA: Adapter lora_gsm8k_26 loss: 0.5391870737075806
[2025-12-23 14:49:53,306] m-LoRA: Adapter lora_gsm8k_28 epoch: 1/1 iteration: 60/128 step: 16
[2025-12-23 14:49:53,420] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 62/128 step: 32
[2025-12-23 14:49:53,682] m-LoRA: Adapter lora_gsm8k_27 epoch: 1/1 iteration: 68/128 step: 18
[2025-12-23 14:49:54,006] m-LoRA: Adapter lora_gsm8k_29 loss: 2.0751962661743164
[2025-12-23 14:49:54,330] m-LoRA: Adapter lora_gsm8k_28 loss: 1.023587703704834
[2025-12-23 14:49:54,333] m-LoRA: Adapter lora_gsm8k_26 epoch: 1/1 iteration: 96/128 step: 25
[2025-12-23 14:49:54,442] m-LoRA: Adapter lora_gsm8k_25 loss: 0.7192707657814026
[2025-12-23 14:49:54,783] m-LoRA: Adapter lora_gsm8k_27 loss: 0.6353796720504761
[2025-12-23 14:49:54,787] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 2/128 step: 2
[2025-12-23 14:49:55,355] m-LoRA: Adapter lora_gsm8k_26 loss: 0.5635881423950195
[2025-12-23 14:49:55,359] m-LoRA: Adapter lora_gsm8k_28 epoch: 1/1 iteration: 64/128 step: 17
[2025-12-23 14:49:55,595] m-LoRA: Adapter lora_gsm8k_29 loss: 1.527632474899292
[2025-12-23 14:49:55,598] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 64/128 step: 33
[2025-12-23 14:49:55,735] m-LoRA: Adapter lora_gsm8k_27 epoch: 1/1 iteration: 72/128 step: 19
[2025-12-23 14:49:56,423] m-LoRA: Adapter lora_gsm8k_26 epoch: 1/1 iteration: 100/128 step: 26
[2025-12-23 14:49:56,525] m-LoRA: Adapter lora_gsm8k_28 loss: 0.9921332597732544
[2025-12-23 14:49:56,530] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 4/128 step: 3
[2025-12-23 14:49:56,578] m-LoRA: Adapter lora_gsm8k_25 loss: 0.5524460673332214
[2025-12-23 14:49:56,955] m-LoRA: Adapter lora_gsm8k_27 loss: 0.6662720441818237
[2025-12-23 14:49:57,390] m-LoRA: Adapter lora_gsm8k_26 loss: 0.4972511827945709
[2025-12-23 14:49:57,532] m-LoRA: Adapter lora_gsm8k_29 loss: 2.2754788398742676
[2025-12-23 14:49:57,534] m-LoRA: Adapter lora_gsm8k_28 epoch: 1/1 iteration: 68/128 step: 18
[2025-12-23 14:49:57,669] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 66/128 step: 34
[2025-12-23 14:49:58,053] m-LoRA: Adapter lora_gsm8k_27 epoch: 1/1 iteration: 76/128 step: 20
[2025-12-23 14:49:58,482] m-LoRA: Adapter lora_gsm8k_26 epoch: 1/1 iteration: 104/128 step: 27
[2025-12-23 14:49:58,588] m-LoRA: Adapter lora_gsm8k_28 loss: 1.032538652420044
[2025-12-23 14:49:58,591] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 6/128 step: 4
[2025-12-23 14:49:58,669] m-LoRA: Adapter lora_gsm8k_25 loss: 0.536836564540863
[2025-12-23 14:49:58,943] m-LoRA: Adapter lora_gsm8k_27 loss: 0.5201654434204102
[2025-12-23 14:49:59,292] m-LoRA: Adapter lora_gsm8k_26 loss: 0.6174866557121277
[2025-12-23 14:49:59,295] m-LoRA: Adapter lora_gsm8k_28 epoch: 1/1 iteration: 72/128 step: 19
[2025-12-23 14:49:59,439] m-LoRA: Adapter lora_gsm8k_29 loss: 1.2984920740127563
[2025-12-23 14:49:59,442] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 68/128 step: 35
[2025-12-23 14:50:00,070] m-LoRA: Adapter lora_gsm8k_28 loss: 0.9598063826560974
[2025-12-23 14:50:00,073] m-LoRA: Adapter lora_gsm8k_27 epoch: 1/1 iteration: 80/128 step: 21
[2025-12-23 14:50:00,262] m-LoRA: Adapter lora_gsm8k_25 loss: 0.4563257694244385
[2025-12-23 14:50:00,264] m-LoRA: Adapter lora_gsm8k_26 epoch: 1/1 iteration: 108/128 step: 28
[2025-12-23 14:50:00,403] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 8/128 step: 5
[2025-12-23 14:50:01,130] m-LoRA: Adapter lora_gsm8k_28 epoch: 1/1 iteration: 76/128 step: 20
[2025-12-23 14:50:01,156] m-LoRA: Adapter lora_gsm8k_27 loss: 0.6044797301292419
[2025-12-23 14:50:01,518] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 70/128 step: 36
[2025-12-23 14:50:01,540] m-LoRA: Adapter lora_gsm8k_26 loss: 0.6686517596244812
[2025-12-23 14:50:01,579] m-LoRA: Adapter lora_gsm8k_29 loss: 1.5382803678512573
[2025-12-23 14:50:01,835] m-LoRA: Adapter lora_gsm8k_28 loss: 1.2097703218460083
[2025-12-23 14:50:01,993] m-LoRA: Adapter lora_gsm8k_25 loss: 0.6011229753494263
[2025-12-23 14:50:02,131] m-LoRA: Adapter lora_gsm8k_27 epoch: 1/1 iteration: 84/128 step: 22
[2025-12-23 14:50:02,560] m-LoRA: Adapter lora_gsm8k_26 epoch: 1/1 iteration: 112/128 step: 29
[2025-12-23 14:50:02,629] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 10/128 step: 6
[2025-12-23 14:50:03,042] m-LoRA: Adapter lora_gsm8k_28 epoch: 1/1 iteration: 80/128 step: 21
[2025-12-23 14:50:03,069] m-LoRA: Adapter lora_gsm8k_27 loss: 0.5100439786911011
[2025-12-23 14:50:03,422] m-LoRA: Adapter lora_gsm8k_26 loss: 0.6121957898139954
[2025-12-23 14:50:03,427] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 72/128 step: 37
[2025-12-23 14:50:03,478] m-LoRA: Adapter lora_gsm8k_29 loss: 1.6749340295791626
[2025-12-23 14:50:03,883] m-LoRA: Adapter lora_gsm8k_28 loss: 0.8530985116958618
[2025-12-23 14:50:04,166] m-LoRA: Adapter lora_gsm8k_25 loss: 0.504364013671875
[2025-12-23 14:50:04,168] m-LoRA: Adapter lora_gsm8k_27 epoch: 1/1 iteration: 88/128 step: 23
[2025-12-23 14:50:04,443] m-LoRA: Adapter lora_gsm8k_26 epoch: 1/1 iteration: 116/128 step: 30
[2025-12-23 14:50:04,544] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 12/128 step: 7
[2025-12-23 14:50:05,103] m-LoRA: Adapter lora_gsm8k_28 epoch: 1/1 iteration: 84/128 step: 22
[2025-12-23 14:50:05,138] m-LoRA: Adapter lora_gsm8k_27 loss: 0.6963300108909607
[2025-12-23 14:50:05,396] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 74/128 step: 38
[2025-12-23 14:50:05,415] m-LoRA: Adapter lora_gsm8k_26 loss: 0.5208788514137268
[2025-12-23 14:50:05,567] m-LoRA: Adapter lora_gsm8k_29 loss: 1.3234909772872925
[2025-12-23 14:50:06,090] m-LoRA: Adapter lora_gsm8k_28 loss: 0.7574247717857361
[2025-12-23 14:50:06,259] m-LoRA: Adapter lora_gsm8k_25 loss: 0.5655930638313293
[2025-12-23 14:50:06,261] m-LoRA: Adapter lora_gsm8k_27 epoch: 1/1 iteration: 92/128 step: 24
[2025-12-23 14:50:06,338] m-LoRA: Adapter lora_gsm8k_26 epoch: 1/1 iteration: 120/128 step: 31
[2025-12-23 14:50:06,402] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 14/128 step: 8
[2025-12-23 14:50:07,305] m-LoRA: Adapter lora_gsm8k_28 epoch: 1/1 iteration: 88/128 step: 23
[2025-12-23 14:50:07,339] m-LoRA: Adapter lora_gsm8k_27 loss: 0.539326012134552
[2025-12-23 14:50:07,706] m-LoRA: Adapter lora_gsm8k_26 loss: 0.5754057765007019
[2025-12-23 14:50:07,711] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 76/128 step: 39
[2025-12-23 14:50:07,938] m-LoRA: Adapter lora_gsm8k_29 loss: 1.2621252536773682
[2025-12-23 14:50:08,338] m-LoRA: Adapter lora_gsm8k_27 epoch: 1/1 iteration: 96/128 step: 25
[2025-12-23 14:50:08,392] m-LoRA: Adapter lora_gsm8k_28 loss: 0.766609787940979
[2025-12-23 14:50:08,567] m-LoRA: Adapter lora_gsm8k_25 loss: 0.36643466353416443
[2025-12-23 14:50:08,682] m-LoRA: Adapter lora_gsm8k_26 epoch: 1/1 iteration: 124/128 step: 32
[2025-12-23 14:50:08,786] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 16/128 step: 9
[2025-12-23 14:50:09,402] m-LoRA: Adapter lora_gsm8k_28 epoch: 1/1 iteration: 92/128 step: 24
[2025-12-23 14:50:09,442] m-LoRA: Adapter lora_gsm8k_27 loss: 0.6088094711303711
[2025-12-23 14:50:09,745] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 78/128 step: 40
[2025-12-23 14:50:09,791] m-LoRA: Adapter lora_gsm8k_26 loss: 0.4447346031665802
[2025-12-23 14:50:09,894] m-LoRA: Adapter lora_gsm8k_29 loss: 1.065478801727295
[2025-12-23 14:50:10,257] m-LoRA: Adapter lora_gsm8k_28 loss: 0.8189650774002075
[2025-12-23 14:50:10,491] m-LoRA: Adapter lora_gsm8k_25 loss: 0.4928952157497406
[2025-12-23 14:50:10,494] m-LoRA: Adapter lora_gsm8k_27 epoch: 1/1 iteration: 100/128 step: 26
[2025-12-23 14:50:10,615] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_26']
[2025-12-23 14:50:11,007] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_30']
[2025-12-23 14:50:11,044] m-LoRA: Adapter lora_gsm8k_30 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:50:11,167] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 18/128 step: 10
[2025-12-23 14:50:11,297] m-LoRA: Adapter lora_gsm8k_28 epoch: 1/1 iteration: 96/128 step: 25
[2025-12-23 14:50:11,392] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 80/128 step: 41
[2025-12-23 14:50:11,814] m-LoRA: Adapter lora_gsm8k_27 loss: 0.48375529050827026
[2025-12-23 14:50:12,188] m-LoRA: Adapter lora_gsm8k_30 loss: 2.107876777648926
[2025-12-23 14:50:12,291] m-LoRA: Adapter lora_gsm8k_29 loss: 0.9118097424507141
[2025-12-23 14:50:12,731] m-LoRA: Adapter lora_gsm8k_28 loss: 0.7297189235687256
[2025-12-23 14:50:12,735] m-LoRA: Adapter lora_gsm8k_27 epoch: 1/1 iteration: 104/128 step: 27
[2025-12-23 14:50:12,861] m-LoRA: Adapter lora_gsm8k_25 loss: 0.6246816515922546
[2025-12-23 14:50:13,193] m-LoRA: Adapter lora_gsm8k_30 epoch: 1/1 iteration: 4/128 step: 2
[2025-12-23 14:50:13,297] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 20/128 step: 11
[2025-12-23 14:50:13,804] m-LoRA: Adapter lora_gsm8k_28 epoch: 1/1 iteration: 100/128 step: 26
[2025-12-23 14:50:13,850] m-LoRA: Adapter lora_gsm8k_27 loss: 0.4663124680519104
[2025-12-23 14:50:14,108] m-LoRA: Adapter lora_gsm8k_30 loss: 2.205820083618164
[2025-12-23 14:50:14,111] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 82/128 step: 42
[2025-12-23 14:50:14,270] m-LoRA: Adapter lora_gsm8k_29 loss: 1.0176194906234741
[2025-12-23 14:50:14,723] m-LoRA: Adapter lora_gsm8k_28 loss: 0.6139282584190369
[2025-12-23 14:50:14,947] m-LoRA: Adapter lora_gsm8k_25 loss: 0.6359304785728455
[2025-12-23 14:50:14,949] m-LoRA: Adapter lora_gsm8k_27 epoch: 1/1 iteration: 108/128 step: 28
[2025-12-23 14:50:15,069] m-LoRA: Adapter lora_gsm8k_30 epoch: 1/1 iteration: 8/128 step: 3
[2025-12-23 14:50:15,158] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 22/128 step: 12
[2025-12-23 14:50:15,948] m-LoRA: Adapter lora_gsm8k_28 epoch: 1/1 iteration: 104/128 step: 27
[2025-12-23 14:50:16,045] m-LoRA: Adapter lora_gsm8k_27 loss: 0.6376792192459106
[2025-12-23 14:50:16,049] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 84/128 step: 43
[2025-12-23 14:50:16,083] m-LoRA: Adapter lora_gsm8k_29 loss: 0.8351210355758667
[2025-12-23 14:50:16,424] m-LoRA: Adapter lora_gsm8k_30 loss: 1.4033372402191162
[2025-12-23 14:50:16,917] m-LoRA: Adapter lora_gsm8k_28 loss: 0.5408440232276917
[2025-12-23 14:50:16,930] m-LoRA: Adapter lora_gsm8k_25 loss: 0.6456137299537659
[2025-12-23 14:50:16,983] m-LoRA: Adapter lora_gsm8k_27 epoch: 1/1 iteration: 112/128 step: 29
[2025-12-23 14:50:17,129] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 24/128 step: 13
[2025-12-23 14:50:17,447] m-LoRA: Adapter lora_gsm8k_30 epoch: 1/1 iteration: 12/128 step: 4
[2025-12-23 14:50:18,086] m-LoRA: Adapter lora_gsm8k_28 epoch: 1/1 iteration: 108/128 step: 28
[2025-12-23 14:50:18,148] m-LoRA: Adapter lora_gsm8k_27 loss: 0.40703001618385315
[2025-12-23 14:50:18,244] m-LoRA: Adapter lora_gsm8k_29 loss: 0.6036149859428406
[2025-12-23 14:50:18,246] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 86/128 step: 44
[2025-12-23 14:50:18,564] m-LoRA: Adapter lora_gsm8k_30 loss: 1.6026666164398193
[2025-12-23 14:50:18,897] m-LoRA: Adapter lora_gsm8k_28 loss: 0.6914473176002502
[2025-12-23 14:50:19,052] m-LoRA: Adapter lora_gsm8k_25 loss: 0.6665881276130676
[2025-12-23 14:50:19,054] m-LoRA: Adapter lora_gsm8k_27 epoch: 1/1 iteration: 116/128 step: 30
[2025-12-23 14:50:19,197] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 26/128 step: 14
[2025-12-23 14:50:19,430] m-LoRA: Adapter lora_gsm8k_30 epoch: 1/1 iteration: 16/128 step: 5
[2025-12-23 14:50:19,965] m-LoRA: Adapter lora_gsm8k_27 loss: 0.6024662852287292
[2025-12-23 14:50:19,970] m-LoRA: Adapter lora_gsm8k_28 epoch: 1/1 iteration: 112/128 step: 29
[2025-12-23 14:50:20,128] m-LoRA: Adapter lora_gsm8k_29 loss: 0.9133523106575012
[2025-12-23 14:50:20,131] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 88/128 step: 45
[2025-12-23 14:50:20,549] m-LoRA: Adapter lora_gsm8k_30 loss: 1.2586537599563599
[2025-12-23 14:50:21,095] m-LoRA: Adapter lora_gsm8k_28 loss: 0.8415268659591675
[2025-12-23 14:50:21,098] m-LoRA: Adapter lora_gsm8k_27 epoch: 1/1 iteration: 120/128 step: 31
[2025-12-23 14:50:21,284] m-LoRA: Adapter lora_gsm8k_25 loss: 0.6747191548347473
[2025-12-23 14:50:21,286] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 28/128 step: 15
[2025-12-23 14:50:21,528] m-LoRA: Adapter lora_gsm8k_30 epoch: 1/1 iteration: 20/128 step: 6
[2025-12-23 14:50:22,069] m-LoRA: Adapter lora_gsm8k_28 epoch: 1/1 iteration: 116/128 step: 30
[2025-12-23 14:50:22,194] m-LoRA: Adapter lora_gsm8k_27 loss: 0.7043771147727966
[2025-12-23 14:50:22,198] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 90/128 step: 46
[2025-12-23 14:50:22,237] m-LoRA: Adapter lora_gsm8k_29 loss: 0.768622100353241
[2025-12-23 14:50:22,483] m-LoRA: Adapter lora_gsm8k_30 loss: 1.6624300479888916
[2025-12-23 14:50:22,896] m-LoRA: Adapter lora_gsm8k_28 loss: 0.747146487236023
[2025-12-23 14:50:22,908] m-LoRA: Adapter lora_gsm8k_25 loss: 0.5243993997573853
[2025-12-23 14:50:23,100] m-LoRA: Adapter lora_gsm8k_27 epoch: 1/1 iteration: 124/128 step: 32
[2025-12-23 14:50:23,205] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 30/128 step: 16
[2025-12-23 14:50:23,360] m-LoRA: Adapter lora_gsm8k_30 epoch: 1/1 iteration: 24/128 step: 7
[2025-12-23 14:50:23,979] m-LoRA: Adapter lora_gsm8k_28 epoch: 1/1 iteration: 120/128 step: 31
[2025-12-23 14:50:24,115] m-LoRA: Adapter lora_gsm8k_27 loss: 0.5414573550224304
[2025-12-23 14:50:24,120] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 92/128 step: 47
[2025-12-23 14:50:24,166] m-LoRA: Adapter lora_gsm8k_29 loss: 0.6229572892189026
[2025-12-23 14:50:24,426] m-LoRA: Adapter lora_gsm8k_30 loss: 1.6436150074005127
[2025-12-23 14:50:24,805] m-LoRA: Adapter lora_gsm8k_28 loss: 0.5319144129753113
[2025-12-23 14:50:25,113] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_27']
[2025-12-23 14:50:25,873] m-LoRA: Adapter lora_gsm8k_25 loss: 0.4988382160663605
[2025-12-23 14:50:25,877] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_31']
[2025-12-23 14:50:26,038] m-LoRA: Adapter lora_gsm8k_31 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:50:26,161] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 32/128 step: 17
[2025-12-23 14:50:26,345] m-LoRA: Adapter lora_gsm8k_30 epoch: 1/1 iteration: 28/128 step: 8
[2025-12-23 14:50:26,435] m-LoRA: Adapter lora_gsm8k_28 epoch: 1/1 iteration: 124/128 step: 32
[2025-12-23 14:50:26,635] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 94/128 step: 48
[2025-12-23 14:50:27,391] m-LoRA: Adapter lora_gsm8k_31 loss: 2.6182522773742676
[2025-12-23 14:50:27,401] m-LoRA: Adapter lora_gsm8k_29 loss: 0.6073023080825806
[2025-12-23 14:50:27,784] m-LoRA: Adapter lora_gsm8k_30 loss: 1.4315495491027832
[2025-12-23 14:50:28,037] m-LoRA: Adapter lora_gsm8k_28 loss: 0.7455718517303467
[2025-12-23 14:50:28,170] m-LoRA: Adapter lora_gsm8k_25 loss: 0.7102711796760559
[2025-12-23 14:50:28,798] m-LoRA: Adapter lora_gsm8k_31 epoch: 1/1 iteration: 8/128 step: 2
[2025-12-23 14:50:28,918] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 34/128 step: 18
[2025-12-23 14:50:28,981] m-LoRA: Adapter lora_gsm8k_30 epoch: 1/1 iteration: 32/128 step: 9
[2025-12-23 14:50:29,286] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_28']
[2025-12-23 14:50:29,479] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_32']
[2025-12-23 14:50:29,661] m-LoRA: Adapter lora_gsm8k_32 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:50:29,767] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 96/128 step: 49
[2025-12-23 14:50:30,147] m-LoRA: Adapter lora_gsm8k_31 loss: 1.4824936389923096
[2025-12-23 14:50:30,163] m-LoRA: Adapter lora_gsm8k_29 loss: 0.6394862532615662
[2025-12-23 14:50:30,552] m-LoRA: Adapter lora_gsm8k_30 loss: 1.3018887042999268
[2025-12-23 14:50:30,837] m-LoRA: Adapter lora_gsm8k_32 loss: 2.3789710998535156
[2025-12-23 14:50:30,937] m-LoRA: Adapter lora_gsm8k_25 loss: 0.6396127343177795
[2025-12-23 14:50:31,628] m-LoRA: Adapter lora_gsm8k_31 epoch: 1/1 iteration: 16/128 step: 3
[2025-12-23 14:50:31,754] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 36/128 step: 19
[2025-12-23 14:50:31,843] m-LoRA: Adapter lora_gsm8k_30 epoch: 1/1 iteration: 36/128 step: 10
[2025-12-23 14:50:32,154] m-LoRA: Adapter lora_gsm8k_32 epoch: 1/1 iteration: 4/128 step: 2
[2025-12-23 14:50:32,697] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 98/128 step: 50
[2025-12-23 14:50:32,735] m-LoRA: Adapter lora_gsm8k_31 loss: 1.6800144910812378
[2025-12-23 14:50:32,928] m-LoRA: Adapter lora_gsm8k_29 loss: 0.5418229103088379
[2025-12-23 14:50:33,281] m-LoRA: Adapter lora_gsm8k_30 loss: 1.362215518951416
[2025-12-23 14:50:33,589] m-LoRA: Adapter lora_gsm8k_32 loss: 1.7324450016021729
[2025-12-23 14:50:33,726] m-LoRA: Adapter lora_gsm8k_25 loss: 0.6210886836051941
[2025-12-23 14:50:33,982] m-LoRA: Adapter lora_gsm8k_31 epoch: 1/1 iteration: 24/128 step: 4
[2025-12-23 14:50:34,148] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 38/128 step: 20
[2025-12-23 14:50:34,414] m-LoRA: Adapter lora_gsm8k_30 epoch: 1/1 iteration: 40/128 step: 11
[2025-12-23 14:50:35,433] m-LoRA: Adapter lora_gsm8k_32 epoch: 1/1 iteration: 8/128 step: 3
[2025-12-23 14:50:35,552] m-LoRA: Adapter lora_gsm8k_31 loss: 1.2032524347305298
[2025-12-23 14:50:35,560] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 100/128 step: 51
[2025-12-23 14:50:35,610] m-LoRA: Adapter lora_gsm8k_29 loss: 0.5574654340744019
[2025-12-23 14:50:35,839] m-LoRA: Adapter lora_gsm8k_30 loss: 1.3183481693267822
[2025-12-23 14:50:36,122] m-LoRA: Adapter lora_gsm8k_32 loss: 1.4994630813598633
[2025-12-23 14:50:36,273] m-LoRA: Adapter lora_gsm8k_25 loss: 0.5596307516098022
[2025-12-23 14:50:37,144] m-LoRA: Adapter lora_gsm8k_31 epoch: 1/1 iteration: 32/128 step: 5
[2025-12-23 14:50:37,275] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 40/128 step: 21
[2025-12-23 14:50:37,352] m-LoRA: Adapter lora_gsm8k_30 epoch: 1/1 iteration: 44/128 step: 12
[2025-12-23 14:50:37,560] m-LoRA: Adapter lora_gsm8k_32 epoch: 1/1 iteration: 12/128 step: 4
[2025-12-23 14:50:37,664] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 102/128 step: 52
[2025-12-23 14:50:38,406] m-LoRA: Adapter lora_gsm8k_31 loss: 1.240358829498291
[2025-12-23 14:50:38,427] m-LoRA: Adapter lora_gsm8k_29 loss: 0.6646923422813416
[2025-12-23 14:50:38,892] m-LoRA: Adapter lora_gsm8k_30 loss: 1.110310435295105
[2025-12-23 14:50:39,215] m-LoRA: Adapter lora_gsm8k_32 loss: 1.2283283472061157
[2025-12-23 14:50:39,423] m-LoRA: Adapter lora_gsm8k_25 loss: 0.7345020174980164
[2025-12-23 14:50:39,954] m-LoRA: Adapter lora_gsm8k_31 epoch: 1/1 iteration: 40/128 step: 6
[2025-12-23 14:50:40,072] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 42/128 step: 22
[2025-12-23 14:50:41,409] m-LoRA: Adapter lora_gsm8k_30 epoch: 1/1 iteration: 48/128 step: 13
[2025-12-23 14:50:41,465] m-LoRA: Adapter lora_gsm8k_31 loss: 1.2953602075576782
[2025-12-23 14:50:41,725] m-LoRA: Adapter lora_gsm8k_29 loss: 0.7054427862167358
[2025-12-23 14:50:41,728] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 104/128 step: 53
[2025-12-23 14:50:41,829] m-LoRA: Adapter lora_gsm8k_32 epoch: 1/1 iteration: 16/128 step: 5
[2025-12-23 14:50:42,126] m-LoRA: Adapter lora_gsm8k_30 loss: 1.3172563314437866
[2025-12-23 14:50:42,764] m-LoRA: Adapter lora_gsm8k_25 loss: 0.6873202323913574
[2025-12-23 14:50:43,094] m-LoRA: Adapter lora_gsm8k_32 loss: 1.5382497310638428
[2025-12-23 14:50:43,098] m-LoRA: Adapter lora_gsm8k_31 epoch: 1/1 iteration: 48/128 step: 7
[2025-12-23 14:50:43,280] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 44/128 step: 23
[2025-12-23 14:50:43,375] m-LoRA: Adapter lora_gsm8k_30 epoch: 1/1 iteration: 52/128 step: 14
[2025-12-23 14:50:43,474] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 106/128 step: 54
[2025-12-23 14:50:44,353] m-LoRA: Adapter lora_gsm8k_31 loss: 1.2560011148452759
[2025-12-23 14:50:44,370] m-LoRA: Adapter lora_gsm8k_32 epoch: 1/1 iteration: 20/128 step: 6
[2025-12-23 14:50:44,444] m-LoRA: Adapter lora_gsm8k_29 loss: 0.5092902779579163
[2025-12-23 14:50:44,823] m-LoRA: Adapter lora_gsm8k_30 loss: 1.1850385665893555
[2025-12-23 14:50:44,972] m-LoRA: Adapter lora_gsm8k_25 loss: 0.5830692648887634
[2025-12-23 14:50:45,393] m-LoRA: Adapter lora_gsm8k_32 loss: 1.0978811979293823
[2025-12-23 14:50:45,710] m-LoRA: Adapter lora_gsm8k_31 epoch: 1/1 iteration: 56/128 step: 8
[2025-12-23 14:50:45,799] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 46/128 step: 24
[2025-12-23 14:50:45,928] m-LoRA: Adapter lora_gsm8k_30 epoch: 1/1 iteration: 56/128 step: 15
[2025-12-23 14:50:46,062] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 108/128 step: 55
[2025-12-23 14:50:47,009] m-LoRA: Adapter lora_gsm8k_32 epoch: 1/1 iteration: 24/128 step: 7
[2025-12-23 14:50:47,089] m-LoRA: Adapter lora_gsm8k_31 loss: 1.2444199323654175
[2025-12-23 14:50:47,096] m-LoRA: Adapter lora_gsm8k_29 loss: 0.6639841794967651
[2025-12-23 14:50:47,436] m-LoRA: Adapter lora_gsm8k_30 loss: 1.1704291105270386
[2025-12-23 14:50:47,531] m-LoRA: Adapter lora_gsm8k_25 loss: 0.480496346950531
[2025-12-23 14:50:47,945] m-LoRA: Adapter lora_gsm8k_32 loss: 1.3405781984329224
[2025-12-23 14:50:48,415] m-LoRA: Adapter lora_gsm8k_31 epoch: 1/1 iteration: 64/128 step: 9
[2025-12-23 14:50:48,527] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 48/128 step: 25
[2025-12-23 14:50:48,656] m-LoRA: Adapter lora_gsm8k_30 epoch: 1/1 iteration: 60/128 step: 16
[2025-12-23 14:50:48,743] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 110/128 step: 56
[2025-12-23 14:50:49,691] m-LoRA: Adapter lora_gsm8k_32 epoch: 1/1 iteration: 28/128 step: 8
[2025-12-23 14:50:49,772] m-LoRA: Adapter lora_gsm8k_31 loss: 0.9902633428573608
[2025-12-23 14:50:49,795] m-LoRA: Adapter lora_gsm8k_29 loss: 0.5394901037216187
[2025-12-23 14:50:50,155] m-LoRA: Adapter lora_gsm8k_30 loss: 1.2221695184707642
[2025-12-23 14:50:50,318] m-LoRA: Adapter lora_gsm8k_25 loss: 0.5940819978713989
[2025-12-23 14:50:50,591] m-LoRA: Adapter lora_gsm8k_32 loss: 1.2791322469711304
[2025-12-23 14:50:51,225] m-LoRA: Adapter lora_gsm8k_31 epoch: 1/1 iteration: 72/128 step: 10
[2025-12-23 14:50:51,343] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 50/128 step: 26
[2025-12-23 14:50:51,525] m-LoRA: Adapter lora_gsm8k_30 epoch: 1/1 iteration: 64/128 step: 17
[2025-12-23 14:50:51,630] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 112/128 step: 57
[2025-12-23 14:50:52,448] m-LoRA: Adapter lora_gsm8k_32 epoch: 1/1 iteration: 32/128 step: 9
[2025-12-23 14:50:52,529] m-LoRA: Adapter lora_gsm8k_31 loss: 1.0668954849243164
[2025-12-23 14:50:52,537] m-LoRA: Adapter lora_gsm8k_29 loss: 0.5492197871208191
[2025-12-23 14:50:52,903] m-LoRA: Adapter lora_gsm8k_30 loss: 1.0310356616973877
[2025-12-23 14:50:53,036] m-LoRA: Adapter lora_gsm8k_25 loss: 0.5264169573783875
[2025-12-23 14:50:53,431] m-LoRA: Adapter lora_gsm8k_32 loss: 1.0360093116760254
[2025-12-23 14:50:53,914] m-LoRA: Adapter lora_gsm8k_31 epoch: 1/1 iteration: 80/128 step: 11
[2025-12-23 14:50:54,059] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 52/128 step: 27
[2025-12-23 14:50:54,200] m-LoRA: Adapter lora_gsm8k_30 epoch: 1/1 iteration: 68/128 step: 18
[2025-12-23 14:50:54,308] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 114/128 step: 58
[2025-12-23 14:50:54,600] m-LoRA: Adapter lora_gsm8k_32 epoch: 1/1 iteration: 36/128 step: 10
[2025-12-23 14:50:55,564] m-LoRA: Adapter lora_gsm8k_31 loss: 0.6820119619369507
[2025-12-23 14:50:55,574] m-LoRA: Adapter lora_gsm8k_29 loss: 0.49869245290756226
[2025-12-23 14:50:55,981] m-LoRA: Adapter lora_gsm8k_30 loss: 1.0672492980957031
[2025-12-23 14:50:56,084] m-LoRA: Adapter lora_gsm8k_25 loss: 0.6283940672874451
[2025-12-23 14:50:56,448] m-LoRA: Adapter lora_gsm8k_32 loss: 0.9552157521247864
[2025-12-23 14:50:57,426] m-LoRA: Adapter lora_gsm8k_31 epoch: 1/1 iteration: 88/128 step: 12
[2025-12-23 14:50:57,588] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 54/128 step: 28
[2025-12-23 14:50:57,685] m-LoRA: Adapter lora_gsm8k_30 epoch: 1/1 iteration: 72/128 step: 19
[2025-12-23 14:50:57,775] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 116/128 step: 59
[2025-12-23 14:50:57,903] m-LoRA: Adapter lora_gsm8k_32 epoch: 1/1 iteration: 40/128 step: 11
[2025-12-23 14:50:58,652] m-LoRA: Adapter lora_gsm8k_31 loss: 0.8817949295043945
[2025-12-23 14:50:58,670] m-LoRA: Adapter lora_gsm8k_29 loss: 0.7129819393157959
[2025-12-23 14:50:59,035] m-LoRA: Adapter lora_gsm8k_30 loss: 1.105921983718872
[2025-12-23 14:50:59,119] m-LoRA: Adapter lora_gsm8k_25 loss: 0.5468705296516418
[2025-12-23 14:50:59,370] m-LoRA: Adapter lora_gsm8k_32 loss: 1.0109368562698364
[2025-12-23 14:51:00,131] m-LoRA: Adapter lora_gsm8k_31 epoch: 1/1 iteration: 96/128 step: 13
[2025-12-23 14:51:00,271] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 56/128 step: 29
[2025-12-23 14:51:00,375] m-LoRA: Adapter lora_gsm8k_30 epoch: 1/1 iteration: 76/128 step: 20
[2025-12-23 14:51:00,458] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 118/128 step: 60
[2025-12-23 14:51:00,657] m-LoRA: Adapter lora_gsm8k_32 epoch: 1/1 iteration: 44/128 step: 12
[2025-12-23 14:51:01,587] m-LoRA: Adapter lora_gsm8k_31 loss: 0.6276013255119324
[2025-12-23 14:51:01,608] m-LoRA: Adapter lora_gsm8k_29 loss: 0.5511696934700012
[2025-12-23 14:51:01,975] m-LoRA: Adapter lora_gsm8k_30 loss: 0.9326083064079285
[2025-12-23 14:51:02,124] m-LoRA: Adapter lora_gsm8k_25 loss: 0.4725784361362457
[2025-12-23 14:51:02,397] m-LoRA: Adapter lora_gsm8k_32 loss: 0.914954423904419
[2025-12-23 14:51:03,330] m-LoRA: Adapter lora_gsm8k_31 epoch: 1/1 iteration: 104/128 step: 14
[2025-12-23 14:51:03,476] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 58/128 step: 30
[2025-12-23 14:51:03,547] m-LoRA: Adapter lora_gsm8k_30 epoch: 1/1 iteration: 80/128 step: 21
[2025-12-23 14:51:03,628] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 120/128 step: 61
[2025-12-23 14:51:03,832] m-LoRA: Adapter lora_gsm8k_32 epoch: 1/1 iteration: 48/128 step: 13
[2025-12-23 14:51:04,510] m-LoRA: Adapter lora_gsm8k_31 loss: 0.6765854358673096
[2025-12-23 14:51:04,525] m-LoRA: Adapter lora_gsm8k_29 loss: 0.5039130449295044
[2025-12-23 14:51:04,841] m-LoRA: Adapter lora_gsm8k_30 loss: 1.0297929048538208
[2025-12-23 14:51:04,993] m-LoRA: Adapter lora_gsm8k_25 loss: 0.6416133046150208
[2025-12-23 14:51:05,394] m-LoRA: Adapter lora_gsm8k_32 loss: 0.6315246224403381
[2025-12-23 14:51:05,794] m-LoRA: Adapter lora_gsm8k_31 epoch: 1/1 iteration: 112/128 step: 15
[2025-12-23 14:51:05,919] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 60/128 step: 31
[2025-12-23 14:51:06,007] m-LoRA: Adapter lora_gsm8k_30 epoch: 1/1 iteration: 84/128 step: 22
[2025-12-23 14:51:06,121] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 122/128 step: 62
[2025-12-23 14:51:07,149] m-LoRA: Adapter lora_gsm8k_31 loss: 0.8278968930244446
[2025-12-23 14:51:07,155] m-LoRA: Adapter lora_gsm8k_32 epoch: 1/1 iteration: 52/128 step: 14
[2025-12-23 14:51:07,206] m-LoRA: Adapter lora_gsm8k_29 loss: 0.715007483959198
[2025-12-23 14:51:07,526] m-LoRA: Adapter lora_gsm8k_30 loss: 0.9336758255958557
[2025-12-23 14:51:07,610] m-LoRA: Adapter lora_gsm8k_25 loss: 0.6778457164764404
[2025-12-23 14:51:07,948] m-LoRA: Adapter lora_gsm8k_32 loss: 0.8001512885093689
[2025-12-23 14:51:08,550] m-LoRA: Adapter lora_gsm8k_31 epoch: 1/1 iteration: 120/128 step: 16
[2025-12-23 14:51:08,676] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 62/128 step: 32
[2025-12-23 14:51:08,802] m-LoRA: Adapter lora_gsm8k_30 epoch: 1/1 iteration: 88/128 step: 23
[2025-12-23 14:51:08,886] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 124/128 step: 63
[2025-12-23 14:51:09,207] m-LoRA: Adapter lora_gsm8k_32 epoch: 1/1 iteration: 56/128 step: 15
[2025-12-23 14:51:09,951] m-LoRA: Adapter lora_gsm8k_31 loss: 0.6361567974090576
[2025-12-23 14:51:09,970] m-LoRA: Adapter lora_gsm8k_29 loss: 0.6170265078544617
[2025-12-23 14:51:10,334] m-LoRA: Adapter lora_gsm8k_30 loss: 0.8841724395751953
[2025-12-23 14:51:10,426] m-LoRA: Adapter lora_gsm8k_25 loss: 0.4345473349094391
[2025-12-23 14:51:10,688] m-LoRA: Adapter lora_gsm8k_32 loss: 0.6115343570709229
[2025-12-23 14:51:11,568] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_31']
[2025-12-23 14:51:11,960] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_33']
[2025-12-23 14:51:12,080] m-LoRA: Adapter lora_gsm8k_33 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:51:12,204] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 64/128 step: 33
[2025-12-23 14:51:12,275] m-LoRA: Adapter lora_gsm8k_30 epoch: 1/1 iteration: 92/128 step: 24
[2025-12-23 14:51:12,355] m-LoRA: Adapter lora_gsm8k_25 epoch: 1/1 iteration: 126/128 step: 64
[2025-12-23 14:51:12,591] m-LoRA: Adapter lora_gsm8k_32 epoch: 1/1 iteration: 60/128 step: 16
[2025-12-23 14:51:13,431] m-LoRA: Adapter lora_gsm8k_33 loss: 2.534464120864868
[2025-12-23 14:51:13,503] m-LoRA: Adapter lora_gsm8k_29 loss: 0.5924320816993713
[2025-12-23 14:51:13,767] m-LoRA: Adapter lora_gsm8k_30 loss: 0.9048503041267395
[2025-12-23 14:51:13,880] m-LoRA: Adapter lora_gsm8k_25 loss: 0.4082081615924835
[2025-12-23 14:51:14,303] m-LoRA: Adapter lora_gsm8k_32 loss: 0.6860896944999695
[2025-12-23 14:51:14,794] m-LoRA: Adapter lora_gsm8k_33 epoch: 1/1 iteration: 8/128 step: 2
[2025-12-23 14:51:14,927] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 66/128 step: 34
[2025-12-23 14:51:15,028] m-LoRA: Adapter lora_gsm8k_30 epoch: 1/1 iteration: 96/128 step: 25
[2025-12-23 14:51:15,106] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_25']
[2025-12-23 14:51:15,791] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_34']
[2025-12-23 14:51:15,946] m-LoRA: Adapter lora_gsm8k_34 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:51:16,749] m-LoRA: Adapter lora_gsm8k_32 epoch: 1/1 iteration: 64/128 step: 17
[2025-12-23 14:51:16,809] m-LoRA: Adapter lora_gsm8k_33 loss: 1.6007577180862427
[2025-12-23 14:51:16,816] m-LoRA: Adapter lora_gsm8k_29 loss: 0.5959858298301697
[2025-12-23 14:51:17,154] m-LoRA: Adapter lora_gsm8k_30 loss: 1.0320863723754883
[2025-12-23 14:51:17,416] m-LoRA: Adapter lora_gsm8k_34 loss: 2.6910154819488525
[2025-12-23 14:51:17,690] m-LoRA: Adapter lora_gsm8k_32 loss: 0.6771000623703003
[2025-12-23 14:51:18,137] m-LoRA: Adapter lora_gsm8k_33 epoch: 1/1 iteration: 16/128 step: 3
[2025-12-23 14:51:18,273] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 68/128 step: 35
[2025-12-23 14:51:18,365] m-LoRA: Adapter lora_gsm8k_30 epoch: 1/1 iteration: 100/128 step: 26
[2025-12-23 14:51:18,590] m-LoRA: Adapter lora_gsm8k_34 epoch: 1/1 iteration: 4/128 step: 2
[2025-12-23 14:51:18,841] m-LoRA: Adapter lora_gsm8k_32 epoch: 1/1 iteration: 68/128 step: 18
[2025-12-23 14:51:19,549] m-LoRA: Adapter lora_gsm8k_33 loss: 1.4121979475021362
[2025-12-23 14:51:19,567] m-LoRA: Adapter lora_gsm8k_29 loss: 0.4780275821685791
[2025-12-23 14:51:19,859] m-LoRA: Adapter lora_gsm8k_30 loss: 0.7916444540023804
[2025-12-23 14:51:20,085] m-LoRA: Adapter lora_gsm8k_34 loss: 1.7938462495803833
[2025-12-23 14:51:20,550] m-LoRA: Adapter lora_gsm8k_32 loss: 0.5736550092697144
[2025-12-23 14:51:21,009] m-LoRA: Adapter lora_gsm8k_33 epoch: 1/1 iteration: 24/128 step: 4
[2025-12-23 14:51:21,156] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 70/128 step: 36
[2025-12-23 14:51:21,253] m-LoRA: Adapter lora_gsm8k_30 epoch: 1/1 iteration: 104/128 step: 27
[2025-12-23 14:51:21,331] m-LoRA: Adapter lora_gsm8k_34 epoch: 1/1 iteration: 8/128 step: 3
[2025-12-23 14:51:22,428] m-LoRA: Adapter lora_gsm8k_33 loss: 1.3055967092514038
[2025-12-23 14:51:22,435] m-LoRA: Adapter lora_gsm8k_32 epoch: 1/1 iteration: 72/128 step: 19
[2025-12-23 14:51:22,461] m-LoRA: Adapter lora_gsm8k_29 loss: 0.604854941368103
[2025-12-23 14:51:22,988] m-LoRA: Adapter lora_gsm8k_30 loss: 0.5979856252670288
[2025-12-23 14:51:23,278] m-LoRA: Adapter lora_gsm8k_34 loss: 1.4343390464782715
[2025-12-23 14:51:23,620] m-LoRA: Adapter lora_gsm8k_32 loss: 0.6245156526565552
[2025-12-23 14:51:23,900] m-LoRA: Adapter lora_gsm8k_33 epoch: 1/1 iteration: 32/128 step: 5
[2025-12-23 14:51:24,038] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 72/128 step: 37
[2025-12-23 14:51:24,320] m-LoRA: Adapter lora_gsm8k_30 epoch: 1/1 iteration: 108/128 step: 28
[2025-12-23 14:51:24,974] m-LoRA: Adapter lora_gsm8k_34 epoch: 1/1 iteration: 12/128 step: 4
[2025-12-23 14:51:25,103] m-LoRA: Adapter lora_gsm8k_33 loss: 1.6234585046768188
[2025-12-23 14:51:25,109] m-LoRA: Adapter lora_gsm8k_32 epoch: 1/1 iteration: 76/128 step: 20
[2025-12-23 14:51:25,141] m-LoRA: Adapter lora_gsm8k_29 loss: 0.4634559154510498
[2025-12-23 14:51:25,390] m-LoRA: Adapter lora_gsm8k_30 loss: 0.6571193337440491
[2025-12-23 14:51:25,657] m-LoRA: Adapter lora_gsm8k_34 loss: 1.354193925857544
[2025-12-23 14:51:25,987] m-LoRA: Adapter lora_gsm8k_32 loss: 0.659403383731842
[2025-12-23 14:51:26,361] m-LoRA: Adapter lora_gsm8k_33 epoch: 1/1 iteration: 40/128 step: 6
[2025-12-23 14:51:26,487] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 74/128 step: 38
[2025-12-23 14:51:26,588] m-LoRA: Adapter lora_gsm8k_30 epoch: 1/1 iteration: 112/128 step: 29
[2025-12-23 14:51:26,805] m-LoRA: Adapter lora_gsm8k_34 epoch: 1/1 iteration: 16/128 step: 5
[2025-12-23 14:51:27,722] m-LoRA: Adapter lora_gsm8k_32 epoch: 1/1 iteration: 80/128 step: 21
[2025-12-23 14:51:27,766] m-LoRA: Adapter lora_gsm8k_33 loss: 1.286496877670288
[2025-12-23 14:51:27,936] m-LoRA: Adapter lora_gsm8k_29 loss: 0.6203319430351257
[2025-12-23 14:51:28,248] m-LoRA: Adapter lora_gsm8k_30 loss: 0.6034928560256958
[2025-12-23 14:51:28,572] m-LoRA: Adapter lora_gsm8k_34 loss: 1.304439663887024
[2025-12-23 14:51:28,781] m-LoRA: Adapter lora_gsm8k_32 loss: 0.6766877174377441
[2025-12-23 14:51:29,235] m-LoRA: Adapter lora_gsm8k_33 epoch: 1/1 iteration: 48/128 step: 7
[2025-12-23 14:51:29,381] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 76/128 step: 39
[2025-12-23 14:51:29,465] m-LoRA: Adapter lora_gsm8k_30 epoch: 1/1 iteration: 116/128 step: 30
[2025-12-23 14:51:29,761] m-LoRA: Adapter lora_gsm8k_34 epoch: 1/1 iteration: 20/128 step: 6
[2025-12-23 14:51:30,568] m-LoRA: Adapter lora_gsm8k_32 epoch: 1/1 iteration: 84/128 step: 22
[2025-12-23 14:51:30,626] m-LoRA: Adapter lora_gsm8k_33 loss: 1.2067137956619263
[2025-12-23 14:51:30,634] m-LoRA: Adapter lora_gsm8k_29 loss: 0.611310601234436
[2025-12-23 14:51:30,956] m-LoRA: Adapter lora_gsm8k_30 loss: 0.7601235508918762
[2025-12-23 14:51:31,207] m-LoRA: Adapter lora_gsm8k_34 loss: 1.4853061437606812
[2025-12-23 14:51:31,484] m-LoRA: Adapter lora_gsm8k_32 loss: 0.6400715708732605
[2025-12-23 14:51:32,124] m-LoRA: Adapter lora_gsm8k_33 epoch: 1/1 iteration: 56/128 step: 8
[2025-12-23 14:51:32,286] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 78/128 step: 40
[2025-12-23 14:51:32,388] m-LoRA: Adapter lora_gsm8k_30 epoch: 1/1 iteration: 120/128 step: 31
[2025-12-23 14:51:32,510] m-LoRA: Adapter lora_gsm8k_34 epoch: 1/1 iteration: 24/128 step: 7
[2025-12-23 14:51:32,789] m-LoRA: Adapter lora_gsm8k_32 epoch: 1/1 iteration: 88/128 step: 23
[2025-12-23 14:51:33,685] m-LoRA: Adapter lora_gsm8k_33 loss: 0.8924568891525269
[2025-12-23 14:51:33,698] m-LoRA: Adapter lora_gsm8k_29 loss: 0.610030472278595
[2025-12-23 14:51:34,083] m-LoRA: Adapter lora_gsm8k_30 loss: 0.7475895881652832
[2025-12-23 14:51:34,370] m-LoRA: Adapter lora_gsm8k_34 loss: 1.0202815532684326
[2025-12-23 14:51:34,638] m-LoRA: Adapter lora_gsm8k_32 loss: 0.6328132748603821
[2025-12-23 14:51:35,452] m-LoRA: Adapter lora_gsm8k_33 epoch: 1/1 iteration: 64/128 step: 9
[2025-12-23 14:51:35,613] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 80/128 step: 41
[2025-12-23 14:51:35,713] m-LoRA: Adapter lora_gsm8k_30 epoch: 1/1 iteration: 124/128 step: 32
[2025-12-23 14:51:35,805] m-LoRA: Adapter lora_gsm8k_34 epoch: 1/1 iteration: 28/128 step: 8
[2025-12-23 14:51:36,057] m-LoRA: Adapter lora_gsm8k_32 epoch: 1/1 iteration: 92/128 step: 24
[2025-12-23 14:51:36,917] m-LoRA: Adapter lora_gsm8k_33 loss: 0.9939693808555603
[2025-12-23 14:51:36,939] m-LoRA: Adapter lora_gsm8k_29 loss: 0.7325785160064697
[2025-12-23 14:51:37,362] m-LoRA: Adapter lora_gsm8k_30 loss: 0.6933541893959045
[2025-12-23 14:51:37,786] m-LoRA: Adapter lora_gsm8k_34 loss: 0.8244572281837463
[2025-12-23 14:51:38,155] m-LoRA: Adapter lora_gsm8k_32 loss: 0.6323534250259399
[2025-12-23 14:51:38,550] m-LoRA: Adapter lora_gsm8k_33 epoch: 1/1 iteration: 72/128 step: 10
[2025-12-23 14:51:38,726] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 82/128 step: 42
[2025-12-23 14:51:38,831] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_30']
[2025-12-23 14:51:39,013] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_35']
[2025-12-23 14:51:39,182] m-LoRA: Adapter lora_gsm8k_35 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:51:39,418] m-LoRA: Adapter lora_gsm8k_34 epoch: 1/1 iteration: 32/128 step: 9
[2025-12-23 14:51:39,587] m-LoRA: Adapter lora_gsm8k_32 epoch: 1/1 iteration: 96/128 step: 25
[2025-12-23 14:51:40,621] m-LoRA: Adapter lora_gsm8k_33 loss: 0.8069300055503845
[2025-12-23 14:51:40,638] m-LoRA: Adapter lora_gsm8k_29 loss: 0.632710874080658
[2025-12-23 14:51:41,001] m-LoRA: Adapter lora_gsm8k_35 loss: 2.499500036239624
[2025-12-23 14:51:41,249] m-LoRA: Adapter lora_gsm8k_34 loss: 0.8454862236976624
[2025-12-23 14:51:41,535] m-LoRA: Adapter lora_gsm8k_32 loss: 0.5115312337875366
[2025-12-23 14:51:42,536] m-LoRA: Adapter lora_gsm8k_33 epoch: 1/1 iteration: 80/128 step: 11
[2025-12-23 14:51:42,727] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 84/128 step: 43
[2025-12-23 14:51:42,831] m-LoRA: Adapter lora_gsm8k_35 epoch: 1/1 iteration: 4/128 step: 2
[2025-12-23 14:51:42,932] m-LoRA: Adapter lora_gsm8k_34 epoch: 1/1 iteration: 36/128 step: 10
[2025-12-23 14:51:43,088] m-LoRA: Adapter lora_gsm8k_32 epoch: 1/1 iteration: 100/128 step: 26
[2025-12-23 14:51:44,039] m-LoRA: Adapter lora_gsm8k_33 loss: 0.9996697902679443
[2025-12-23 14:51:44,057] m-LoRA: Adapter lora_gsm8k_29 loss: 0.629921019077301
[2025-12-23 14:51:44,455] m-LoRA: Adapter lora_gsm8k_35 loss: 1.5025866031646729
[2025-12-23 14:51:44,773] m-LoRA: Adapter lora_gsm8k_34 loss: 0.7025291323661804
[2025-12-23 14:51:45,160] m-LoRA: Adapter lora_gsm8k_32 loss: 0.49837154150009155
[2025-12-23 14:51:45,645] m-LoRA: Adapter lora_gsm8k_33 epoch: 1/1 iteration: 88/128 step: 12
[2025-12-23 14:51:45,820] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 86/128 step: 44
[2025-12-23 14:51:45,902] m-LoRA: Adapter lora_gsm8k_35 epoch: 1/1 iteration: 8/128 step: 3
[2025-12-23 14:51:46,184] m-LoRA: Adapter lora_gsm8k_34 epoch: 1/1 iteration: 40/128 step: 11
[2025-12-23 14:51:47,100] m-LoRA: Adapter lora_gsm8k_32 epoch: 1/1 iteration: 104/128 step: 27
[2025-12-23 14:51:47,155] m-LoRA: Adapter lora_gsm8k_33 loss: 0.9352030754089355
[2025-12-23 14:51:47,390] m-LoRA: Adapter lora_gsm8k_29 loss: 0.6940354704856873
[2025-12-23 14:51:47,798] m-LoRA: Adapter lora_gsm8k_35 loss: 1.1676273345947266
[2025-12-23 14:51:48,101] m-LoRA: Adapter lora_gsm8k_34 loss: 0.612372100353241
[2025-12-23 14:51:48,497] m-LoRA: Adapter lora_gsm8k_32 loss: 0.4026261270046234
[2025-12-23 14:51:48,621] m-LoRA: Adapter lora_gsm8k_33 epoch: 1/1 iteration: 96/128 step: 13
[2025-12-23 14:51:48,758] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 88/128 step: 45
[2025-12-23 14:51:50,536] m-LoRA: Adapter lora_gsm8k_35 epoch: 1/1 iteration: 12/128 step: 4
[2025-12-23 14:51:50,604] m-LoRA: Adapter lora_gsm8k_33 loss: 0.6743736267089844
[2025-12-23 14:51:50,852] m-LoRA: Adapter lora_gsm8k_29 loss: 0.5877826809883118
[2025-12-23 14:51:50,855] m-LoRA: Adapter lora_gsm8k_34 epoch: 1/1 iteration: 44/128 step: 12
[2025-12-23 14:51:50,989] m-LoRA: Adapter lora_gsm8k_32 epoch: 1/1 iteration: 108/128 step: 28
[2025-12-23 14:51:51,248] m-LoRA: Adapter lora_gsm8k_35 loss: 1.3467254638671875
[2025-12-23 14:51:52,416] m-LoRA: Adapter lora_gsm8k_34 loss: 0.587229311466217
[2025-12-23 14:51:52,420] m-LoRA: Adapter lora_gsm8k_33 epoch: 1/1 iteration: 104/128 step: 14
[2025-12-23 14:51:52,724] m-LoRA: Adapter lora_gsm8k_32 loss: 0.5284126996994019
[2025-12-23 14:51:52,728] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 90/128 step: 46
[2025-12-23 14:51:52,867] m-LoRA: Adapter lora_gsm8k_35 epoch: 1/1 iteration: 16/128 step: 5
[2025-12-23 14:51:53,854] m-LoRA: Adapter lora_gsm8k_34 epoch: 1/1 iteration: 48/128 step: 13
[2025-12-23 14:51:53,992] m-LoRA: Adapter lora_gsm8k_33 loss: 0.7068421244621277
[2025-12-23 14:51:53,998] m-LoRA: Adapter lora_gsm8k_32 epoch: 1/1 iteration: 112/128 step: 29
[2025-12-23 14:51:54,034] m-LoRA: Adapter lora_gsm8k_29 loss: 0.5316123962402344
[2025-12-23 14:51:54,307] m-LoRA: Adapter lora_gsm8k_35 loss: 1.5074903964996338
[2025-12-23 14:51:54,691] m-LoRA: Adapter lora_gsm8k_34 loss: 0.47330766916275024
[2025-12-23 14:51:54,976] m-LoRA: Adapter lora_gsm8k_32 loss: 0.5507557392120361
[2025-12-23 14:51:55,430] m-LoRA: Adapter lora_gsm8k_33 epoch: 1/1 iteration: 112/128 step: 15
[2025-12-23 14:51:55,559] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 92/128 step: 47
[2025-12-23 14:51:55,653] m-LoRA: Adapter lora_gsm8k_35 epoch: 1/1 iteration: 20/128 step: 6
[2025-12-23 14:51:55,987] m-LoRA: Adapter lora_gsm8k_34 epoch: 1/1 iteration: 52/128 step: 14
[2025-12-23 14:51:56,729] m-LoRA: Adapter lora_gsm8k_32 epoch: 1/1 iteration: 116/128 step: 30
[2025-12-23 14:51:56,777] m-LoRA: Adapter lora_gsm8k_33 loss: 0.6915871500968933
[2025-12-23 14:51:56,801] m-LoRA: Adapter lora_gsm8k_29 loss: 0.5561476349830627
[2025-12-23 14:51:57,187] m-LoRA: Adapter lora_gsm8k_35 loss: 1.204785943031311
[2025-12-23 14:51:57,484] m-LoRA: Adapter lora_gsm8k_34 loss: 0.6150412559509277
[2025-12-23 14:51:57,816] m-LoRA: Adapter lora_gsm8k_32 loss: 0.4709150195121765
[2025-12-23 14:51:58,186] m-LoRA: Adapter lora_gsm8k_33 epoch: 1/1 iteration: 120/128 step: 16
[2025-12-23 14:51:58,326] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 94/128 step: 48
[2025-12-23 14:51:58,418] m-LoRA: Adapter lora_gsm8k_35 epoch: 1/1 iteration: 24/128 step: 7
[2025-12-23 14:51:58,695] m-LoRA: Adapter lora_gsm8k_34 epoch: 1/1 iteration: 56/128 step: 15
[2025-12-23 14:51:59,564] m-LoRA: Adapter lora_gsm8k_32 epoch: 1/1 iteration: 120/128 step: 31
[2025-12-23 14:51:59,625] m-LoRA: Adapter lora_gsm8k_33 loss: 0.6149778962135315
[2025-12-23 14:51:59,640] m-LoRA: Adapter lora_gsm8k_29 loss: 0.5802583694458008
[2025-12-23 14:51:59,946] m-LoRA: Adapter lora_gsm8k_35 loss: 1.1682628393173218
[2025-12-23 14:52:00,249] m-LoRA: Adapter lora_gsm8k_34 loss: 0.6210365295410156
[2025-12-23 14:52:00,419] m-LoRA: Adapter lora_gsm8k_32 loss: 0.5699653625488281
[2025-12-23 14:52:01,105] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_33']
[2025-12-23 14:52:01,887] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_36']
[2025-12-23 14:52:02,041] m-LoRA: Adapter lora_gsm8k_36 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:52:02,221] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 96/128 step: 49
[2025-12-23 14:52:02,319] m-LoRA: Adapter lora_gsm8k_35 epoch: 1/1 iteration: 28/128 step: 8
[2025-12-23 14:52:02,439] m-LoRA: Adapter lora_gsm8k_34 epoch: 1/1 iteration: 60/128 step: 16
[2025-12-23 14:52:02,623] m-LoRA: Adapter lora_gsm8k_32 epoch: 1/1 iteration: 124/128 step: 32
[2025-12-23 14:52:04,640] m-LoRA: Adapter lora_gsm8k_36 loss: 2.4824016094207764
[2025-12-23 14:52:04,657] m-LoRA: Adapter lora_gsm8k_29 loss: 0.502508282661438
[2025-12-23 14:52:05,018] m-LoRA: Adapter lora_gsm8k_35 loss: 1.0291032791137695
[2025-12-23 14:52:05,318] m-LoRA: Adapter lora_gsm8k_34 loss: 0.6078979969024658
[2025-12-23 14:52:05,720] m-LoRA: Adapter lora_gsm8k_32 loss: 0.4754906892776489
[2025-12-23 14:52:07,541] m-LoRA: Adapter lora_gsm8k_36 epoch: 1/1 iteration: 16/128 step: 2
[2025-12-23 14:52:07,817] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 98/128 step: 50
[2025-12-23 14:52:07,913] m-LoRA: Adapter lora_gsm8k_35 epoch: 1/1 iteration: 32/128 step: 9
[2025-12-23 14:52:07,998] m-LoRA: Adapter lora_gsm8k_34 epoch: 1/1 iteration: 64/128 step: 17
[2025-12-23 14:52:08,095] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_32']
[2025-12-23 14:52:08,684] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_37']
[2025-12-23 14:52:08,753] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:52:11,146] m-LoRA: Adapter lora_gsm8k_36 loss: 1.2880429029464722
[2025-12-23 14:52:11,161] m-LoRA: Adapter lora_gsm8k_29 loss: 0.5231941342353821
[2025-12-23 14:52:11,445] m-LoRA: Adapter lora_gsm8k_35 loss: 0.7155671715736389
[2025-12-23 14:52:11,679] m-LoRA: Adapter lora_gsm8k_34 loss: 0.581935703754425
[2025-12-23 14:52:11,810] m-LoRA: Adapter lora_gsm8k_37 loss: 1.9974524974822998
[2025-12-23 14:52:14,469] m-LoRA: Adapter lora_gsm8k_36 epoch: 1/1 iteration: 32/128 step: 3
[2025-12-23 14:52:14,715] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 100/128 step: 51
[2025-12-23 14:52:14,791] m-LoRA: Adapter lora_gsm8k_35 epoch: 1/1 iteration: 36/128 step: 10
[2025-12-23 14:52:14,881] m-LoRA: Adapter lora_gsm8k_34 epoch: 1/1 iteration: 68/128 step: 18
[2025-12-23 14:52:14,979] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 2/128 step: 2
[2025-12-23 14:52:15,234] m-LoRA: Adapter lora_gsm8k_29 loss: 0.536107063293457
[2025-12-23 14:52:16,944] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 102/128 step: 52
[2025-12-23 14:52:17,043] m-LoRA: Adapter lora_gsm8k_36 loss: 1.2604042291641235
[2025-12-23 14:52:17,191] m-LoRA: Adapter lora_gsm8k_35 loss: 0.6126447319984436
[2025-12-23 14:52:17,514] m-LoRA: Adapter lora_gsm8k_34 loss: 0.5367019176483154
[2025-12-23 14:52:17,571] m-LoRA: Adapter lora_gsm8k_37 loss: 2.328984022140503
[2025-12-23 14:52:17,665] m-LoRA: Adapter lora_gsm8k_29 loss: 0.9472144246101379
[2025-12-23 14:52:19,982] m-LoRA: Adapter lora_gsm8k_36 epoch: 1/1 iteration: 48/128 step: 4
[2025-12-23 14:52:20,236] m-LoRA: Adapter lora_gsm8k_35 epoch: 1/1 iteration: 40/128 step: 11
[2025-12-23 14:52:20,341] m-LoRA: Adapter lora_gsm8k_34 epoch: 1/1 iteration: 72/128 step: 19
[2025-12-23 14:52:20,440] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 4/128 step: 3
[2025-12-23 14:52:20,519] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 104/128 step: 53
[2025-12-23 14:52:22,102] m-LoRA: Adapter lora_gsm8k_36 loss: 1.3703961372375488
[2025-12-23 14:52:22,340] m-LoRA: Adapter lora_gsm8k_35 loss: 0.6385452747344971
[2025-12-23 14:52:22,606] m-LoRA: Adapter lora_gsm8k_34 loss: 0.6928953528404236
[2025-12-23 14:52:22,700] m-LoRA: Adapter lora_gsm8k_37 loss: 2.4588875770568848
[2025-12-23 14:52:22,882] m-LoRA: Adapter lora_gsm8k_29 loss: 0.5115070343017578
[2025-12-23 14:52:24,560] m-LoRA: Adapter lora_gsm8k_36 epoch: 1/1 iteration: 64/128 step: 5
[2025-12-23 14:52:24,832] m-LoRA: Adapter lora_gsm8k_35 epoch: 1/1 iteration: 44/128 step: 12
[2025-12-23 14:52:24,929] m-LoRA: Adapter lora_gsm8k_34 epoch: 1/1 iteration: 76/128 step: 20
[2025-12-23 14:52:25,012] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 6/128 step: 4
[2025-12-23 14:52:25,088] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 106/128 step: 54
[2025-12-23 14:52:27,641] m-LoRA: Adapter lora_gsm8k_36 loss: 0.9718984961509705
[2025-12-23 14:52:27,687] m-LoRA: Adapter lora_gsm8k_35 loss: 0.5417894124984741
[2025-12-23 14:52:28,003] m-LoRA: Adapter lora_gsm8k_34 loss: 0.6023162603378296
[2025-12-23 14:52:28,208] m-LoRA: Adapter lora_gsm8k_37 loss: 1.7523597478866577
[2025-12-23 14:52:28,308] m-LoRA: Adapter lora_gsm8k_29 loss: 0.45662233233451843
[2025-12-23 14:52:31,244] m-LoRA: Adapter lora_gsm8k_36 epoch: 1/1 iteration: 80/128 step: 6
[2025-12-23 14:52:31,502] m-LoRA: Adapter lora_gsm8k_35 epoch: 1/1 iteration: 48/128 step: 13
[2025-12-23 14:52:31,628] m-LoRA: Adapter lora_gsm8k_34 epoch: 1/1 iteration: 80/128 step: 21
[2025-12-23 14:52:31,719] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 8/128 step: 5
[2025-12-23 14:52:31,801] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 108/128 step: 55
[2025-12-23 14:52:33,769] m-LoRA: Adapter lora_gsm8k_36 loss: 1.0885783433914185
[2025-12-23 14:52:34,044] m-LoRA: Adapter lora_gsm8k_35 loss: 0.7118229866027832
[2025-12-23 14:52:34,354] m-LoRA: Adapter lora_gsm8k_34 loss: 0.415161669254303
[2025-12-23 14:52:34,440] m-LoRA: Adapter lora_gsm8k_37 loss: 2.169100522994995
[2025-12-23 14:52:34,525] m-LoRA: Adapter lora_gsm8k_29 loss: 0.5041297078132629
[2025-12-23 14:52:36,657] m-LoRA: Adapter lora_gsm8k_36 epoch: 1/1 iteration: 96/128 step: 7
[2025-12-23 14:52:36,914] m-LoRA: Adapter lora_gsm8k_35 epoch: 1/1 iteration: 52/128 step: 14
[2025-12-23 14:52:36,994] m-LoRA: Adapter lora_gsm8k_34 epoch: 1/1 iteration: 84/128 step: 22
[2025-12-23 14:52:37,097] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 10/128 step: 6
[2025-12-23 14:52:37,176] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 110/128 step: 56
[2025-12-23 14:52:38,956] m-LoRA: Adapter lora_gsm8k_36 loss: 1.2129225730895996
[2025-12-23 14:52:38,992] m-LoRA: Adapter lora_gsm8k_35 loss: 0.49736782908439636
[2025-12-23 14:52:39,386] m-LoRA: Adapter lora_gsm8k_34 loss: 0.6395919322967529
[2025-12-23 14:52:39,510] m-LoRA: Adapter lora_gsm8k_37 loss: 1.7889305353164673
[2025-12-23 14:52:39,652] m-LoRA: Adapter lora_gsm8k_29 loss: 0.7831921577453613
[2025-12-23 14:52:41,607] m-LoRA: Adapter lora_gsm8k_36 epoch: 1/1 iteration: 112/128 step: 8
[2025-12-23 14:52:41,865] m-LoRA: Adapter lora_gsm8k_35 epoch: 1/1 iteration: 56/128 step: 15
[2025-12-23 14:52:41,962] m-LoRA: Adapter lora_gsm8k_34 epoch: 1/1 iteration: 88/128 step: 23
[2025-12-23 14:52:42,043] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 12/128 step: 7
[2025-12-23 14:52:42,120] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 112/128 step: 57
[2025-12-23 14:52:43,939] m-LoRA: Adapter lora_gsm8k_36 loss: 1.1037609577178955
[2025-12-23 14:52:43,975] m-LoRA: Adapter lora_gsm8k_35 loss: 0.657285213470459
[2025-12-23 14:52:44,322] m-LoRA: Adapter lora_gsm8k_34 loss: 0.6212859153747559
[2025-12-23 14:52:44,443] m-LoRA: Adapter lora_gsm8k_37 loss: 1.87666916847229
[2025-12-23 14:52:44,613] m-LoRA: Adapter lora_gsm8k_29 loss: 0.5965384244918823
[2025-12-23 14:52:46,659] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_36']
[2025-12-23 14:52:47,483] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_38']
[2025-12-23 14:52:47,517] m-LoRA: Adapter lora_gsm8k_38 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:52:47,622] m-LoRA: Adapter lora_gsm8k_35 epoch: 1/1 iteration: 60/128 step: 16
[2025-12-23 14:52:47,710] m-LoRA: Adapter lora_gsm8k_34 epoch: 1/1 iteration: 92/128 step: 24
[2025-12-23 14:52:47,803] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 14/128 step: 8
[2025-12-23 14:52:47,878] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 114/128 step: 58
[2025-12-23 14:52:48,692] m-LoRA: Adapter lora_gsm8k_38 loss: 2.4972169399261475
[2025-12-23 14:52:49,007] m-LoRA: Adapter lora_gsm8k_35 loss: 0.6365197896957397
[2025-12-23 14:52:49,302] m-LoRA: Adapter lora_gsm8k_34 loss: 0.5529541969299316
[2025-12-23 14:52:49,428] m-LoRA: Adapter lora_gsm8k_37 loss: 1.7465811967849731
[2025-12-23 14:52:49,638] m-LoRA: Adapter lora_gsm8k_29 loss: 0.8582866191864014
[2025-12-23 14:52:50,030] m-LoRA: Adapter lora_gsm8k_38 epoch: 1/1 iteration: 8/128 step: 2
[2025-12-23 14:52:50,286] m-LoRA: Adapter lora_gsm8k_35 epoch: 1/1 iteration: 64/128 step: 17
[2025-12-23 14:52:51,768] m-LoRA: Adapter lora_gsm8k_34 epoch: 1/1 iteration: 96/128 step: 25
[2025-12-23 14:52:51,822] m-LoRA: Adapter lora_gsm8k_38 loss: 1.9893537759780884
[2025-12-23 14:52:52,107] m-LoRA: Adapter lora_gsm8k_35 loss: 0.5447277426719666
[2025-12-23 14:52:52,111] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 16/128 step: 9
[2025-12-23 14:52:52,226] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 116/128 step: 59
[2025-12-23 14:52:52,440] m-LoRA: Adapter lora_gsm8k_34 loss: 0.5691630840301514
[2025-12-23 14:52:52,947] m-LoRA: Adapter lora_gsm8k_37 loss: 1.569861888885498
[2025-12-23 14:52:53,112] m-LoRA: Adapter lora_gsm8k_38 epoch: 1/1 iteration: 16/128 step: 3
[2025-12-23 14:52:53,203] m-LoRA: Adapter lora_gsm8k_29 loss: 0.5999332070350647
[2025-12-23 14:52:53,358] m-LoRA: Adapter lora_gsm8k_35 epoch: 1/1 iteration: 68/128 step: 18
[2025-12-23 14:52:53,587] m-LoRA: Adapter lora_gsm8k_34 epoch: 1/1 iteration: 100/128 step: 26
[2025-12-23 14:52:53,687] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 18/128 step: 10
[2025-12-23 14:52:54,676] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 118/128 step: 60
[2025-12-23 14:52:54,739] m-LoRA: Adapter lora_gsm8k_38 loss: 1.0637868642807007
[2025-12-23 14:52:55,116] m-LoRA: Adapter lora_gsm8k_35 loss: 0.5165178775787354
[2025-12-23 14:52:55,353] m-LoRA: Adapter lora_gsm8k_34 loss: 0.6225072145462036
[2025-12-23 14:52:55,465] m-LoRA: Adapter lora_gsm8k_37 loss: 1.5863569974899292
[2025-12-23 14:52:55,638] m-LoRA: Adapter lora_gsm8k_29 loss: 0.6658631563186646
[2025-12-23 14:52:56,495] m-LoRA: Adapter lora_gsm8k_38 epoch: 1/1 iteration: 24/128 step: 4
[2025-12-23 14:52:56,648] m-LoRA: Adapter lora_gsm8k_35 epoch: 1/1 iteration: 72/128 step: 19
[2025-12-23 14:52:56,782] m-LoRA: Adapter lora_gsm8k_34 epoch: 1/1 iteration: 104/128 step: 27
[2025-12-23 14:52:56,916] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 20/128 step: 11
[2025-12-23 14:52:57,631] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 120/128 step: 61
[2025-12-23 14:52:57,667] m-LoRA: Adapter lora_gsm8k_38 loss: 1.4353275299072266
[2025-12-23 14:52:57,829] m-LoRA: Adapter lora_gsm8k_35 loss: 0.6378014087677002
[2025-12-23 14:52:58,247] m-LoRA: Adapter lora_gsm8k_34 loss: 0.6163482666015625
[2025-12-23 14:52:58,373] m-LoRA: Adapter lora_gsm8k_37 loss: 1.6479674577713013
[2025-12-23 14:52:58,578] m-LoRA: Adapter lora_gsm8k_29 loss: 0.6403102874755859
[2025-12-23 14:52:59,031] m-LoRA: Adapter lora_gsm8k_38 epoch: 1/1 iteration: 32/128 step: 5
[2025-12-23 14:52:59,150] m-LoRA: Adapter lora_gsm8k_35 epoch: 1/1 iteration: 76/128 step: 20
[2025-12-23 14:52:59,459] m-LoRA: Adapter lora_gsm8k_34 epoch: 1/1 iteration: 108/128 step: 28
[2025-12-23 14:52:59,543] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 22/128 step: 12
[2025-12-23 14:53:00,290] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 122/128 step: 62
[2025-12-23 14:53:00,335] m-LoRA: Adapter lora_gsm8k_38 loss: 1.2619600296020508
[2025-12-23 14:53:00,641] m-LoRA: Adapter lora_gsm8k_35 loss: 0.6843518018722534
[2025-12-23 14:53:00,943] m-LoRA: Adapter lora_gsm8k_34 loss: 0.46130865812301636
[2025-12-23 14:53:01,016] m-LoRA: Adapter lora_gsm8k_37 loss: 1.8268555402755737
[2025-12-23 14:53:01,143] m-LoRA: Adapter lora_gsm8k_29 loss: 0.5840768814086914
[2025-12-23 14:53:01,737] m-LoRA: Adapter lora_gsm8k_38 epoch: 1/1 iteration: 40/128 step: 6
[2025-12-23 14:53:01,930] m-LoRA: Adapter lora_gsm8k_35 epoch: 1/1 iteration: 80/128 step: 21
[2025-12-23 14:53:02,201] m-LoRA: Adapter lora_gsm8k_34 epoch: 1/1 iteration: 112/128 step: 29
[2025-12-23 14:53:02,917] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 24/128 step: 13
[2025-12-23 14:53:02,959] m-LoRA: Adapter lora_gsm8k_38 loss: 1.2737154960632324
[2025-12-23 14:53:03,230] m-LoRA: Adapter lora_gsm8k_35 loss: 0.5246537923812866
[2025-12-23 14:53:03,233] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 124/128 step: 63
[2025-12-23 14:53:03,453] m-LoRA: Adapter lora_gsm8k_34 loss: 0.6058922410011292
[2025-12-23 14:53:03,568] m-LoRA: Adapter lora_gsm8k_37 loss: 1.805509090423584
[2025-12-23 14:53:04,196] m-LoRA: Adapter lora_gsm8k_29 loss: 0.6062743663787842
[2025-12-23 14:53:04,360] m-LoRA: Adapter lora_gsm8k_38 epoch: 1/1 iteration: 48/128 step: 7
[2025-12-23 14:53:04,498] m-LoRA: Adapter lora_gsm8k_35 epoch: 1/1 iteration: 84/128 step: 22
[2025-12-23 14:53:04,633] m-LoRA: Adapter lora_gsm8k_34 epoch: 1/1 iteration: 116/128 step: 30
[2025-12-23 14:53:04,746] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 26/128 step: 14
[2025-12-23 14:53:04,842] m-LoRA: Adapter lora_gsm8k_29 epoch: 1/1 iteration: 126/128 step: 64
[2025-12-23 14:53:05,705] m-LoRA: Adapter lora_gsm8k_38 loss: 1.097116470336914
[2025-12-23 14:53:05,997] m-LoRA: Adapter lora_gsm8k_35 loss: 0.4853686988353729
[2025-12-23 14:53:06,253] m-LoRA: Adapter lora_gsm8k_34 loss: 0.551946759223938
[2025-12-23 14:53:06,371] m-LoRA: Adapter lora_gsm8k_37 loss: 1.6697323322296143
[2025-12-23 14:53:06,502] m-LoRA: Adapter lora_gsm8k_29 loss: 0.6103329062461853
[2025-12-23 14:53:07,249] m-LoRA: Adapter lora_gsm8k_38 epoch: 1/1 iteration: 56/128 step: 8
[2025-12-23 14:53:07,374] m-LoRA: Adapter lora_gsm8k_35 epoch: 1/1 iteration: 88/128 step: 23
[2025-12-23 14:53:07,513] m-LoRA: Adapter lora_gsm8k_34 epoch: 1/1 iteration: 120/128 step: 31
[2025-12-23 14:53:07,639] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 28/128 step: 15
[2025-12-23 14:53:07,724] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_29']
[2025-12-23 14:53:08,093] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_39']
[2025-12-23 14:53:08,189] m-LoRA: Adapter lora_gsm8k_39 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:53:08,450] m-LoRA: Adapter lora_gsm8k_38 loss: 1.4198317527770996
[2025-12-23 14:53:08,745] m-LoRA: Adapter lora_gsm8k_35 loss: 0.3993389904499054
[2025-12-23 14:53:09,055] m-LoRA: Adapter lora_gsm8k_34 loss: 0.5462926626205444
[2025-12-23 14:53:09,204] m-LoRA: Adapter lora_gsm8k_37 loss: 1.393916368484497
[2025-12-23 14:53:10,593] m-LoRA: Adapter lora_gsm8k_38 epoch: 1/1 iteration: 64/128 step: 9
[2025-12-23 14:53:10,744] m-LoRA: Adapter lora_gsm8k_39 loss: 2.419095993041992
[2025-12-23 14:53:10,757] m-LoRA: Adapter lora_gsm8k_35 epoch: 1/1 iteration: 92/128 step: 24
[2025-12-23 14:53:10,877] m-LoRA: Adapter lora_gsm8k_34 epoch: 1/1 iteration: 124/128 step: 32
[2025-12-23 14:53:11,150] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 30/128 step: 16
[2025-12-23 14:53:11,853] m-LoRA: Adapter lora_gsm8k_38 loss: 1.252448320388794
[2025-12-23 14:53:12,089] m-LoRA: Adapter lora_gsm8k_35 loss: 0.583355188369751
[2025-12-23 14:53:13,308] m-LoRA: Adapter lora_gsm8k_34 loss: 0.45037272572517395
[2025-12-23 14:53:13,552] m-LoRA: Adapter lora_gsm8k_37 loss: 1.8293524980545044
[2025-12-23 14:53:13,704] m-LoRA: Adapter lora_gsm8k_39 epoch: 1/1 iteration: 16/128 step: 2
[2025-12-23 14:53:13,946] m-LoRA: Adapter lora_gsm8k_38 epoch: 1/1 iteration: 72/128 step: 10
[2025-12-23 14:53:14,068] m-LoRA: Adapter lora_gsm8k_35 epoch: 1/1 iteration: 96/128 step: 25
[2025-12-23 14:53:14,210] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_34']
[2025-12-23 14:53:14,816] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_40']
[2025-12-23 14:53:14,896] m-LoRA: Adapter lora_gsm8k_40 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:53:15,905] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 32/128 step: 17
[2025-12-23 14:53:16,018] m-LoRA: Adapter lora_gsm8k_39 loss: 2.538207530975342
[2025-12-23 14:53:16,694] m-LoRA: Adapter lora_gsm8k_38 loss: 1.2619856595993042
[2025-12-23 14:53:16,914] m-LoRA: Adapter lora_gsm8k_35 loss: 0.5615391731262207
[2025-12-23 14:53:18,957] m-LoRA: Adapter lora_gsm8k_40 loss: 2.3060548305511475
[2025-12-23 14:53:18,972] m-LoRA: Adapter lora_gsm8k_39 epoch: 1/1 iteration: 32/128 step: 3
[2025-12-23 14:53:19,275] m-LoRA: Adapter lora_gsm8k_37 loss: 1.6737539768218994
[2025-12-23 14:53:19,278] m-LoRA: Adapter lora_gsm8k_38 epoch: 1/1 iteration: 80/128 step: 11
[2025-12-23 14:53:19,476] m-LoRA: Adapter lora_gsm8k_35 epoch: 1/1 iteration: 100/128 step: 26
[2025-12-23 14:53:22,763] m-LoRA: Adapter lora_gsm8k_39 loss: 2.34696102142334
[2025-12-23 14:53:23,010] m-LoRA: Adapter lora_gsm8k_40 epoch: 1/1 iteration: 16/128 step: 2
[2025-12-23 14:53:23,339] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 34/128 step: 18
[2025-12-23 14:53:23,377] m-LoRA: Adapter lora_gsm8k_38 loss: 1.3772133588790894
[2025-12-23 14:53:23,603] m-LoRA: Adapter lora_gsm8k_35 loss: 0.6875412464141846
[2025-12-23 14:53:25,199] m-LoRA: Adapter lora_gsm8k_39 epoch: 1/1 iteration: 48/128 step: 4
[2025-12-23 14:53:26,798] m-LoRA: Adapter lora_gsm8k_40 loss: 2.2901504039764404
[2025-12-23 14:53:26,811] m-LoRA: Adapter lora_gsm8k_38 epoch: 1/1 iteration: 88/128 step: 12
[2025-12-23 14:53:26,917] m-LoRA: Adapter lora_gsm8k_37 loss: 1.5878084897994995
[2025-12-23 14:53:26,919] m-LoRA: Adapter lora_gsm8k_35 epoch: 1/1 iteration: 104/128 step: 27
[2025-12-23 14:53:28,199] m-LoRA: Adapter lora_gsm8k_39 loss: 2.1368825435638428
[2025-12-23 14:53:28,917] m-LoRA: Adapter lora_gsm8k_38 loss: 1.2111257314682007
[2025-12-23 14:53:29,310] m-LoRA: Adapter lora_gsm8k_35 loss: 0.6317006349563599
[2025-12-23 14:53:29,814] m-LoRA: Adapter lora_gsm8k_40 epoch: 1/1 iteration: 32/128 step: 3
[2025-12-23 14:53:30,134] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 36/128 step: 19
[2025-12-23 14:53:31,027] m-LoRA: Adapter lora_gsm8k_39 epoch: 1/1 iteration: 64/128 step: 5
[2025-12-23 14:53:32,612] m-LoRA: Adapter lora_gsm8k_40 loss: 2.1659204959869385
[2025-12-23 14:53:32,625] m-LoRA: Adapter lora_gsm8k_38 epoch: 1/1 iteration: 96/128 step: 13
[2025-12-23 14:53:32,898] m-LoRA: Adapter lora_gsm8k_37 loss: 1.5016052722930908
[2025-12-23 14:53:32,900] m-LoRA: Adapter lora_gsm8k_35 epoch: 1/1 iteration: 108/128 step: 28
[2025-12-23 14:53:34,554] m-LoRA: Adapter lora_gsm8k_39 loss: 1.5070381164550781
[2025-12-23 14:53:35,408] m-LoRA: Adapter lora_gsm8k_38 loss: 1.0749849081039429
[2025-12-23 14:53:35,645] m-LoRA: Adapter lora_gsm8k_35 loss: 0.663286030292511
[2025-12-23 14:53:35,757] m-LoRA: Adapter lora_gsm8k_40 epoch: 1/1 iteration: 48/128 step: 4
[2025-12-23 14:53:35,966] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 38/128 step: 20
[2025-12-23 14:53:38,868] m-LoRA: Adapter lora_gsm8k_40 loss: 1.9274537563323975
[2025-12-23 14:53:39,131] m-LoRA: Adapter lora_gsm8k_37 loss: 1.7533913850784302
[2025-12-23 14:53:39,133] m-LoRA: Adapter lora_gsm8k_39 epoch: 1/1 iteration: 80/128 step: 6
[2025-12-23 14:53:39,328] m-LoRA: Adapter lora_gsm8k_38 epoch: 1/1 iteration: 104/128 step: 14
[2025-12-23 14:53:39,481] m-LoRA: Adapter lora_gsm8k_35 epoch: 1/1 iteration: 112/128 step: 29
[2025-12-23 14:53:42,194] m-LoRA: Adapter lora_gsm8k_39 loss: 1.9177308082580566
[2025-12-23 14:53:42,418] m-LoRA: Adapter lora_gsm8k_40 epoch: 1/1 iteration: 64/128 step: 5
[2025-12-23 14:53:42,912] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 40/128 step: 21
[2025-12-23 14:53:42,972] m-LoRA: Adapter lora_gsm8k_38 loss: 1.1477025747299194
[2025-12-23 14:53:43,164] m-LoRA: Adapter lora_gsm8k_35 loss: 0.6690052151679993
[2025-12-23 14:53:45,717] m-LoRA: Adapter lora_gsm8k_40 loss: 2.01566219329834
[2025-12-23 14:53:45,815] m-LoRA: Adapter lora_gsm8k_39 epoch: 1/1 iteration: 96/128 step: 7
[2025-12-23 14:53:46,147] m-LoRA: Adapter lora_gsm8k_37 loss: 1.4080824851989746
[2025-12-23 14:53:46,151] m-LoRA: Adapter lora_gsm8k_38 epoch: 1/1 iteration: 112/128 step: 15
[2025-12-23 14:53:46,297] m-LoRA: Adapter lora_gsm8k_35 epoch: 1/1 iteration: 116/128 step: 30
[2025-12-23 14:53:49,626] m-LoRA: Adapter lora_gsm8k_39 loss: 1.3554540872573853
[2025-12-23 14:53:49,735] m-LoRA: Adapter lora_gsm8k_40 epoch: 1/1 iteration: 80/128 step: 6
[2025-12-23 14:53:50,416] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 42/128 step: 22
[2025-12-23 14:53:50,483] m-LoRA: Adapter lora_gsm8k_38 loss: 0.9054514169692993
[2025-12-23 14:53:50,722] m-LoRA: Adapter lora_gsm8k_35 loss: 0.4724682867527008
[2025-12-23 14:53:53,999] m-LoRA: Adapter lora_gsm8k_40 loss: 1.647692084312439
[2025-12-23 14:53:54,260] m-LoRA: Adapter lora_gsm8k_37 loss: 1.9874523878097534
[2025-12-23 14:53:54,262] m-LoRA: Adapter lora_gsm8k_39 epoch: 1/1 iteration: 112/128 step: 8
[2025-12-23 14:53:54,452] m-LoRA: Adapter lora_gsm8k_38 epoch: 1/1 iteration: 120/128 step: 16
[2025-12-23 14:53:54,579] m-LoRA: Adapter lora_gsm8k_35 epoch: 1/1 iteration: 120/128 step: 31
[2025-12-23 14:53:58,032] m-LoRA: Adapter lora_gsm8k_39 loss: 1.4758471250534058
[2025-12-23 14:53:58,278] m-LoRA: Adapter lora_gsm8k_40 epoch: 1/1 iteration: 96/128 step: 7
[2025-12-23 14:53:58,699] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 44/128 step: 23
[2025-12-23 14:53:58,747] m-LoRA: Adapter lora_gsm8k_38 loss: 1.1700096130371094
[2025-12-23 14:53:59,008] m-LoRA: Adapter lora_gsm8k_35 loss: 0.47576314210891724
[2025-12-23 14:54:01,845] m-LoRA: Adapter lora_gsm8k_40 loss: 1.2630717754364014
[2025-12-23 14:54:01,954] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_39']
[2025-12-23 14:54:02,474] m-LoRA: Adapter lora_gsm8k_37 loss: 1.945139765739441
[2025-12-23 14:54:02,476] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_41']
[2025-12-23 14:54:02,670] m-LoRA: Adapter lora_gsm8k_41 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:54:02,809] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_38']
[2025-12-23 14:54:02,949] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_42']
[2025-12-23 14:54:03,119] m-LoRA: Adapter lora_gsm8k_42 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:54:03,301] m-LoRA: Adapter lora_gsm8k_35 epoch: 1/1 iteration: 124/128 step: 32
[2025-12-23 14:54:05,061] m-LoRA: Adapter lora_gsm8k_41 loss: 2.3223674297332764
[2025-12-23 14:54:05,227] m-LoRA: Adapter lora_gsm8k_40 epoch: 1/1 iteration: 112/128 step: 8
[2025-12-23 14:54:06,505] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 46/128 step: 24
[2025-12-23 14:54:06,597] m-LoRA: Adapter lora_gsm8k_42 loss: 2.454542636871338
[2025-12-23 14:54:06,972] m-LoRA: Adapter lora_gsm8k_35 loss: 0.5058654546737671
[2025-12-23 14:54:06,976] m-LoRA: Adapter lora_gsm8k_41 epoch: 1/1 iteration: 8/128 step: 2
[2025-12-23 14:54:08,087] m-LoRA: Adapter lora_gsm8k_40 loss: 1.4435858726501465
[2025-12-23 14:54:08,101] m-LoRA: Adapter lora_gsm8k_37 loss: 1.5474036931991577
[2025-12-23 14:54:09,536] m-LoRA: Adapter lora_gsm8k_41 loss: 1.3579500913619995
[2025-12-23 14:54:09,796] m-LoRA: Adapter lora_gsm8k_42 epoch: 1/1 iteration: 16/128 step: 2
[2025-12-23 14:54:10,002] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_35']
[2025-12-23 14:54:10,558] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_43']
[2025-12-23 14:54:10,645] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:54:11,574] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_40']
[2025-12-23 14:54:11,876] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_44']
[2025-12-23 14:54:11,953] m-LoRA: Adapter lora_gsm8k_44 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:54:12,016] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 48/128 step: 25
[2025-12-23 14:54:13,012] m-LoRA: Adapter lora_gsm8k_41 epoch: 1/1 iteration: 16/128 step: 3
[2025-12-23 14:54:13,157] m-LoRA: Adapter lora_gsm8k_42 loss: 1.398730754852295
[2025-12-23 14:54:13,170] m-LoRA: Adapter lora_gsm8k_43 loss: 1.8009366989135742
[2025-12-23 14:54:13,579] m-LoRA: Adapter lora_gsm8k_44 loss: 2.4128754138946533
[2025-12-23 14:54:13,676] m-LoRA: Adapter lora_gsm8k_37 loss: 1.4283548593521118
[2025-12-23 14:54:14,331] m-LoRA: Adapter lora_gsm8k_41 loss: 1.3254106044769287
[2025-12-23 14:54:15,845] m-LoRA: Adapter lora_gsm8k_42 epoch: 1/1 iteration: 32/128 step: 3
[2025-12-23 14:54:16,076] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 2/128 step: 2
[2025-12-23 14:54:16,155] m-LoRA: Adapter lora_gsm8k_44 epoch: 1/1 iteration: 4/128 step: 2
[2025-12-23 14:54:16,232] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 50/128 step: 26
[2025-12-23 14:54:16,524] m-LoRA: Adapter lora_gsm8k_41 epoch: 1/1 iteration: 24/128 step: 4
[2025-12-23 14:54:18,529] m-LoRA: Adapter lora_gsm8k_42 loss: 1.1191993951797485
[2025-12-23 14:54:18,543] m-LoRA: Adapter lora_gsm8k_43 loss: 2.426478862762451
[2025-12-23 14:54:18,815] m-LoRA: Adapter lora_gsm8k_44 loss: 2.5904784202575684
[2025-12-23 14:54:18,820] m-LoRA: Adapter lora_gsm8k_37 loss: 1.3989169597625732
[2025-12-23 14:54:20,147] m-LoRA: Adapter lora_gsm8k_41 loss: 1.2800709009170532
[2025-12-23 14:54:21,783] m-LoRA: Adapter lora_gsm8k_42 epoch: 1/1 iteration: 48/128 step: 4
[2025-12-23 14:54:22,022] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 4/128 step: 3
[2025-12-23 14:54:22,100] m-LoRA: Adapter lora_gsm8k_44 epoch: 1/1 iteration: 8/128 step: 3
[2025-12-23 14:54:22,181] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 52/128 step: 27
[2025-12-23 14:54:22,269] m-LoRA: Adapter lora_gsm8k_41 epoch: 1/1 iteration: 32/128 step: 5
[2025-12-23 14:54:24,375] m-LoRA: Adapter lora_gsm8k_42 loss: 1.1498385667800903
[2025-12-23 14:54:24,391] m-LoRA: Adapter lora_gsm8k_43 loss: 2.201871395111084
[2025-12-23 14:54:24,799] m-LoRA: Adapter lora_gsm8k_44 loss: 2.1257472038269043
[2025-12-23 14:54:24,922] m-LoRA: Adapter lora_gsm8k_37 loss: 1.5262407064437866
[2025-12-23 14:54:26,436] m-LoRA: Adapter lora_gsm8k_41 loss: 1.0774433612823486
[2025-12-23 14:54:27,542] m-LoRA: Adapter lora_gsm8k_42 epoch: 1/1 iteration: 64/128 step: 5
[2025-12-23 14:54:27,786] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 6/128 step: 4
[2025-12-23 14:54:27,872] m-LoRA: Adapter lora_gsm8k_44 epoch: 1/1 iteration: 12/128 step: 4
[2025-12-23 14:54:27,949] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 54/128 step: 28
[2025-12-23 14:54:28,282] m-LoRA: Adapter lora_gsm8k_41 epoch: 1/1 iteration: 40/128 step: 6
[2025-12-23 14:54:30,366] m-LoRA: Adapter lora_gsm8k_42 loss: 1.0123623609542847
[2025-12-23 14:54:30,402] m-LoRA: Adapter lora_gsm8k_43 loss: 2.4911813735961914
[2025-12-23 14:54:30,458] m-LoRA: Adapter lora_gsm8k_44 loss: 2.2663638591766357
[2025-12-23 14:54:30,665] m-LoRA: Adapter lora_gsm8k_37 loss: 1.6990594863891602
[2025-12-23 14:54:31,377] m-LoRA: Adapter lora_gsm8k_41 loss: 0.9863489270210266
[2025-12-23 14:54:33,659] m-LoRA: Adapter lora_gsm8k_42 epoch: 1/1 iteration: 80/128 step: 6
[2025-12-23 14:54:33,957] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 8/128 step: 5
[2025-12-23 14:54:34,048] m-LoRA: Adapter lora_gsm8k_44 epoch: 1/1 iteration: 16/128 step: 5
[2025-12-23 14:54:34,139] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 56/128 step: 29
[2025-12-23 14:54:34,234] m-LoRA: Adapter lora_gsm8k_41 epoch: 1/1 iteration: 48/128 step: 7
[2025-12-23 14:54:36,762] m-LoRA: Adapter lora_gsm8k_42 loss: 0.7986080646514893
[2025-12-23 14:54:36,800] m-LoRA: Adapter lora_gsm8k_43 loss: 2.188917636871338
[2025-12-23 14:54:36,859] m-LoRA: Adapter lora_gsm8k_44 loss: 1.9936891794204712
[2025-12-23 14:54:37,085] m-LoRA: Adapter lora_gsm8k_37 loss: 1.1800976991653442
[2025-12-23 14:54:37,729] m-LoRA: Adapter lora_gsm8k_41 loss: 0.9555308222770691
[2025-12-23 14:54:40,405] m-LoRA: Adapter lora_gsm8k_42 epoch: 1/1 iteration: 96/128 step: 7
[2025-12-23 14:54:40,688] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 10/128 step: 6
[2025-12-23 14:54:40,781] m-LoRA: Adapter lora_gsm8k_44 epoch: 1/1 iteration: 20/128 step: 6
[2025-12-23 14:54:40,860] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 58/128 step: 30
[2025-12-23 14:54:40,950] m-LoRA: Adapter lora_gsm8k_41 epoch: 1/1 iteration: 56/128 step: 8
[2025-12-23 14:54:42,760] m-LoRA: Adapter lora_gsm8k_42 loss: 1.0604991912841797
[2025-12-23 14:54:43,003] m-LoRA: Adapter lora_gsm8k_43 loss: 1.8429380655288696
[2025-12-23 14:54:43,657] m-LoRA: Adapter lora_gsm8k_44 loss: 1.6919161081314087
[2025-12-23 14:54:43,878] m-LoRA: Adapter lora_gsm8k_37 loss: 1.5630582571029663
[2025-12-23 14:54:44,625] m-LoRA: Adapter lora_gsm8k_41 loss: 0.9052651524543762
[2025-12-23 14:54:45,535] m-LoRA: Adapter lora_gsm8k_42 epoch: 1/1 iteration: 112/128 step: 8
[2025-12-23 14:54:45,778] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 12/128 step: 7
[2025-12-23 14:54:45,876] m-LoRA: Adapter lora_gsm8k_44 epoch: 1/1 iteration: 24/128 step: 7
[2025-12-23 14:54:45,956] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 60/128 step: 31
[2025-12-23 14:54:46,308] m-LoRA: Adapter lora_gsm8k_41 epoch: 1/1 iteration: 64/128 step: 9
[2025-12-23 14:54:47,685] m-LoRA: Adapter lora_gsm8k_42 loss: 0.9790129065513611
[2025-12-23 14:54:47,705] m-LoRA: Adapter lora_gsm8k_43 loss: 1.698646903038025
[2025-12-23 14:54:48,084] m-LoRA: Adapter lora_gsm8k_44 loss: 1.6673462390899658
[2025-12-23 14:54:48,147] m-LoRA: Adapter lora_gsm8k_37 loss: 1.7590912580490112
[2025-12-23 14:54:48,779] m-LoRA: Adapter lora_gsm8k_41 loss: 0.7446925044059753
[2025-12-23 14:54:50,312] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_42']
[2025-12-23 14:54:50,990] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_45']
[2025-12-23 14:54:51,065] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:54:51,147] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 14/128 step: 8
[2025-12-23 14:54:51,215] m-LoRA: Adapter lora_gsm8k_44 epoch: 1/1 iteration: 28/128 step: 8
[2025-12-23 14:54:51,296] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 62/128 step: 32
[2025-12-23 14:54:51,951] m-LoRA: Adapter lora_gsm8k_45 loss: 2.1059041023254395
[2025-12-23 14:54:51,954] m-LoRA: Adapter lora_gsm8k_41 epoch: 1/1 iteration: 72/128 step: 10
[2025-12-23 14:54:52,041] m-LoRA: Adapter lora_gsm8k_43 loss: 1.7978897094726562
[2025-12-23 14:54:52,407] m-LoRA: Adapter lora_gsm8k_44 loss: 1.5634596347808838
[2025-12-23 14:54:52,570] m-LoRA: Adapter lora_gsm8k_37 loss: 1.7159322500228882
[2025-12-23 14:54:52,675] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 2/128 step: 2
[2025-12-23 14:54:53,305] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 16/128 step: 9
[2025-12-23 14:54:53,428] m-LoRA: Adapter lora_gsm8k_41 loss: 0.6346156597137451
[2025-12-23 14:54:53,435] m-LoRA: Adapter lora_gsm8k_44 epoch: 1/1 iteration: 32/128 step: 9
[2025-12-23 14:54:53,605] m-LoRA: Adapter lora_gsm8k_45 loss: 1.5234507322311401
[2025-12-23 14:54:53,608] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 64/128 step: 33
[2025-12-23 14:54:53,772] m-LoRA: Adapter lora_gsm8k_43 loss: 1.5906904935836792
[2025-12-23 14:54:54,046] m-LoRA: Adapter lora_gsm8k_44 loss: 1.767390489578247
[2025-12-23 14:54:54,658] m-LoRA: Adapter lora_gsm8k_37 loss: 1.4502151012420654
[2025-12-23 14:54:54,859] m-LoRA: Adapter lora_gsm8k_41 epoch: 1/1 iteration: 80/128 step: 11
[2025-12-23 14:54:55,031] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 4/128 step: 3
[2025-12-23 14:54:55,121] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 18/128 step: 10
[2025-12-23 14:54:55,197] m-LoRA: Adapter lora_gsm8k_44 epoch: 1/1 iteration: 36/128 step: 10
[2025-12-23 14:54:55,376] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 66/128 step: 34
[2025-12-23 14:54:56,416] m-LoRA: Adapter lora_gsm8k_41 loss: 0.4446081817150116
[2025-12-23 14:54:56,433] m-LoRA: Adapter lora_gsm8k_45 loss: 1.5425845384597778
[2025-12-23 14:54:56,605] m-LoRA: Adapter lora_gsm8k_43 loss: 2.2634003162384033
[2025-12-23 14:54:56,878] m-LoRA: Adapter lora_gsm8k_44 loss: 1.9301626682281494
[2025-12-23 14:54:56,955] m-LoRA: Adapter lora_gsm8k_37 loss: 1.6160364151000977
[2025-12-23 14:54:58,278] m-LoRA: Adapter lora_gsm8k_41 epoch: 1/1 iteration: 88/128 step: 12
[2025-12-23 14:54:58,427] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 6/128 step: 4
[2025-12-23 14:54:58,513] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 20/128 step: 11
[2025-12-23 14:54:58,582] m-LoRA: Adapter lora_gsm8k_44 epoch: 1/1 iteration: 40/128 step: 11
[2025-12-23 14:54:58,665] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 68/128 step: 35
[2025-12-23 14:54:59,576] m-LoRA: Adapter lora_gsm8k_41 loss: 0.6358699202537537
[2025-12-23 14:54:59,594] m-LoRA: Adapter lora_gsm8k_45 loss: 1.7880395650863647
[2025-12-23 14:54:59,805] m-LoRA: Adapter lora_gsm8k_43 loss: 1.8478254079818726
[2025-12-23 14:55:00,141] m-LoRA: Adapter lora_gsm8k_44 loss: 1.4220526218414307
[2025-12-23 14:55:00,298] m-LoRA: Adapter lora_gsm8k_37 loss: 1.3136727809906006
[2025-12-23 14:55:01,144] m-LoRA: Adapter lora_gsm8k_41 epoch: 1/1 iteration: 96/128 step: 13
[2025-12-23 14:55:01,312] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 8/128 step: 5
[2025-12-23 14:55:01,409] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 22/128 step: 12
[2025-12-23 14:55:01,496] m-LoRA: Adapter lora_gsm8k_44 epoch: 1/1 iteration: 44/128 step: 12
[2025-12-23 14:55:01,567] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 70/128 step: 36
[2025-12-23 14:55:02,398] m-LoRA: Adapter lora_gsm8k_41 loss: 0.5294920802116394
[2025-12-23 14:55:02,413] m-LoRA: Adapter lora_gsm8k_45 loss: 1.6852854490280151
[2025-12-23 14:55:02,618] m-LoRA: Adapter lora_gsm8k_43 loss: 1.7591966390609741
[2025-12-23 14:55:02,949] m-LoRA: Adapter lora_gsm8k_44 loss: 1.4567521810531616
[2025-12-23 14:55:03,059] m-LoRA: Adapter lora_gsm8k_37 loss: 1.3104660511016846
[2025-12-23 14:55:03,848] m-LoRA: Adapter lora_gsm8k_41 epoch: 1/1 iteration: 104/128 step: 14
[2025-12-23 14:55:03,993] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 10/128 step: 6
[2025-12-23 14:55:04,048] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 24/128 step: 13
[2025-12-23 14:55:04,122] m-LoRA: Adapter lora_gsm8k_44 epoch: 1/1 iteration: 48/128 step: 13
[2025-12-23 14:55:04,226] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 72/128 step: 37
[2025-12-23 14:55:05,414] m-LoRA: Adapter lora_gsm8k_41 loss: 0.425994873046875
[2025-12-23 14:55:05,430] m-LoRA: Adapter lora_gsm8k_45 loss: 1.2637394666671753
[2025-12-23 14:55:05,648] m-LoRA: Adapter lora_gsm8k_43 loss: 1.8054054975509644
[2025-12-23 14:55:06,012] m-LoRA: Adapter lora_gsm8k_44 loss: 1.4574052095413208
[2025-12-23 14:55:06,266] m-LoRA: Adapter lora_gsm8k_37 loss: 1.4687684774398804
[2025-12-23 14:55:07,185] m-LoRA: Adapter lora_gsm8k_41 epoch: 1/1 iteration: 112/128 step: 15
[2025-12-23 14:55:07,328] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 12/128 step: 7
[2025-12-23 14:55:07,402] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 26/128 step: 14
[2025-12-23 14:55:07,476] m-LoRA: Adapter lora_gsm8k_44 epoch: 1/1 iteration: 52/128 step: 14
[2025-12-23 14:55:07,560] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 74/128 step: 38
[2025-12-23 14:55:08,430] m-LoRA: Adapter lora_gsm8k_41 loss: 0.6060566306114197
[2025-12-23 14:55:08,574] m-LoRA: Adapter lora_gsm8k_45 loss: 1.1536070108413696
[2025-12-23 14:55:08,702] m-LoRA: Adapter lora_gsm8k_43 loss: 1.7563918828964233
[2025-12-23 14:55:08,983] m-LoRA: Adapter lora_gsm8k_44 loss: 1.6573797464370728
[2025-12-23 14:55:09,062] m-LoRA: Adapter lora_gsm8k_37 loss: 1.7704002857208252
[2025-12-23 14:55:09,892] m-LoRA: Adapter lora_gsm8k_41 epoch: 1/1 iteration: 120/128 step: 16
[2025-12-23 14:55:10,033] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 14/128 step: 8
[2025-12-23 14:55:10,089] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 28/128 step: 15
[2025-12-23 14:55:10,174] m-LoRA: Adapter lora_gsm8k_44 epoch: 1/1 iteration: 56/128 step: 15
[2025-12-23 14:55:10,298] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 76/128 step: 39
[2025-12-23 14:55:11,250] m-LoRA: Adapter lora_gsm8k_41 loss: 0.4805310368537903
[2025-12-23 14:55:11,270] m-LoRA: Adapter lora_gsm8k_45 loss: 0.9795103669166565
[2025-12-23 14:55:11,473] m-LoRA: Adapter lora_gsm8k_43 loss: 1.6479740142822266
[2025-12-23 14:55:11,787] m-LoRA: Adapter lora_gsm8k_44 loss: 1.4619625806808472
[2025-12-23 14:55:11,881] m-LoRA: Adapter lora_gsm8k_37 loss: 1.3087611198425293
[2025-12-23 14:55:12,903] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_41']
[2025-12-23 14:55:13,480] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_46']
[2025-12-23 14:55:13,632] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:55:13,726] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 16/128 step: 9
[2025-12-23 14:55:13,793] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 30/128 step: 16
[2025-12-23 14:55:13,864] m-LoRA: Adapter lora_gsm8k_44 epoch: 1/1 iteration: 60/128 step: 16
[2025-12-23 14:55:13,965] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 78/128 step: 40
[2025-12-23 14:55:14,306] m-LoRA: Adapter lora_gsm8k_46 loss: 2.055901527404785
[2025-12-23 14:55:14,429] m-LoRA: Adapter lora_gsm8k_45 loss: 1.2989087104797363
[2025-12-23 14:55:14,516] m-LoRA: Adapter lora_gsm8k_43 loss: 1.88523268699646
[2025-12-23 14:55:14,847] m-LoRA: Adapter lora_gsm8k_44 loss: 1.4942904710769653
[2025-12-23 14:55:15,106] m-LoRA: Adapter lora_gsm8k_37 loss: 1.1380428075790405
[2025-12-23 14:55:15,109] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 2/128 step: 2
[2025-12-23 14:55:15,244] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 18/128 step: 10
[2025-12-23 14:55:15,333] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 32/128 step: 17
[2025-12-23 14:55:15,574] m-LoRA: Adapter lora_gsm8k_44 epoch: 1/1 iteration: 64/128 step: 17
[2025-12-23 14:55:15,683] m-LoRA: Adapter lora_gsm8k_46 loss: 1.8175833225250244
[2025-12-23 14:55:16,004] m-LoRA: Adapter lora_gsm8k_45 loss: 0.9561834931373596
[2025-12-23 14:55:16,007] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 80/128 step: 41
[2025-12-23 14:55:16,137] m-LoRA: Adapter lora_gsm8k_43 loss: 1.422011375427246
[2025-12-23 14:55:16,412] m-LoRA: Adapter lora_gsm8k_44 loss: 1.8306849002838135
[2025-12-23 14:55:16,416] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 4/128 step: 3
[2025-12-23 14:55:16,666] m-LoRA: Adapter lora_gsm8k_37 loss: 1.287803053855896
[2025-12-23 14:55:16,669] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 20/128 step: 11
[2025-12-23 14:55:16,845] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 34/128 step: 18
[2025-12-23 14:55:17,094] m-LoRA: Adapter lora_gsm8k_46 loss: 1.337776780128479
[2025-12-23 14:55:17,097] m-LoRA: Adapter lora_gsm8k_44 epoch: 1/1 iteration: 68/128 step: 18
[2025-12-23 14:55:17,265] m-LoRA: Adapter lora_gsm8k_45 loss: 1.0260918140411377
[2025-12-23 14:55:17,267] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 82/128 step: 42
[2025-12-23 14:55:17,405] m-LoRA: Adapter lora_gsm8k_43 loss: 1.6252772808074951
[2025-12-23 14:55:17,917] m-LoRA: Adapter lora_gsm8k_44 loss: 1.402287483215332
[2025-12-23 14:55:17,921] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 6/128 step: 4
[2025-12-23 14:55:18,074] m-LoRA: Adapter lora_gsm8k_37 loss: 1.472525715827942
[2025-12-23 14:55:18,078] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 36/128 step: 19
[2025-12-23 14:55:18,193] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 22/128 step: 12
[2025-12-23 14:55:18,605] m-LoRA: Adapter lora_gsm8k_46 loss: 1.765649437904358
[2025-12-23 14:55:18,940] m-LoRA: Adapter lora_gsm8k_43 loss: 1.5623044967651367
[2025-12-23 14:55:18,943] m-LoRA: Adapter lora_gsm8k_44 epoch: 1/1 iteration: 72/128 step: 19
[2025-12-23 14:55:19,103] m-LoRA: Adapter lora_gsm8k_45 loss: 0.6253806352615356
[2025-12-23 14:55:19,107] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 84/128 step: 43
[2025-12-23 14:55:19,240] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 8/128 step: 5
[2025-12-23 14:55:19,776] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 38/128 step: 20
[2025-12-23 14:55:19,887] m-LoRA: Adapter lora_gsm8k_44 loss: 1.4031727313995361
[2025-12-23 14:55:19,892] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 24/128 step: 13
[2025-12-23 14:55:19,934] m-LoRA: Adapter lora_gsm8k_37 loss: 1.3592387437820435
[2025-12-23 14:55:20,012] m-LoRA: Adapter lora_gsm8k_46 loss: 1.5913811922073364
[2025-12-23 14:55:20,226] m-LoRA: Adapter lora_gsm8k_43 loss: 1.5035196542739868
[2025-12-23 14:55:20,305] m-LoRA: Adapter lora_gsm8k_45 loss: 0.8211607336997986
[2025-12-23 14:55:20,809] m-LoRA: Adapter lora_gsm8k_44 epoch: 1/1 iteration: 76/128 step: 20
[2025-12-23 14:55:20,921] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 86/128 step: 44
[2025-12-23 14:55:21,015] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 10/128 step: 6
[2025-12-23 14:55:21,105] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 40/128 step: 21
[2025-12-23 14:55:21,179] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 26/128 step: 14
[2025-12-23 14:55:21,731] m-LoRA: Adapter lora_gsm8k_44 loss: 1.0055419206619263
[2025-12-23 14:55:21,848] m-LoRA: Adapter lora_gsm8k_37 loss: 1.1887595653533936
[2025-12-23 14:55:22,030] m-LoRA: Adapter lora_gsm8k_46 loss: 1.0156335830688477
[2025-12-23 14:55:22,140] m-LoRA: Adapter lora_gsm8k_43 loss: 1.79328453540802
[2025-12-23 14:55:22,292] m-LoRA: Adapter lora_gsm8k_45 loss: 0.632564127445221
[2025-12-23 14:55:22,771] m-LoRA: Adapter lora_gsm8k_44 epoch: 1/1 iteration: 80/128 step: 21
[2025-12-23 14:55:22,880] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 88/128 step: 45
[2025-12-23 14:55:22,974] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 12/128 step: 7
[2025-12-23 14:55:23,072] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 42/128 step: 22
[2025-12-23 14:55:23,151] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 28/128 step: 15
[2025-12-23 14:55:23,482] m-LoRA: Adapter lora_gsm8k_44 loss: 1.5987402200698853
[2025-12-23 14:55:23,650] m-LoRA: Adapter lora_gsm8k_37 loss: 1.0964361429214478
[2025-12-23 14:55:23,721] m-LoRA: Adapter lora_gsm8k_46 loss: 1.1320298910140991
[2025-12-23 14:55:23,830] m-LoRA: Adapter lora_gsm8k_43 loss: 1.8637313842773438
[2025-12-23 14:55:23,984] m-LoRA: Adapter lora_gsm8k_45 loss: 0.5983911156654358
[2025-12-23 14:55:24,280] m-LoRA: Adapter lora_gsm8k_44 epoch: 1/1 iteration: 84/128 step: 22
[2025-12-23 14:55:24,414] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 90/128 step: 46
[2025-12-23 14:55:24,528] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 14/128 step: 8
[2025-12-23 14:55:24,650] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 44/128 step: 23
[2025-12-23 14:55:25,040] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 30/128 step: 16
[2025-12-23 14:55:25,078] m-LoRA: Adapter lora_gsm8k_44 loss: 1.39214026927948
[2025-12-23 14:55:25,259] m-LoRA: Adapter lora_gsm8k_37 loss: 1.3302314281463623
[2025-12-23 14:55:25,382] m-LoRA: Adapter lora_gsm8k_46 loss: 0.8854396939277649
[2025-12-23 14:55:25,624] m-LoRA: Adapter lora_gsm8k_43 loss: 1.457932472229004
[2025-12-23 14:55:25,742] m-LoRA: Adapter lora_gsm8k_45 loss: 0.679235577583313
[2025-12-23 14:55:25,916] m-LoRA: Adapter lora_gsm8k_44 epoch: 1/1 iteration: 88/128 step: 23
[2025-12-23 14:55:26,152] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 92/128 step: 47
[2025-12-23 14:55:26,229] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 16/128 step: 9
[2025-12-23 14:55:26,361] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 46/128 step: 24
[2025-12-23 14:55:26,874] m-LoRA: Adapter lora_gsm8k_44 loss: 1.1829564571380615
[2025-12-23 14:55:26,879] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 32/128 step: 17
[2025-12-23 14:55:26,990] m-LoRA: Adapter lora_gsm8k_37 loss: 1.3402334451675415
[2025-12-23 14:55:27,148] m-LoRA: Adapter lora_gsm8k_46 loss: 0.8610568046569824
[2025-12-23 14:55:27,249] m-LoRA: Adapter lora_gsm8k_43 loss: 1.6421008110046387
[2025-12-23 14:55:27,396] m-LoRA: Adapter lora_gsm8k_45 loss: 0.7525426745414734
[2025-12-23 14:55:27,855] m-LoRA: Adapter lora_gsm8k_44 epoch: 1/1 iteration: 92/128 step: 24
[2025-12-23 14:55:27,951] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 94/128 step: 48
[2025-12-23 14:55:28,029] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 18/128 step: 10
[2025-12-23 14:55:28,106] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 48/128 step: 25
[2025-12-23 14:55:28,180] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 34/128 step: 18
[2025-12-23 14:55:28,460] m-LoRA: Adapter lora_gsm8k_44 loss: 1.875154972076416
[2025-12-23 14:55:28,591] m-LoRA: Adapter lora_gsm8k_37 loss: 1.407314419746399
[2025-12-23 14:55:28,730] m-LoRA: Adapter lora_gsm8k_46 loss: 0.7257298231124878
[2025-12-23 14:55:28,875] m-LoRA: Adapter lora_gsm8k_43 loss: 1.3597077131271362
[2025-12-23 14:55:28,992] m-LoRA: Adapter lora_gsm8k_45 loss: 0.5759115219116211
[2025-12-23 14:55:29,097] m-LoRA: Adapter lora_gsm8k_44 epoch: 1/1 iteration: 96/128 step: 25
[2025-12-23 14:55:29,195] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 96/128 step: 49
[2025-12-23 14:55:29,302] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 20/128 step: 11
[2025-12-23 14:55:29,499] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 50/128 step: 26
[2025-12-23 14:55:29,779] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 36/128 step: 19
[2025-12-23 14:55:29,819] m-LoRA: Adapter lora_gsm8k_44 loss: 1.4766414165496826
[2025-12-23 14:55:29,849] m-LoRA: Adapter lora_gsm8k_37 loss: 1.2626932859420776
[2025-12-23 14:55:30,036] m-LoRA: Adapter lora_gsm8k_46 loss: 0.7799859642982483
[2025-12-23 14:55:30,158] m-LoRA: Adapter lora_gsm8k_43 loss: 1.696408748626709
[2025-12-23 14:55:30,332] m-LoRA: Adapter lora_gsm8k_45 loss: 0.5480955243110657
[2025-12-23 14:55:30,532] m-LoRA: Adapter lora_gsm8k_44 epoch: 1/1 iteration: 100/128 step: 26
[2025-12-23 14:55:30,628] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 98/128 step: 50
[2025-12-23 14:55:30,796] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 22/128 step: 12
[2025-12-23 14:55:30,894] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 52/128 step: 27
[2025-12-23 14:55:31,369] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 38/128 step: 20
[2025-12-23 14:55:31,417] m-LoRA: Adapter lora_gsm8k_44 loss: 1.1948260068893433
[2025-12-23 14:55:31,423] m-LoRA: Adapter lora_gsm8k_37 loss: 1.2761603593826294
[2025-12-23 14:55:31,651] m-LoRA: Adapter lora_gsm8k_46 loss: 0.5640573501586914
[2025-12-23 14:55:31,781] m-LoRA: Adapter lora_gsm8k_43 loss: 1.6180698871612549
[2025-12-23 14:55:31,871] m-LoRA: Adapter lora_gsm8k_45 loss: 0.617420494556427
[2025-12-23 14:55:32,329] m-LoRA: Adapter lora_gsm8k_44 epoch: 1/1 iteration: 104/128 step: 27
[2025-12-23 14:55:32,431] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 100/128 step: 51
[2025-12-23 14:55:32,521] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 24/128 step: 13
[2025-12-23 14:55:32,600] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 54/128 step: 28
[2025-12-23 14:55:32,679] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 40/128 step: 21
[2025-12-23 14:55:33,028] m-LoRA: Adapter lora_gsm8k_44 loss: 1.383998155593872
[2025-12-23 14:55:33,116] m-LoRA: Adapter lora_gsm8k_37 loss: 1.4596843719482422
[2025-12-23 14:55:33,266] m-LoRA: Adapter lora_gsm8k_46 loss: 0.6244442462921143
[2025-12-23 14:55:33,416] m-LoRA: Adapter lora_gsm8k_43 loss: 1.3839887380599976
[2025-12-23 14:55:33,563] m-LoRA: Adapter lora_gsm8k_45 loss: 0.591077446937561
[2025-12-23 14:55:33,823] m-LoRA: Adapter lora_gsm8k_44 epoch: 1/1 iteration: 108/128 step: 28
[2025-12-23 14:55:33,932] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 102/128 step: 52
[2025-12-23 14:55:34,027] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 26/128 step: 14
[2025-12-23 14:55:34,118] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 56/128 step: 29
[2025-12-23 14:55:34,213] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 42/128 step: 22
[2025-12-23 14:55:34,652] m-LoRA: Adapter lora_gsm8k_44 loss: 1.2337905168533325
[2025-12-23 14:55:34,665] m-LoRA: Adapter lora_gsm8k_37 loss: 1.4958502054214478
[2025-12-23 14:55:34,888] m-LoRA: Adapter lora_gsm8k_46 loss: 0.628555417060852
[2025-12-23 14:55:35,015] m-LoRA: Adapter lora_gsm8k_43 loss: 1.3552474975585938
[2025-12-23 14:55:35,109] m-LoRA: Adapter lora_gsm8k_45 loss: 0.885597288608551
[2025-12-23 14:55:35,516] m-LoRA: Adapter lora_gsm8k_44 epoch: 1/1 iteration: 112/128 step: 29
[2025-12-23 14:55:35,624] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 104/128 step: 53
[2025-12-23 14:55:35,704] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 28/128 step: 15
[2025-12-23 14:55:35,802] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 58/128 step: 30
[2025-12-23 14:55:36,246] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 44/128 step: 23
[2025-12-23 14:55:36,295] m-LoRA: Adapter lora_gsm8k_44 loss: 1.2629579305648804
[2025-12-23 14:55:36,301] m-LoRA: Adapter lora_gsm8k_37 loss: 1.4565224647521973
[2025-12-23 14:55:36,489] m-LoRA: Adapter lora_gsm8k_46 loss: 0.5825826525688171
[2025-12-23 14:55:36,650] m-LoRA: Adapter lora_gsm8k_43 loss: 1.3850489854812622
[2025-12-23 14:55:36,750] m-LoRA: Adapter lora_gsm8k_45 loss: 0.6787157654762268
[2025-12-23 14:55:37,130] m-LoRA: Adapter lora_gsm8k_44 epoch: 1/1 iteration: 116/128 step: 30
[2025-12-23 14:55:37,238] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 106/128 step: 54
[2025-12-23 14:55:37,338] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 30/128 step: 16
[2025-12-23 14:55:37,417] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 60/128 step: 31
[2025-12-23 14:55:37,495] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 46/128 step: 24
[2025-12-23 14:55:37,843] m-LoRA: Adapter lora_gsm8k_44 loss: 1.5441503524780273
[2025-12-23 14:55:38,000] m-LoRA: Adapter lora_gsm8k_37 loss: 1.0575742721557617
[2025-12-23 14:55:38,138] m-LoRA: Adapter lora_gsm8k_46 loss: 0.5285208225250244
[2025-12-23 14:55:38,259] m-LoRA: Adapter lora_gsm8k_43 loss: 1.4838719367980957
[2025-12-23 14:55:38,469] m-LoRA: Adapter lora_gsm8k_45 loss: 0.8827475309371948
[2025-12-23 14:55:38,623] m-LoRA: Adapter lora_gsm8k_44 epoch: 1/1 iteration: 120/128 step: 31
[2025-12-23 14:55:38,731] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 108/128 step: 55
[2025-12-23 14:55:38,834] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 32/128 step: 17
[2025-12-23 14:55:38,951] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 62/128 step: 32
[2025-12-23 14:55:39,413] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 48/128 step: 25
[2025-12-23 14:55:39,456] m-LoRA: Adapter lora_gsm8k_44 loss: 1.3442586660385132
[2025-12-23 14:55:39,571] m-LoRA: Adapter lora_gsm8k_37 loss: 1.1521995067596436
[2025-12-23 14:55:39,758] m-LoRA: Adapter lora_gsm8k_46 loss: 0.6976849436759949
[2025-12-23 14:55:39,873] m-LoRA: Adapter lora_gsm8k_43 loss: 1.5534489154815674
[2025-12-23 14:55:40,022] m-LoRA: Adapter lora_gsm8k_45 loss: 0.477265864610672
[2025-12-23 14:55:40,291] m-LoRA: Adapter lora_gsm8k_44 epoch: 1/1 iteration: 124/128 step: 32
[2025-12-23 14:55:40,439] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 110/128 step: 56
[2025-12-23 14:55:40,514] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 34/128 step: 18
[2025-12-23 14:55:40,629] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 64/128 step: 33
[2025-12-23 14:55:41,088] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 50/128 step: 26
[2025-12-23 14:55:41,126] m-LoRA: Adapter lora_gsm8k_44 loss: 1.3595629930496216
[2025-12-23 14:55:41,173] m-LoRA: Adapter lora_gsm8k_37 loss: 1.1236604452133179
[2025-12-23 14:55:41,352] m-LoRA: Adapter lora_gsm8k_46 loss: 0.5991519689559937
[2025-12-23 14:55:41,487] m-LoRA: Adapter lora_gsm8k_43 loss: 1.5133962631225586
[2025-12-23 14:55:41,622] m-LoRA: Adapter lora_gsm8k_45 loss: 0.642914891242981
[2025-12-23 14:55:42,036] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_44']
[2025-12-23 14:55:42,339] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_47']
[2025-12-23 14:55:42,496] m-LoRA: Adapter lora_gsm8k_47 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:55:42,618] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 112/128 step: 57
[2025-12-23 14:55:42,688] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 36/128 step: 19
[2025-12-23 14:55:42,763] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 66/128 step: 34
[2025-12-23 14:55:42,876] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 52/128 step: 27
[2025-12-23 14:55:43,753] m-LoRA: Adapter lora_gsm8k_47 loss: 2.415168523788452
[2025-12-23 14:55:43,769] m-LoRA: Adapter lora_gsm8k_37 loss: 1.3023210763931274
[2025-12-23 14:55:43,986] m-LoRA: Adapter lora_gsm8k_46 loss: 0.6444734334945679
[2025-12-23 14:55:44,076] m-LoRA: Adapter lora_gsm8k_43 loss: 1.8167420625686646
[2025-12-23 14:55:44,213] m-LoRA: Adapter lora_gsm8k_45 loss: 0.698207437992096
[2025-12-23 14:55:45,116] m-LoRA: Adapter lora_gsm8k_47 epoch: 1/1 iteration: 8/128 step: 2
[2025-12-23 14:55:45,238] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 114/128 step: 58
[2025-12-23 14:55:45,333] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 38/128 step: 20
[2025-12-23 14:55:45,413] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 68/128 step: 35
[2025-12-23 14:55:45,485] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 54/128 step: 28
[2025-12-23 14:55:46,703] m-LoRA: Adapter lora_gsm8k_47 loss: 1.0483289957046509
[2025-12-23 14:55:46,714] m-LoRA: Adapter lora_gsm8k_37 loss: 1.3645744323730469
[2025-12-23 14:55:46,905] m-LoRA: Adapter lora_gsm8k_46 loss: 0.6288056969642639
[2025-12-23 14:55:47,007] m-LoRA: Adapter lora_gsm8k_43 loss: 1.3340853452682495
[2025-12-23 14:55:47,136] m-LoRA: Adapter lora_gsm8k_45 loss: 0.5587639808654785
[2025-12-23 14:55:48,532] m-LoRA: Adapter lora_gsm8k_47 epoch: 1/1 iteration: 16/128 step: 3
[2025-12-23 14:55:48,700] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 116/128 step: 59
[2025-12-23 14:55:48,778] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 40/128 step: 21
[2025-12-23 14:55:48,856] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 70/128 step: 36
[2025-12-23 14:55:48,936] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 56/128 step: 29
[2025-12-23 14:55:49,725] m-LoRA: Adapter lora_gsm8k_47 loss: 1.2989178895950317
[2025-12-23 14:55:49,741] m-LoRA: Adapter lora_gsm8k_37 loss: 1.096702218055725
[2025-12-23 14:55:49,960] m-LoRA: Adapter lora_gsm8k_46 loss: 0.5963046550750732
[2025-12-23 14:55:50,078] m-LoRA: Adapter lora_gsm8k_43 loss: 1.6578601598739624
[2025-12-23 14:55:50,207] m-LoRA: Adapter lora_gsm8k_45 loss: 0.5271815061569214
[2025-12-23 14:55:51,238] m-LoRA: Adapter lora_gsm8k_47 epoch: 1/1 iteration: 24/128 step: 4
[2025-12-23 14:55:51,381] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 118/128 step: 60
[2025-12-23 14:55:51,460] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 42/128 step: 22
[2025-12-23 14:55:51,541] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 72/128 step: 37
[2025-12-23 14:55:51,613] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 58/128 step: 30
[2025-12-23 14:55:52,640] m-LoRA: Adapter lora_gsm8k_47 loss: 0.984015166759491
[2025-12-23 14:55:52,657] m-LoRA: Adapter lora_gsm8k_37 loss: 1.0683599710464478
[2025-12-23 14:55:52,870] m-LoRA: Adapter lora_gsm8k_46 loss: 0.543939471244812
[2025-12-23 14:55:53,005] m-LoRA: Adapter lora_gsm8k_43 loss: 1.3173307180404663
[2025-12-23 14:55:53,230] m-LoRA: Adapter lora_gsm8k_45 loss: 0.7036502957344055
[2025-12-23 14:55:54,306] m-LoRA: Adapter lora_gsm8k_47 epoch: 1/1 iteration: 32/128 step: 5
[2025-12-23 14:55:54,474] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 120/128 step: 61
[2025-12-23 14:55:54,569] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 44/128 step: 23
[2025-12-23 14:55:54,656] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 74/128 step: 38
[2025-12-23 14:55:54,729] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 60/128 step: 31
[2025-12-23 14:55:55,473] m-LoRA: Adapter lora_gsm8k_47 loss: 1.2230747938156128
[2025-12-23 14:55:55,489] m-LoRA: Adapter lora_gsm8k_37 loss: 1.0741097927093506
[2025-12-23 14:55:55,717] m-LoRA: Adapter lora_gsm8k_46 loss: 0.4821825325489044
[2025-12-23 14:55:55,897] m-LoRA: Adapter lora_gsm8k_43 loss: 1.0809444189071655
[2025-12-23 14:55:55,998] m-LoRA: Adapter lora_gsm8k_45 loss: 0.7911931872367859
[2025-12-23 14:55:56,852] m-LoRA: Adapter lora_gsm8k_47 epoch: 1/1 iteration: 40/128 step: 6
[2025-12-23 14:55:56,998] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 122/128 step: 62
[2025-12-23 14:55:57,094] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 46/128 step: 24
[2025-12-23 14:55:57,172] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 76/128 step: 39
[2025-12-23 14:55:57,258] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 62/128 step: 32
[2025-12-23 14:55:58,157] m-LoRA: Adapter lora_gsm8k_47 loss: 1.0548076629638672
[2025-12-23 14:55:58,177] m-LoRA: Adapter lora_gsm8k_37 loss: 0.9044342041015625
[2025-12-23 14:55:58,434] m-LoRA: Adapter lora_gsm8k_46 loss: 0.7940076589584351
[2025-12-23 14:55:58,801] m-LoRA: Adapter lora_gsm8k_43 loss: 0.9379005432128906
[2025-12-23 14:55:58,897] m-LoRA: Adapter lora_gsm8k_45 loss: 0.6953148245811462
[2025-12-23 14:55:59,817] m-LoRA: Adapter lora_gsm8k_47 epoch: 1/1 iteration: 48/128 step: 7
[2025-12-23 14:55:59,950] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 124/128 step: 63
[2025-12-23 14:56:00,042] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 48/128 step: 25
[2025-12-23 14:56:00,122] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 78/128 step: 40
[2025-12-23 14:56:00,193] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 64/128 step: 33
[2025-12-23 14:56:00,816] m-LoRA: Adapter lora_gsm8k_47 loss: 1.1590847969055176
[2025-12-23 14:56:00,833] m-LoRA: Adapter lora_gsm8k_37 loss: 1.2413396835327148
[2025-12-23 14:56:01,026] m-LoRA: Adapter lora_gsm8k_46 loss: 0.6373074054718018
[2025-12-23 14:56:01,160] m-LoRA: Adapter lora_gsm8k_43 loss: 1.1721538305282593
[2025-12-23 14:56:01,276] m-LoRA: Adapter lora_gsm8k_45 loss: 0.5146733522415161
[2025-12-23 14:56:02,041] m-LoRA: Adapter lora_gsm8k_47 epoch: 1/1 iteration: 56/128 step: 8
[2025-12-23 14:56:02,155] m-LoRA: Adapter lora_gsm8k_37 epoch: 1/1 iteration: 126/128 step: 64
[2025-12-23 14:56:02,234] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 50/128 step: 26
[2025-12-23 14:56:02,320] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 80/128 step: 41
[2025-12-23 14:56:02,396] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 66/128 step: 34
[2025-12-23 14:56:03,068] m-LoRA: Adapter lora_gsm8k_47 loss: 0.9872636795043945
[2025-12-23 14:56:03,082] m-LoRA: Adapter lora_gsm8k_37 loss: 1.1112116575241089
[2025-12-23 14:56:03,288] m-LoRA: Adapter lora_gsm8k_46 loss: 0.5348711609840393
[2025-12-23 14:56:03,418] m-LoRA: Adapter lora_gsm8k_43 loss: 1.267472267150879
[2025-12-23 14:56:03,556] m-LoRA: Adapter lora_gsm8k_45 loss: 0.5407525897026062
[2025-12-23 14:56:04,392] m-LoRA: Adapter lora_gsm8k_47 epoch: 1/1 iteration: 64/128 step: 9
[2025-12-23 14:56:04,497] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_37']
[2025-12-23 14:56:04,805] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_48']
[2025-12-23 14:56:04,844] m-LoRA: Adapter lora_gsm8k_48 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:56:04,944] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 52/128 step: 27
[2025-12-23 14:56:05,008] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 82/128 step: 42
[2025-12-23 14:56:05,063] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 68/128 step: 35
[2025-12-23 14:56:05,938] m-LoRA: Adapter lora_gsm8k_47 loss: 0.7032549381256104
[2025-12-23 14:56:06,201] m-LoRA: Adapter lora_gsm8k_48 loss: 2.391903877258301
[2025-12-23 14:56:06,273] m-LoRA: Adapter lora_gsm8k_46 loss: 0.46293264627456665
[2025-12-23 14:56:06,394] m-LoRA: Adapter lora_gsm8k_43 loss: 1.0133261680603027
[2025-12-23 14:56:06,564] m-LoRA: Adapter lora_gsm8k_45 loss: 0.4076068699359894
[2025-12-23 14:56:07,389] m-LoRA: Adapter lora_gsm8k_47 epoch: 1/1 iteration: 72/128 step: 10
[2025-12-23 14:56:07,540] m-LoRA: Adapter lora_gsm8k_48 epoch: 1/1 iteration: 4/128 step: 2
[2025-12-23 14:56:07,634] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 54/128 step: 28
[2025-12-23 14:56:07,708] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 84/128 step: 43
[2025-12-23 14:56:07,763] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 70/128 step: 36
[2025-12-23 14:56:08,393] m-LoRA: Adapter lora_gsm8k_47 loss: 0.7454928755760193
[2025-12-23 14:56:08,787] m-LoRA: Adapter lora_gsm8k_48 loss: 1.5260157585144043
[2025-12-23 14:56:08,891] m-LoRA: Adapter lora_gsm8k_46 loss: 0.6308717131614685
[2025-12-23 14:56:08,993] m-LoRA: Adapter lora_gsm8k_43 loss: 1.5717389583587646
[2025-12-23 14:56:09,184] m-LoRA: Adapter lora_gsm8k_45 loss: 0.7496359944343567
[2025-12-23 14:56:09,626] m-LoRA: Adapter lora_gsm8k_47 epoch: 1/1 iteration: 80/128 step: 11
[2025-12-23 14:56:09,917] m-LoRA: Adapter lora_gsm8k_48 epoch: 1/1 iteration: 8/128 step: 3
[2025-12-23 14:56:10,046] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 56/128 step: 29
[2025-12-23 14:56:10,120] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 86/128 step: 44
[2025-12-23 14:56:10,196] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 72/128 step: 37
[2025-12-23 14:56:10,961] m-LoRA: Adapter lora_gsm8k_47 loss: 0.5856009721755981
[2025-12-23 14:56:11,248] m-LoRA: Adapter lora_gsm8k_48 loss: 1.4983800649642944
[2025-12-23 14:56:11,435] m-LoRA: Adapter lora_gsm8k_46 loss: 0.513092041015625
[2025-12-23 14:56:11,514] m-LoRA: Adapter lora_gsm8k_43 loss: 1.4886914491653442
[2025-12-23 14:56:11,705] m-LoRA: Adapter lora_gsm8k_45 loss: 0.6910919547080994
[2025-12-23 14:56:12,502] m-LoRA: Adapter lora_gsm8k_47 epoch: 1/1 iteration: 88/128 step: 12
[2025-12-23 14:56:12,649] m-LoRA: Adapter lora_gsm8k_48 epoch: 1/1 iteration: 12/128 step: 4
[2025-12-23 14:56:12,757] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 58/128 step: 30
[2025-12-23 14:56:12,852] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 88/128 step: 45
[2025-12-23 14:56:12,935] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 74/128 step: 38
[2025-12-23 14:56:13,812] m-LoRA: Adapter lora_gsm8k_47 loss: 0.5772428512573242
[2025-12-23 14:56:14,090] m-LoRA: Adapter lora_gsm8k_48 loss: 1.3680437803268433
[2025-12-23 14:56:14,230] m-LoRA: Adapter lora_gsm8k_46 loss: 0.6888497471809387
[2025-12-23 14:56:14,522] m-LoRA: Adapter lora_gsm8k_43 loss: 1.238153100013733
[2025-12-23 14:56:14,655] m-LoRA: Adapter lora_gsm8k_45 loss: 0.574465811252594
[2025-12-23 14:56:15,610] m-LoRA: Adapter lora_gsm8k_47 epoch: 1/1 iteration: 96/128 step: 13
[2025-12-23 14:56:15,749] m-LoRA: Adapter lora_gsm8k_48 epoch: 1/1 iteration: 16/128 step: 5
[2025-12-23 14:56:15,820] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 60/128 step: 31
[2025-12-23 14:56:15,891] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 90/128 step: 46
[2025-12-23 14:56:15,970] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 76/128 step: 39
[2025-12-23 14:56:16,665] m-LoRA: Adapter lora_gsm8k_47 loss: 0.6097968816757202
[2025-12-23 14:56:16,891] m-LoRA: Adapter lora_gsm8k_48 loss: 1.6429346799850464
[2025-12-23 14:56:17,054] m-LoRA: Adapter lora_gsm8k_46 loss: 0.4607102870941162
[2025-12-23 14:56:17,234] m-LoRA: Adapter lora_gsm8k_43 loss: 0.9899681210517883
[2025-12-23 14:56:17,373] m-LoRA: Adapter lora_gsm8k_45 loss: 0.7302047610282898
[2025-12-23 14:56:17,949] m-LoRA: Adapter lora_gsm8k_47 epoch: 1/1 iteration: 104/128 step: 14
[2025-12-23 14:56:18,076] m-LoRA: Adapter lora_gsm8k_48 epoch: 1/1 iteration: 20/128 step: 6
[2025-12-23 14:56:18,181] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 62/128 step: 32
[2025-12-23 14:56:18,276] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 92/128 step: 47
[2025-12-23 14:56:18,360] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 78/128 step: 40
[2025-12-23 14:56:19,279] m-LoRA: Adapter lora_gsm8k_47 loss: 0.4492451846599579
[2025-12-23 14:56:19,518] m-LoRA: Adapter lora_gsm8k_48 loss: 1.5172277688980103
[2025-12-23 14:56:19,576] m-LoRA: Adapter lora_gsm8k_46 loss: 0.6151387095451355
[2025-12-23 14:56:19,733] m-LoRA: Adapter lora_gsm8k_43 loss: 1.24889075756073
[2025-12-23 14:56:19,866] m-LoRA: Adapter lora_gsm8k_45 loss: 0.506187915802002
[2025-12-23 14:56:20,841] m-LoRA: Adapter lora_gsm8k_47 epoch: 1/1 iteration: 112/128 step: 15
[2025-12-23 14:56:21,000] m-LoRA: Adapter lora_gsm8k_48 epoch: 1/1 iteration: 24/128 step: 7
[2025-12-23 14:56:21,109] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 64/128 step: 33
[2025-12-23 14:56:21,204] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 94/128 step: 48
[2025-12-23 14:56:21,283] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 80/128 step: 41
[2025-12-23 14:56:22,008] m-LoRA: Adapter lora_gsm8k_47 loss: 0.3809647262096405
[2025-12-23 14:56:22,248] m-LoRA: Adapter lora_gsm8k_48 loss: 1.4123420715332031
[2025-12-23 14:56:22,362] m-LoRA: Adapter lora_gsm8k_46 loss: 0.462753564119339
[2025-12-23 14:56:22,523] m-LoRA: Adapter lora_gsm8k_43 loss: 1.0780049562454224
[2025-12-23 14:56:22,653] m-LoRA: Adapter lora_gsm8k_45 loss: 0.7835788726806641
[2025-12-23 14:56:23,611] m-LoRA: Adapter lora_gsm8k_47 epoch: 1/1 iteration: 120/128 step: 16
[2025-12-23 14:56:23,756] m-LoRA: Adapter lora_gsm8k_48 epoch: 1/1 iteration: 28/128 step: 8
[2025-12-23 14:56:23,857] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 66/128 step: 34
[2025-12-23 14:56:23,946] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 96/128 step: 49
[2025-12-23 14:56:24,020] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 82/128 step: 42
[2025-12-23 14:56:24,700] m-LoRA: Adapter lora_gsm8k_47 loss: 0.6177685260772705
[2025-12-23 14:56:24,945] m-LoRA: Adapter lora_gsm8k_48 loss: 1.4521446228027344
[2025-12-23 14:56:25,061] m-LoRA: Adapter lora_gsm8k_46 loss: 0.7063308358192444
[2025-12-23 14:56:25,163] m-LoRA: Adapter lora_gsm8k_43 loss: 1.1288161277770996
[2025-12-23 14:56:25,336] m-LoRA: Adapter lora_gsm8k_45 loss: 0.5178780555725098
[2025-12-23 14:56:26,068] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_47']
[2025-12-23 14:56:26,679] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_49']
[2025-12-23 14:56:26,710] m-LoRA: Adapter lora_gsm8k_49 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:56:26,810] m-LoRA: Adapter lora_gsm8k_48 epoch: 1/1 iteration: 32/128 step: 9
[2025-12-23 14:56:26,869] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 68/128 step: 35
[2025-12-23 14:56:27,005] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 98/128 step: 50
[2025-12-23 14:56:27,088] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 84/128 step: 43
[2025-12-23 14:56:27,551] m-LoRA: Adapter lora_gsm8k_49 loss: 2.402024507522583
[2025-12-23 14:56:27,800] m-LoRA: Adapter lora_gsm8k_48 loss: 1.5308949947357178
[2025-12-23 14:56:27,892] m-LoRA: Adapter lora_gsm8k_46 loss: 0.6944618225097656
[2025-12-23 14:56:28,040] m-LoRA: Adapter lora_gsm8k_43 loss: 1.3228791952133179
[2025-12-23 14:56:28,506] m-LoRA: Adapter lora_gsm8k_45 loss: 0.5703022480010986
[2025-12-23 14:56:28,510] m-LoRA: Adapter lora_gsm8k_49 epoch: 1/1 iteration: 4/128 step: 2
[2025-12-23 14:56:28,679] m-LoRA: Adapter lora_gsm8k_48 epoch: 1/1 iteration: 36/128 step: 10
[2025-12-23 14:56:28,785] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 70/128 step: 36
[2025-12-23 14:56:28,878] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 100/128 step: 51
[2025-12-23 14:56:29,294] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 86/128 step: 44
[2025-12-23 14:56:29,332] m-LoRA: Adapter lora_gsm8k_49 loss: 1.8737658262252808
[2025-12-23 14:56:29,561] m-LoRA: Adapter lora_gsm8k_48 loss: 1.344038724899292
[2025-12-23 14:56:29,707] m-LoRA: Adapter lora_gsm8k_46 loss: 0.5324233770370483
[2025-12-23 14:56:29,859] m-LoRA: Adapter lora_gsm8k_43 loss: 0.8850268125534058
[2025-12-23 14:56:30,035] m-LoRA: Adapter lora_gsm8k_45 loss: 0.6238267421722412
[2025-12-23 14:56:30,209] m-LoRA: Adapter lora_gsm8k_49 epoch: 1/1 iteration: 8/128 step: 3
[2025-12-23 14:56:30,501] m-LoRA: Adapter lora_gsm8k_48 epoch: 1/1 iteration: 40/128 step: 11
[2025-12-23 14:56:30,588] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 72/128 step: 37
[2025-12-23 14:56:30,668] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 102/128 step: 52
[2025-12-23 14:56:31,018] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 88/128 step: 45
[2025-12-23 14:56:31,062] m-LoRA: Adapter lora_gsm8k_49 loss: 1.5779889822006226
[2025-12-23 14:56:31,367] m-LoRA: Adapter lora_gsm8k_48 loss: 1.2292035818099976
[2025-12-23 14:56:31,490] m-LoRA: Adapter lora_gsm8k_46 loss: 0.5220703482627869
[2025-12-23 14:56:31,697] m-LoRA: Adapter lora_gsm8k_43 loss: 1.089005470275879
[2025-12-23 14:56:31,898] m-LoRA: Adapter lora_gsm8k_45 loss: 0.6458511352539062
[2025-12-23 14:56:32,011] m-LoRA: Adapter lora_gsm8k_49 epoch: 1/1 iteration: 12/128 step: 4
[2025-12-23 14:56:32,167] m-LoRA: Adapter lora_gsm8k_48 epoch: 1/1 iteration: 44/128 step: 12
[2025-12-23 14:56:32,268] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 74/128 step: 38
[2025-12-23 14:56:32,475] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 104/128 step: 53
[2025-12-23 14:56:32,893] m-LoRA: Adapter lora_gsm8k_49 loss: 1.6430816650390625
[2025-12-23 14:56:32,897] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 90/128 step: 46
[2025-12-23 14:56:33,211] m-LoRA: Adapter lora_gsm8k_48 loss: 1.2336273193359375
[2025-12-23 14:56:33,319] m-LoRA: Adapter lora_gsm8k_46 loss: 0.5794909000396729
[2025-12-23 14:56:33,499] m-LoRA: Adapter lora_gsm8k_43 loss: 1.0931143760681152
[2025-12-23 14:56:33,720] m-LoRA: Adapter lora_gsm8k_45 loss: 0.662477433681488
[2025-12-23 14:56:33,722] m-LoRA: Adapter lora_gsm8k_49 epoch: 1/1 iteration: 16/128 step: 5
[2025-12-23 14:56:34,002] m-LoRA: Adapter lora_gsm8k_48 epoch: 1/1 iteration: 48/128 step: 13
[2025-12-23 14:56:34,111] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 76/128 step: 39
[2025-12-23 14:56:34,206] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 106/128 step: 54
[2025-12-23 14:56:34,621] m-LoRA: Adapter lora_gsm8k_49 loss: 1.34140944480896
[2025-12-23 14:56:34,626] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 92/128 step: 47
[2025-12-23 14:56:34,885] m-LoRA: Adapter lora_gsm8k_48 loss: 1.4968467950820923
[2025-12-23 14:56:35,058] m-LoRA: Adapter lora_gsm8k_46 loss: 0.5675091743469238
[2025-12-23 14:56:35,159] m-LoRA: Adapter lora_gsm8k_43 loss: 1.155579686164856
[2025-12-23 14:56:35,489] m-LoRA: Adapter lora_gsm8k_45 loss: 0.5927006602287292
[2025-12-23 14:56:35,493] m-LoRA: Adapter lora_gsm8k_49 epoch: 1/1 iteration: 20/128 step: 6
[2025-12-23 14:56:35,727] m-LoRA: Adapter lora_gsm8k_48 epoch: 1/1 iteration: 52/128 step: 14
[2025-12-23 14:56:35,916] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 78/128 step: 40
[2025-12-23 14:56:36,012] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 108/128 step: 55
[2025-12-23 14:56:36,553] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 94/128 step: 48
[2025-12-23 14:56:36,594] m-LoRA: Adapter lora_gsm8k_49 loss: 1.1821627616882324
[2025-12-23 14:56:36,879] m-LoRA: Adapter lora_gsm8k_48 loss: 1.2747892141342163
[2025-12-23 14:56:36,990] m-LoRA: Adapter lora_gsm8k_46 loss: 0.6931033134460449
[2025-12-23 14:56:37,182] m-LoRA: Adapter lora_gsm8k_43 loss: 0.9336713552474976
[2025-12-23 14:56:37,398] m-LoRA: Adapter lora_gsm8k_45 loss: 0.7228479981422424
[2025-12-23 14:56:37,555] m-LoRA: Adapter lora_gsm8k_49 epoch: 1/1 iteration: 24/128 step: 7
[2025-12-23 14:56:37,861] m-LoRA: Adapter lora_gsm8k_48 epoch: 1/1 iteration: 56/128 step: 15
[2025-12-23 14:56:37,972] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 80/128 step: 41
[2025-12-23 14:56:38,051] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 110/128 step: 56
[2025-12-23 14:56:38,282] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 96/128 step: 49
[2025-12-23 14:56:38,315] m-LoRA: Adapter lora_gsm8k_49 loss: 1.7318979501724243
[2025-12-23 14:56:38,597] m-LoRA: Adapter lora_gsm8k_48 loss: 1.1063286066055298
[2025-12-23 14:56:38,756] m-LoRA: Adapter lora_gsm8k_46 loss: 0.6182844042778015
[2025-12-23 14:56:38,908] m-LoRA: Adapter lora_gsm8k_43 loss: 0.8009893298149109
[2025-12-23 14:56:39,158] m-LoRA: Adapter lora_gsm8k_45 loss: 0.6221566200256348
[2025-12-23 14:56:39,161] m-LoRA: Adapter lora_gsm8k_49 epoch: 1/1 iteration: 28/128 step: 8
[2025-12-23 14:56:39,423] m-LoRA: Adapter lora_gsm8k_48 epoch: 1/1 iteration: 60/128 step: 16
[2025-12-23 14:56:39,527] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 82/128 step: 42
[2025-12-23 14:56:39,607] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 112/128 step: 57
[2025-12-23 14:56:40,008] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 98/128 step: 50
[2025-12-23 14:56:40,048] m-LoRA: Adapter lora_gsm8k_49 loss: 1.3305726051330566
[2025-12-23 14:56:40,227] m-LoRA: Adapter lora_gsm8k_48 loss: 1.5554922819137573
[2025-12-23 14:56:40,383] m-LoRA: Adapter lora_gsm8k_46 loss: 0.7320753335952759
[2025-12-23 14:56:40,466] m-LoRA: Adapter lora_gsm8k_43 loss: 0.8913576602935791
[2025-12-23 14:56:40,598] m-LoRA: Adapter lora_gsm8k_45 loss: 0.5462522506713867
[2025-12-23 14:56:40,862] m-LoRA: Adapter lora_gsm8k_49 epoch: 1/1 iteration: 32/128 step: 9
[2025-12-23 14:56:41,015] m-LoRA: Adapter lora_gsm8k_48 epoch: 1/1 iteration: 64/128 step: 17
[2025-12-23 14:56:41,117] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 84/128 step: 43
[2025-12-23 14:56:41,204] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 114/128 step: 58
[2025-12-23 14:56:41,656] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 100/128 step: 51
[2025-12-23 14:56:41,689] m-LoRA: Adapter lora_gsm8k_49 loss: 1.1790657043457031
[2025-12-23 14:56:42,046] m-LoRA: Adapter lora_gsm8k_48 loss: 0.7783105969429016
[2025-12-23 14:56:42,154] m-LoRA: Adapter lora_gsm8k_46 loss: 0.8804964423179626
[2025-12-23 14:56:42,281] m-LoRA: Adapter lora_gsm8k_43 loss: 0.856192946434021
[2025-12-23 14:56:42,390] m-LoRA: Adapter lora_gsm8k_45 loss: 0.565773606300354
[2025-12-23 14:56:42,565] m-LoRA: Adapter lora_gsm8k_49 epoch: 1/1 iteration: 36/128 step: 10
[2025-12-23 14:56:42,950] m-LoRA: Adapter lora_gsm8k_48 epoch: 1/1 iteration: 68/128 step: 18
[2025-12-23 14:56:43,051] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 86/128 step: 44
[2025-12-23 14:56:43,308] m-LoRA: Adapter lora_gsm8k_49 loss: 1.5286716222763062
[2025-12-23 14:56:43,311] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 116/128 step: 59
[2025-12-23 14:56:43,473] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 102/128 step: 52
[2025-12-23 14:56:43,694] m-LoRA: Adapter lora_gsm8k_48 loss: 0.8704391121864319
[2025-12-23 14:56:43,881] m-LoRA: Adapter lora_gsm8k_46 loss: 0.570860743522644
[2025-12-23 14:56:44,028] m-LoRA: Adapter lora_gsm8k_43 loss: 0.7752862572669983
[2025-12-23 14:56:44,273] m-LoRA: Adapter lora_gsm8k_45 loss: 0.5144791007041931
[2025-12-23 14:56:44,275] m-LoRA: Adapter lora_gsm8k_49 epoch: 1/1 iteration: 40/128 step: 11
[2025-12-23 14:56:44,583] m-LoRA: Adapter lora_gsm8k_48 epoch: 1/1 iteration: 72/128 step: 19
[2025-12-23 14:56:44,689] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 88/128 step: 45
[2025-12-23 14:56:45,034] m-LoRA: Adapter lora_gsm8k_49 loss: 1.298998475074768
[2025-12-23 14:56:45,038] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 118/128 step: 60
[2025-12-23 14:56:45,285] m-LoRA: Adapter lora_gsm8k_48 loss: 1.063671350479126
[2025-12-23 14:56:45,288] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 104/128 step: 53
[2025-12-23 14:56:45,368] m-LoRA: Adapter lora_gsm8k_46 loss: 0.4721452295780182
[2025-12-23 14:56:45,745] m-LoRA: Adapter lora_gsm8k_43 loss: 0.908817708492279
[2025-12-23 14:56:45,855] m-LoRA: Adapter lora_gsm8k_49 epoch: 1/1 iteration: 44/128 step: 12
[2025-12-23 14:56:46,013] m-LoRA: Adapter lora_gsm8k_45 loss: 0.5273097157478333
[2025-12-23 14:56:46,146] m-LoRA: Adapter lora_gsm8k_48 epoch: 1/1 iteration: 76/128 step: 20
[2025-12-23 14:56:46,249] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 90/128 step: 46
[2025-12-23 14:56:46,339] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 120/128 step: 61
[2025-12-23 14:56:46,562] m-LoRA: Adapter lora_gsm8k_49 loss: 1.2764413356781006
[2025-12-23 14:56:46,565] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 106/128 step: 54
[2025-12-23 14:56:46,921] m-LoRA: Adapter lora_gsm8k_48 loss: 0.9747250080108643
[2025-12-23 14:56:47,081] m-LoRA: Adapter lora_gsm8k_46 loss: 0.6167291402816772
[2025-12-23 14:56:47,267] m-LoRA: Adapter lora_gsm8k_43 loss: 0.6615943312644958
[2025-12-23 14:56:47,460] m-LoRA: Adapter lora_gsm8k_45 loss: 0.6914762854576111
[2025-12-23 14:56:47,462] m-LoRA: Adapter lora_gsm8k_49 epoch: 1/1 iteration: 48/128 step: 13
[2025-12-23 14:56:47,705] m-LoRA: Adapter lora_gsm8k_48 epoch: 1/1 iteration: 80/128 step: 21
[2025-12-23 14:56:47,804] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 92/128 step: 47
[2025-12-23 14:56:47,893] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 122/128 step: 62
[2025-12-23 14:56:48,338] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 108/128 step: 55
[2025-12-23 14:56:48,379] m-LoRA: Adapter lora_gsm8k_49 loss: 1.1859462261199951
[2025-12-23 14:56:48,734] m-LoRA: Adapter lora_gsm8k_48 loss: 0.6545224785804749
[2025-12-23 14:56:48,760] m-LoRA: Adapter lora_gsm8k_46 loss: 0.5178599953651428
[2025-12-23 14:56:48,891] m-LoRA: Adapter lora_gsm8k_43 loss: 0.8337286114692688
[2025-12-23 14:56:49,095] m-LoRA: Adapter lora_gsm8k_45 loss: 0.6740660667419434
[2025-12-23 14:56:49,267] m-LoRA: Adapter lora_gsm8k_49 epoch: 1/1 iteration: 52/128 step: 14
[2025-12-23 14:56:49,666] m-LoRA: Adapter lora_gsm8k_48 epoch: 1/1 iteration: 84/128 step: 22
[2025-12-23 14:56:49,998] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 94/128 step: 48
[2025-12-23 14:56:50,068] m-LoRA: Adapter lora_gsm8k_49 loss: 1.1449589729309082
[2025-12-23 14:56:50,072] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 124/128 step: 63
[2025-12-23 14:56:50,187] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 110/128 step: 56
[2025-12-23 14:56:50,744] m-LoRA: Adapter lora_gsm8k_48 loss: 0.6839372515678406
[2025-12-23 14:56:50,760] m-LoRA: Adapter lora_gsm8k_46 loss: 0.5840401649475098
[2025-12-23 14:56:51,022] m-LoRA: Adapter lora_gsm8k_43 loss: 0.8893347382545471
[2025-12-23 14:56:51,024] m-LoRA: Adapter lora_gsm8k_49 epoch: 1/1 iteration: 56/128 step: 15
[2025-12-23 14:56:51,204] m-LoRA: Adapter lora_gsm8k_45 loss: 0.5652953386306763
[2025-12-23 14:56:51,971] m-LoRA: Adapter lora_gsm8k_49 loss: 1.2168104648590088
[2025-12-23 14:56:51,975] m-LoRA: Adapter lora_gsm8k_48 epoch: 1/1 iteration: 88/128 step: 23
[2025-12-23 14:56:52,151] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 96/128 step: 49
[2025-12-23 14:56:52,229] m-LoRA: Adapter lora_gsm8k_43 epoch: 1/1 iteration: 126/128 step: 64
[2025-12-23 14:56:52,306] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 112/128 step: 57
[2025-12-23 14:56:52,858] m-LoRA: Adapter lora_gsm8k_49 epoch: 1/1 iteration: 60/128 step: 16
[2025-12-23 14:56:52,901] m-LoRA: Adapter lora_gsm8k_48 loss: 0.917789876461029
[2025-12-23 14:56:53,001] m-LoRA: Adapter lora_gsm8k_46 loss: 0.5471276044845581
[2025-12-23 14:56:53,230] m-LoRA: Adapter lora_gsm8k_43 loss: 0.7519188523292542
[2025-12-23 14:56:53,302] m-LoRA: Adapter lora_gsm8k_45 loss: 0.6678112745285034
[2025-12-23 14:56:53,622] m-LoRA: Adapter lora_gsm8k_49 loss: 1.2367922067642212
[2025-12-23 14:56:53,779] m-LoRA: Adapter lora_gsm8k_48 epoch: 1/1 iteration: 92/128 step: 24
[2025-12-23 14:56:53,895] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 98/128 step: 50
[2025-12-23 14:56:54,000] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_43']
[2025-12-23 14:56:54,310] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_50']
[2025-12-23 14:56:54,395] m-LoRA: Adapter lora_gsm8k_50 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:56:54,518] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 114/128 step: 58
[2025-12-23 14:56:54,547] m-LoRA: Adapter lora_gsm8k_48 loss: 0.6606327295303345
[2025-12-23 14:56:54,935] m-LoRA: Adapter lora_gsm8k_46 loss: 0.5696313977241516
[2025-12-23 14:56:54,937] m-LoRA: Adapter lora_gsm8k_49 epoch: 1/1 iteration: 64/128 step: 17
[2025-12-23 14:56:55,112] m-LoRA: Adapter lora_gsm8k_50 loss: 2.863016366958618
[2025-12-23 14:56:55,242] m-LoRA: Adapter lora_gsm8k_45 loss: 0.5821667313575745
[2025-12-23 14:56:55,363] m-LoRA: Adapter lora_gsm8k_48 epoch: 1/1 iteration: 96/128 step: 25
[2025-12-23 14:56:55,789] m-LoRA: Adapter lora_gsm8k_49 loss: 0.9934728741645813
[2025-12-23 14:56:55,793] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 100/128 step: 51
[2025-12-23 14:56:55,991] m-LoRA: Adapter lora_gsm8k_50 epoch: 1/1 iteration: 4/128 step: 2
[2025-12-23 14:56:56,056] m-LoRA: Adapter lora_gsm8k_48 loss: 0.6711367964744568
[2025-12-23 14:56:56,059] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 116/128 step: 59
[2025-12-23 14:56:56,628] m-LoRA: Adapter lora_gsm8k_46 loss: 0.7413979172706604
[2025-12-23 14:56:56,630] m-LoRA: Adapter lora_gsm8k_49 epoch: 1/1 iteration: 68/128 step: 18
[2025-12-23 14:56:57,119] m-LoRA: Adapter lora_gsm8k_48 epoch: 1/1 iteration: 100/128 step: 26
[2025-12-23 14:56:57,161] m-LoRA: Adapter lora_gsm8k_50 loss: 2.4312307834625244
[2025-12-23 14:56:57,311] m-LoRA: Adapter lora_gsm8k_45 loss: 0.5651901364326477
[2025-12-23 14:56:57,313] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 102/128 step: 52
[2025-12-23 14:56:57,718] m-LoRA: Adapter lora_gsm8k_49 loss: 0.7532179355621338
[2025-12-23 14:56:58,131] m-LoRA: Adapter lora_gsm8k_48 loss: 0.6379950046539307
[2025-12-23 14:56:58,135] m-LoRA: Adapter lora_gsm8k_50 epoch: 1/1 iteration: 8/128 step: 3
[2025-12-23 14:56:58,301] m-LoRA: Adapter lora_gsm8k_46 loss: 0.4466988444328308
[2025-12-23 14:56:58,303] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 118/128 step: 60
[2025-12-23 14:56:58,736] m-LoRA: Adapter lora_gsm8k_49 epoch: 1/1 iteration: 72/128 step: 19
[2025-12-23 14:56:59,145] m-LoRA: Adapter lora_gsm8k_48 epoch: 1/1 iteration: 104/128 step: 27
[2025-12-23 14:56:59,192] m-LoRA: Adapter lora_gsm8k_50 loss: 2.3989570140838623
[2025-12-23 14:56:59,385] m-LoRA: Adapter lora_gsm8k_45 loss: 0.5341313481330872
[2025-12-23 14:56:59,388] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 104/128 step: 53
[2025-12-23 14:56:59,667] m-LoRA: Adapter lora_gsm8k_49 loss: 0.9258009195327759
[2025-12-23 14:57:00,079] m-LoRA: Adapter lora_gsm8k_48 loss: 0.6834188103675842
[2025-12-23 14:57:00,084] m-LoRA: Adapter lora_gsm8k_50 epoch: 1/1 iteration: 12/128 step: 4
[2025-12-23 14:57:00,326] m-LoRA: Adapter lora_gsm8k_46 loss: 0.6934090256690979
[2025-12-23 14:57:00,329] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 120/128 step: 61
[2025-12-23 14:57:00,482] m-LoRA: Adapter lora_gsm8k_49 epoch: 1/1 iteration: 76/128 step: 20
[2025-12-23 14:57:00,818] m-LoRA: Adapter lora_gsm8k_50 loss: 2.3803300857543945
[2025-12-23 14:57:01,036] m-LoRA: Adapter lora_gsm8k_45 loss: 0.6959771513938904
[2025-12-23 14:57:01,038] m-LoRA: Adapter lora_gsm8k_48 epoch: 1/1 iteration: 108/128 step: 28
[2025-12-23 14:57:01,272] m-LoRA: Adapter lora_gsm8k_49 loss: 1.2296981811523438
[2025-12-23 14:57:01,275] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 106/128 step: 54
[2025-12-23 14:57:01,788] m-LoRA: Adapter lora_gsm8k_48 loss: 0.7359515428543091
[2025-12-23 14:57:01,791] m-LoRA: Adapter lora_gsm8k_50 epoch: 1/1 iteration: 16/128 step: 5
[2025-12-23 14:57:02,043] m-LoRA: Adapter lora_gsm8k_46 loss: 0.7693068385124207
[2025-12-23 14:57:02,045] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 122/128 step: 62
[2025-12-23 14:57:02,176] m-LoRA: Adapter lora_gsm8k_49 epoch: 1/1 iteration: 80/128 step: 21
[2025-12-23 14:57:02,836] m-LoRA: Adapter lora_gsm8k_48 epoch: 1/1 iteration: 112/128 step: 29
[2025-12-23 14:57:02,897] m-LoRA: Adapter lora_gsm8k_50 loss: 1.9165300130844116
[2025-12-23 14:57:02,945] m-LoRA: Adapter lora_gsm8k_45 loss: 0.5171740055084229
[2025-12-23 14:57:02,948] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 108/128 step: 55
[2025-12-23 14:57:03,311] m-LoRA: Adapter lora_gsm8k_49 loss: 0.8358771800994873
[2025-12-23 14:57:03,606] m-LoRA: Adapter lora_gsm8k_48 loss: 0.5945978760719299
[2025-12-23 14:57:03,796] m-LoRA: Adapter lora_gsm8k_46 loss: 0.7305091023445129
[2025-12-23 14:57:03,798] m-LoRA: Adapter lora_gsm8k_50 epoch: 1/1 iteration: 20/128 step: 6
[2025-12-23 14:57:03,924] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 124/128 step: 63
[2025-12-23 14:57:04,242] m-LoRA: Adapter lora_gsm8k_49 epoch: 1/1 iteration: 84/128 step: 22
[2025-12-23 14:57:04,662] m-LoRA: Adapter lora_gsm8k_48 epoch: 1/1 iteration: 116/128 step: 30
[2025-12-23 14:57:04,706] m-LoRA: Adapter lora_gsm8k_50 loss: 1.8200501203536987
[2025-12-23 14:57:04,899] m-LoRA: Adapter lora_gsm8k_45 loss: 0.5480003356933594
[2025-12-23 14:57:04,903] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 110/128 step: 56
[2025-12-23 14:57:05,191] m-LoRA: Adapter lora_gsm8k_49 loss: 0.7104969620704651
[2025-12-23 14:57:05,606] m-LoRA: Adapter lora_gsm8k_48 loss: 0.7087401747703552
[2025-12-23 14:57:05,610] m-LoRA: Adapter lora_gsm8k_50 epoch: 1/1 iteration: 24/128 step: 7
[2025-12-23 14:57:05,771] m-LoRA: Adapter lora_gsm8k_46 loss: 0.6232859492301941
[2025-12-23 14:57:05,773] m-LoRA: Adapter lora_gsm8k_45 epoch: 1/1 iteration: 126/128 step: 64
[2025-12-23 14:57:06,116] m-LoRA: Adapter lora_gsm8k_49 epoch: 1/1 iteration: 88/128 step: 23
[2025-12-23 14:57:06,612] m-LoRA: Adapter lora_gsm8k_48 epoch: 1/1 iteration: 120/128 step: 31
[2025-12-23 14:57:06,763] m-LoRA: Adapter lora_gsm8k_50 loss: 1.4668406248092651
[2025-12-23 14:57:06,768] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 112/128 step: 57
[2025-12-23 14:57:06,812] m-LoRA: Adapter lora_gsm8k_45 loss: 0.40776148438453674
[2025-12-23 14:57:07,198] m-LoRA: Adapter lora_gsm8k_49 loss: 0.6261133551597595
[2025-12-23 14:57:07,563] m-LoRA: Adapter lora_gsm8k_48 loss: 0.4918462038040161
[2025-12-23 14:57:07,727] m-LoRA: Adapter lora_gsm8k_46 loss: 0.7983465790748596
[2025-12-23 14:57:07,729] m-LoRA: Adapter lora_gsm8k_50 epoch: 1/1 iteration: 28/128 step: 8
[2025-12-23 14:57:07,880] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_45']
[2025-12-23 14:57:08,128] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_51']
[2025-12-23 14:57:08,230] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:57:08,410] m-LoRA: Adapter lora_gsm8k_49 epoch: 1/1 iteration: 92/128 step: 24
[2025-12-23 14:57:09,000] m-LoRA: Adapter lora_gsm8k_48 epoch: 1/1 iteration: 124/128 step: 32
[2025-12-23 14:57:09,051] m-LoRA: Adapter lora_gsm8k_50 loss: 1.65640389919281
[2025-12-23 14:57:09,226] m-LoRA: Adapter lora_gsm8k_51 loss: 2.2151873111724854
[2025-12-23 14:57:09,228] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 114/128 step: 58
[2025-12-23 14:57:09,402] m-LoRA: Adapter lora_gsm8k_49 loss: 0.9226202964782715
[2025-12-23 14:57:09,787] m-LoRA: Adapter lora_gsm8k_48 loss: 0.6750710010528564
[2025-12-23 14:57:10,018] m-LoRA: Adapter lora_gsm8k_46 loss: 0.6994500756263733
[2025-12-23 14:57:10,020] m-LoRA: Adapter lora_gsm8k_50 epoch: 1/1 iteration: 32/128 step: 9
[2025-12-23 14:57:10,157] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 2/128 step: 2
[2025-12-23 14:57:10,399] m-LoRA: Adapter lora_gsm8k_49 epoch: 1/1 iteration: 96/128 step: 25
[2025-12-23 14:57:10,975] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_48']
[2025-12-23 14:57:11,183] m-LoRA: Adapter lora_gsm8k_50 loss: 1.4676945209503174
[2025-12-23 14:57:11,188] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_52']
[2025-12-23 14:57:11,419] m-LoRA: Adapter lora_gsm8k_52 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:57:11,607] m-LoRA: Adapter lora_gsm8k_51 loss: 1.7363187074661255
[2025-12-23 14:57:11,609] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 116/128 step: 59
[2025-12-23 14:57:11,656] m-LoRA: Adapter lora_gsm8k_49 loss: 0.8913233876228333
[2025-12-23 14:57:12,103] m-LoRA: Adapter lora_gsm8k_50 epoch: 1/1 iteration: 36/128 step: 10
[2025-12-23 14:57:12,201] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 4/128 step: 3
[2025-12-23 14:57:12,434] m-LoRA: Adapter lora_gsm8k_49 epoch: 1/1 iteration: 100/128 step: 26
[2025-12-23 14:57:14,360] m-LoRA: Adapter lora_gsm8k_52 loss: 2.383391857147217
[2025-12-23 14:57:14,377] m-LoRA: Adapter lora_gsm8k_46 loss: 0.6559407114982605
[2025-12-23 14:57:14,683] m-LoRA: Adapter lora_gsm8k_50 loss: 1.2482287883758545
[2025-12-23 14:57:14,915] m-LoRA: Adapter lora_gsm8k_51 loss: 1.1720554828643799
[2025-12-23 14:57:15,283] m-LoRA: Adapter lora_gsm8k_49 loss: 0.48993706703186035
[2025-12-23 14:57:17,464] m-LoRA: Adapter lora_gsm8k_52 epoch: 1/1 iteration: 16/128 step: 2
[2025-12-23 14:57:17,713] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 118/128 step: 60
[2025-12-23 14:57:17,812] m-LoRA: Adapter lora_gsm8k_50 epoch: 1/1 iteration: 40/128 step: 11
[2025-12-23 14:57:17,910] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 6/128 step: 4
[2025-12-23 14:57:17,988] m-LoRA: Adapter lora_gsm8k_49 epoch: 1/1 iteration: 104/128 step: 27
[2025-12-23 14:57:19,919] m-LoRA: Adapter lora_gsm8k_52 loss: 1.3563668727874756
[2025-12-23 14:57:19,938] m-LoRA: Adapter lora_gsm8k_46 loss: 0.6239498257637024
[2025-12-23 14:57:20,387] m-LoRA: Adapter lora_gsm8k_50 loss: 1.226941466331482
[2025-12-23 14:57:20,471] m-LoRA: Adapter lora_gsm8k_51 loss: 1.8406596183776855
[2025-12-23 14:57:20,774] m-LoRA: Adapter lora_gsm8k_49 loss: 0.613336443901062
[2025-12-23 14:57:22,705] m-LoRA: Adapter lora_gsm8k_52 epoch: 1/1 iteration: 32/128 step: 3
[2025-12-23 14:57:22,925] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 120/128 step: 61
[2025-12-23 14:57:23,027] m-LoRA: Adapter lora_gsm8k_50 epoch: 1/1 iteration: 44/128 step: 12
[2025-12-23 14:57:23,125] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 8/128 step: 5
[2025-12-23 14:57:23,203] m-LoRA: Adapter lora_gsm8k_49 epoch: 1/1 iteration: 108/128 step: 28
[2025-12-23 14:57:24,590] m-LoRA: Adapter lora_gsm8k_52 loss: 1.6127926111221313
[2025-12-23 14:57:24,610] m-LoRA: Adapter lora_gsm8k_46 loss: 0.6640938520431519
[2025-12-23 14:57:24,994] m-LoRA: Adapter lora_gsm8k_50 loss: 1.5264911651611328
[2025-12-23 14:57:25,061] m-LoRA: Adapter lora_gsm8k_51 loss: 1.6972720623016357
[2025-12-23 14:57:25,396] m-LoRA: Adapter lora_gsm8k_49 loss: 0.630704402923584
[2025-12-23 14:57:26,683] m-LoRA: Adapter lora_gsm8k_52 epoch: 1/1 iteration: 48/128 step: 4
[2025-12-23 14:57:26,915] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 122/128 step: 62
[2025-12-23 14:57:27,014] m-LoRA: Adapter lora_gsm8k_50 epoch: 1/1 iteration: 48/128 step: 13
[2025-12-23 14:57:27,111] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 10/128 step: 6
[2025-12-23 14:57:27,192] m-LoRA: Adapter lora_gsm8k_49 epoch: 1/1 iteration: 112/128 step: 29
[2025-12-23 14:57:29,231] m-LoRA: Adapter lora_gsm8k_52 loss: 1.1369671821594238
[2025-12-23 14:57:29,254] m-LoRA: Adapter lora_gsm8k_46 loss: 0.5395004153251648
[2025-12-23 14:57:29,643] m-LoRA: Adapter lora_gsm8k_50 loss: 1.4805810451507568
[2025-12-23 14:57:29,814] m-LoRA: Adapter lora_gsm8k_51 loss: 1.189693808555603
[2025-12-23 14:57:30,315] m-LoRA: Adapter lora_gsm8k_49 loss: 0.7990747094154358
[2025-12-23 14:57:32,212] m-LoRA: Adapter lora_gsm8k_52 epoch: 1/1 iteration: 64/128 step: 5
[2025-12-23 14:57:32,412] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 124/128 step: 63
[2025-12-23 14:57:32,470] m-LoRA: Adapter lora_gsm8k_50 epoch: 1/1 iteration: 52/128 step: 14
[2025-12-23 14:57:32,547] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 12/128 step: 7
[2025-12-23 14:57:32,620] m-LoRA: Adapter lora_gsm8k_49 epoch: 1/1 iteration: 116/128 step: 30
[2025-12-23 14:57:34,311] m-LoRA: Adapter lora_gsm8k_52 loss: 1.4167335033416748
[2025-12-23 14:57:34,329] m-LoRA: Adapter lora_gsm8k_46 loss: 0.43338868021965027
[2025-12-23 14:57:34,759] m-LoRA: Adapter lora_gsm8k_50 loss: 1.5122766494750977
[2025-12-23 14:57:34,867] m-LoRA: Adapter lora_gsm8k_51 loss: 1.5758070945739746
[2025-12-23 14:57:35,183] m-LoRA: Adapter lora_gsm8k_49 loss: 0.63377845287323
[2025-12-23 14:57:36,696] m-LoRA: Adapter lora_gsm8k_52 epoch: 1/1 iteration: 80/128 step: 6
[2025-12-23 14:57:36,887] m-LoRA: Adapter lora_gsm8k_46 epoch: 1/1 iteration: 126/128 step: 64
[2025-12-23 14:57:36,983] m-LoRA: Adapter lora_gsm8k_50 epoch: 1/1 iteration: 56/128 step: 15
[2025-12-23 14:57:37,087] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 14/128 step: 8
[2025-12-23 14:57:37,167] m-LoRA: Adapter lora_gsm8k_49 epoch: 1/1 iteration: 120/128 step: 31
[2025-12-23 14:57:38,890] m-LoRA: Adapter lora_gsm8k_52 loss: 1.1869282722473145
[2025-12-23 14:57:38,909] m-LoRA: Adapter lora_gsm8k_46 loss: 0.6179296374320984
[2025-12-23 14:57:39,408] m-LoRA: Adapter lora_gsm8k_50 loss: 1.1722869873046875
[2025-12-23 14:57:39,483] m-LoRA: Adapter lora_gsm8k_51 loss: 1.5223143100738525
[2025-12-23 14:57:39,789] m-LoRA: Adapter lora_gsm8k_49 loss: 0.6762335896492004
[2025-12-23 14:57:41,405] m-LoRA: Adapter lora_gsm8k_52 epoch: 1/1 iteration: 96/128 step: 7
[2025-12-23 14:57:41,599] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_46']
[2025-12-23 14:57:42,209] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_53']
[2025-12-23 14:57:42,246] m-LoRA: Adapter lora_gsm8k_53 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:57:42,348] m-LoRA: Adapter lora_gsm8k_50 epoch: 1/1 iteration: 60/128 step: 16
[2025-12-23 14:57:42,423] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 16/128 step: 9
[2025-12-23 14:57:42,479] m-LoRA: Adapter lora_gsm8k_49 epoch: 1/1 iteration: 124/128 step: 32
[2025-12-23 14:57:44,366] m-LoRA: Adapter lora_gsm8k_52 loss: 1.2439426183700562
[2025-12-23 14:57:44,433] m-LoRA: Adapter lora_gsm8k_53 loss: 2.3437094688415527
[2025-12-23 14:57:44,795] m-LoRA: Adapter lora_gsm8k_50 loss: 1.643237590789795
[2025-12-23 14:57:44,893] m-LoRA: Adapter lora_gsm8k_51 loss: 1.3226314783096313
[2025-12-23 14:57:45,204] m-LoRA: Adapter lora_gsm8k_49 loss: 0.5488420724868774
[2025-12-23 14:57:46,929] m-LoRA: Adapter lora_gsm8k_52 epoch: 1/1 iteration: 112/128 step: 8
[2025-12-23 14:57:47,188] m-LoRA: Adapter lora_gsm8k_53 epoch: 1/1 iteration: 4/128 step: 2
[2025-12-23 14:57:47,259] m-LoRA: Adapter lora_gsm8k_50 epoch: 1/1 iteration: 64/128 step: 17
[2025-12-23 14:57:47,334] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 18/128 step: 10
[2025-12-23 14:57:47,401] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_49']
[2025-12-23 14:57:47,568] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_54']
[2025-12-23 14:57:47,729] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:57:50,038] m-LoRA: Adapter lora_gsm8k_52 loss: 0.8977954983711243
[2025-12-23 14:57:50,066] m-LoRA: Adapter lora_gsm8k_53 loss: 2.3484268188476562
[2025-12-23 14:57:50,498] m-LoRA: Adapter lora_gsm8k_50 loss: 1.3836960792541504
[2025-12-23 14:57:50,592] m-LoRA: Adapter lora_gsm8k_51 loss: 1.0091428756713867
[2025-12-23 14:57:50,694] m-LoRA: Adapter lora_gsm8k_54 loss: 2.663693904876709
[2025-12-23 14:57:53,422] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_52']
[2025-12-23 14:57:53,817] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_55']
[2025-12-23 14:57:53,974] m-LoRA: Adapter lora_gsm8k_55 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:57:54,039] m-LoRA: Adapter lora_gsm8k_53 epoch: 1/1 iteration: 8/128 step: 3
[2025-12-23 14:57:54,098] m-LoRA: Adapter lora_gsm8k_50 epoch: 1/1 iteration: 68/128 step: 18
[2025-12-23 14:57:54,167] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 20/128 step: 11
[2025-12-23 14:57:54,229] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 2/128 step: 2
[2025-12-23 14:57:54,911] m-LoRA: Adapter lora_gsm8k_55 loss: 2.223095655441284
[2025-12-23 14:57:55,171] m-LoRA: Adapter lora_gsm8k_53 loss: 1.6987982988357544
[2025-12-23 14:57:55,405] m-LoRA: Adapter lora_gsm8k_50 loss: 1.8006643056869507
[2025-12-23 14:57:55,644] m-LoRA: Adapter lora_gsm8k_51 loss: 1.0898221731185913
[2025-12-23 14:57:55,856] m-LoRA: Adapter lora_gsm8k_54 loss: 1.6809570789337158
[2025-12-23 14:57:55,859] m-LoRA: Adapter lora_gsm8k_55 epoch: 1/1 iteration: 4/128 step: 2
[2025-12-23 14:57:56,152] m-LoRA: Adapter lora_gsm8k_53 epoch: 1/1 iteration: 12/128 step: 4
[2025-12-23 14:57:56,341] m-LoRA: Adapter lora_gsm8k_50 epoch: 1/1 iteration: 72/128 step: 19
[2025-12-23 14:57:56,783] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 22/128 step: 12
[2025-12-23 14:57:56,822] m-LoRA: Adapter lora_gsm8k_55 loss: 1.436286211013794
[2025-12-23 14:57:57,185] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 4/128 step: 3
[2025-12-23 14:57:57,225] m-LoRA: Adapter lora_gsm8k_53 loss: 1.2824572324752808
[2025-12-23 14:57:57,490] m-LoRA: Adapter lora_gsm8k_50 loss: 1.529206395149231
[2025-12-23 14:57:57,608] m-LoRA: Adapter lora_gsm8k_51 loss: 0.89397794008255
[2025-12-23 14:57:57,813] m-LoRA: Adapter lora_gsm8k_54 loss: 1.4418864250183105
[2025-12-23 14:57:57,815] m-LoRA: Adapter lora_gsm8k_55 epoch: 1/1 iteration: 8/128 step: 3
[2025-12-23 14:57:58,203] m-LoRA: Adapter lora_gsm8k_53 epoch: 1/1 iteration: 16/128 step: 5
[2025-12-23 14:57:58,372] m-LoRA: Adapter lora_gsm8k_50 epoch: 1/1 iteration: 76/128 step: 20
[2025-12-23 14:57:58,734] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 24/128 step: 13
[2025-12-23 14:57:58,766] m-LoRA: Adapter lora_gsm8k_55 loss: 1.514211893081665
[2025-12-23 14:57:59,089] m-LoRA: Adapter lora_gsm8k_53 loss: 1.4144532680511475
[2025-12-23 14:57:59,094] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 6/128 step: 4
[2025-12-23 14:57:59,365] m-LoRA: Adapter lora_gsm8k_50 loss: 1.3499808311462402
[2025-12-23 14:57:59,496] m-LoRA: Adapter lora_gsm8k_51 loss: 0.8440617918968201
[2025-12-23 14:57:59,604] m-LoRA: Adapter lora_gsm8k_55 epoch: 1/1 iteration: 12/128 step: 4
[2025-12-23 14:57:59,832] m-LoRA: Adapter lora_gsm8k_54 loss: 1.7029281854629517
[2025-12-23 14:58:00,008] m-LoRA: Adapter lora_gsm8k_53 epoch: 1/1 iteration: 20/128 step: 6
[2025-12-23 14:58:00,476] m-LoRA: Adapter lora_gsm8k_50 epoch: 1/1 iteration: 80/128 step: 21
[2025-12-23 14:58:00,518] m-LoRA: Adapter lora_gsm8k_55 loss: 1.2168333530426025
[2025-12-23 14:58:00,854] m-LoRA: Adapter lora_gsm8k_53 loss: 1.4267206192016602
[2025-12-23 14:58:00,858] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 26/128 step: 14
[2025-12-23 14:58:01,214] m-LoRA: Adapter lora_gsm8k_50 loss: 1.3444100618362427
[2025-12-23 14:58:01,218] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 8/128 step: 5
[2025-12-23 14:58:01,540] m-LoRA: Adapter lora_gsm8k_51 loss: 1.0573112964630127
[2025-12-23 14:58:01,621] m-LoRA: Adapter lora_gsm8k_55 epoch: 1/1 iteration: 16/128 step: 5
[2025-12-23 14:58:01,778] m-LoRA: Adapter lora_gsm8k_54 loss: 1.5267695188522339
[2025-12-23 14:58:01,920] m-LoRA: Adapter lora_gsm8k_53 epoch: 1/1 iteration: 24/128 step: 7
[2025-12-23 14:58:02,487] m-LoRA: Adapter lora_gsm8k_55 loss: 1.2418071031570435
[2025-12-23 14:58:02,491] m-LoRA: Adapter lora_gsm8k_50 epoch: 1/1 iteration: 84/128 step: 22
[2025-12-23 14:58:02,647] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 28/128 step: 15
[2025-12-23 14:58:02,728] m-LoRA: Adapter lora_gsm8k_53 loss: 1.920790433883667
[2025-12-23 14:58:02,731] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 10/128 step: 6
[2025-12-23 14:58:03,418] m-LoRA: Adapter lora_gsm8k_55 epoch: 1/1 iteration: 20/128 step: 6
[2025-12-23 14:58:03,560] m-LoRA: Adapter lora_gsm8k_50 loss: 1.6823499202728271
[2025-12-23 14:58:03,564] m-LoRA: Adapter lora_gsm8k_53 epoch: 1/1 iteration: 28/128 step: 8
[2025-12-23 14:58:03,645] m-LoRA: Adapter lora_gsm8k_51 loss: 0.6861146688461304
[2025-12-23 14:58:03,678] m-LoRA: Adapter lora_gsm8k_54 loss: 1.3397750854492188
[2025-12-23 14:58:04,165] m-LoRA: Adapter lora_gsm8k_55 loss: 1.3458935022354126
[2025-12-23 14:58:04,344] m-LoRA: Adapter lora_gsm8k_50 epoch: 1/1 iteration: 88/128 step: 23
[2025-12-23 14:58:04,712] m-LoRA: Adapter lora_gsm8k_53 loss: 1.1339398622512817
[2025-12-23 14:58:04,717] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 30/128 step: 16
[2025-12-23 14:58:04,833] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 12/128 step: 7
[2025-12-23 14:58:05,146] m-LoRA: Adapter lora_gsm8k_55 epoch: 1/1 iteration: 24/128 step: 7
[2025-12-23 14:58:05,216] m-LoRA: Adapter lora_gsm8k_50 loss: 1.6412426233291626
[2025-12-23 14:58:05,222] m-LoRA: Adapter lora_gsm8k_51 loss: 0.6205703020095825
[2025-12-23 14:58:05,519] m-LoRA: Adapter lora_gsm8k_54 loss: 1.5863077640533447
[2025-12-23 14:58:05,659] m-LoRA: Adapter lora_gsm8k_53 epoch: 1/1 iteration: 32/128 step: 9
[2025-12-23 14:58:06,198] m-LoRA: Adapter lora_gsm8k_50 epoch: 1/1 iteration: 92/128 step: 24
[2025-12-23 14:58:06,241] m-LoRA: Adapter lora_gsm8k_55 loss: 0.9456728100776672
[2025-12-23 14:58:06,463] m-LoRA: Adapter lora_gsm8k_53 loss: 1.5302554368972778
[2025-12-23 14:58:06,466] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 32/128 step: 17
[2025-12-23 14:58:06,557] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 14/128 step: 8
[2025-12-23 14:58:07,035] m-LoRA: Adapter lora_gsm8k_50 loss: 1.0350924730300903
[2025-12-23 14:58:07,339] m-LoRA: Adapter lora_gsm8k_51 loss: 0.8141182065010071
[2025-12-23 14:58:07,342] m-LoRA: Adapter lora_gsm8k_55 epoch: 1/1 iteration: 28/128 step: 8
[2025-12-23 14:58:07,491] m-LoRA: Adapter lora_gsm8k_53 epoch: 1/1 iteration: 36/128 step: 10
[2025-12-23 14:58:07,523] m-LoRA: Adapter lora_gsm8k_54 loss: 1.1225792169570923
[2025-12-23 14:58:08,267] m-LoRA: Adapter lora_gsm8k_55 loss: 1.1428847312927246
[2025-12-23 14:58:08,271] m-LoRA: Adapter lora_gsm8k_50 epoch: 1/1 iteration: 96/128 step: 25
[2025-12-23 14:58:08,740] m-LoRA: Adapter lora_gsm8k_53 loss: 1.0000501871109009
[2025-12-23 14:58:08,745] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 34/128 step: 18
[2025-12-23 14:58:08,870] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 16/128 step: 9
[2025-12-23 14:58:09,229] m-LoRA: Adapter lora_gsm8k_55 epoch: 1/1 iteration: 32/128 step: 9
[2025-12-23 14:58:09,286] m-LoRA: Adapter lora_gsm8k_50 loss: 1.4425218105316162
[2025-12-23 14:58:09,367] m-LoRA: Adapter lora_gsm8k_51 loss: 0.8917295932769775
[2025-12-23 14:58:09,691] m-LoRA: Adapter lora_gsm8k_54 loss: 1.1607474088668823
[2025-12-23 14:58:10,159] m-LoRA: Adapter lora_gsm8k_55 loss: 1.0569039583206177
[2025-12-23 14:58:10,164] m-LoRA: Adapter lora_gsm8k_53 epoch: 1/1 iteration: 40/128 step: 11
[2025-12-23 14:58:10,323] m-LoRA: Adapter lora_gsm8k_50 epoch: 1/1 iteration: 100/128 step: 26
[2025-12-23 14:58:10,422] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 36/128 step: 19
[2025-12-23 14:58:10,517] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 18/128 step: 10
[2025-12-23 14:58:11,143] m-LoRA: Adapter lora_gsm8k_55 epoch: 1/1 iteration: 36/128 step: 10
[2025-12-23 14:58:11,201] m-LoRA: Adapter lora_gsm8k_53 loss: 1.235711693763733
[2025-12-23 14:58:11,545] m-LoRA: Adapter lora_gsm8k_50 loss: 1.2611429691314697
[2025-12-23 14:58:11,673] m-LoRA: Adapter lora_gsm8k_51 loss: 0.5682824850082397
[2025-12-23 14:58:11,855] m-LoRA: Adapter lora_gsm8k_54 loss: 1.025464653968811
[2025-12-23 14:58:12,201] m-LoRA: Adapter lora_gsm8k_55 loss: 1.086907982826233
[2025-12-23 14:58:12,204] m-LoRA: Adapter lora_gsm8k_53 epoch: 1/1 iteration: 44/128 step: 12
[2025-12-23 14:58:12,414] m-LoRA: Adapter lora_gsm8k_50 epoch: 1/1 iteration: 104/128 step: 27
[2025-12-23 14:58:12,624] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 38/128 step: 20
[2025-12-23 14:58:12,706] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 20/128 step: 11
[2025-12-23 14:58:13,214] m-LoRA: Adapter lora_gsm8k_55 epoch: 1/1 iteration: 40/128 step: 11
[2025-12-23 14:58:13,251] m-LoRA: Adapter lora_gsm8k_53 loss: 1.1872402429580688
[2025-12-23 14:58:13,575] m-LoRA: Adapter lora_gsm8k_50 loss: 1.3099626302719116
[2025-12-23 14:58:13,697] m-LoRA: Adapter lora_gsm8k_51 loss: 0.5568987131118774
[2025-12-23 14:58:13,841] m-LoRA: Adapter lora_gsm8k_54 loss: 0.8203181624412537
[2025-12-23 14:58:14,130] m-LoRA: Adapter lora_gsm8k_55 loss: 0.9180657267570496
[2025-12-23 14:58:14,257] m-LoRA: Adapter lora_gsm8k_53 epoch: 1/1 iteration: 48/128 step: 13
[2025-12-23 14:58:14,380] m-LoRA: Adapter lora_gsm8k_50 epoch: 1/1 iteration: 108/128 step: 28
[2025-12-23 14:58:14,602] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 40/128 step: 21
[2025-12-23 14:58:14,704] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 22/128 step: 12
[2025-12-23 14:58:15,169] m-LoRA: Adapter lora_gsm8k_53 loss: 1.2922593355178833
[2025-12-23 14:58:15,172] m-LoRA: Adapter lora_gsm8k_55 epoch: 1/1 iteration: 44/128 step: 12
[2025-12-23 14:58:15,494] m-LoRA: Adapter lora_gsm8k_50 loss: 1.4706934690475464
[2025-12-23 14:58:15,666] m-LoRA: Adapter lora_gsm8k_51 loss: 0.7019595503807068
[2025-12-23 14:58:15,851] m-LoRA: Adapter lora_gsm8k_54 loss: 0.5623428225517273
[2025-12-23 14:58:16,221] m-LoRA: Adapter lora_gsm8k_55 loss: 0.8918725252151489
[2025-12-23 14:58:16,224] m-LoRA: Adapter lora_gsm8k_53 epoch: 1/1 iteration: 52/128 step: 14
[2025-12-23 14:58:16,384] m-LoRA: Adapter lora_gsm8k_50 epoch: 1/1 iteration: 112/128 step: 29
[2025-12-23 14:58:16,481] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 42/128 step: 22
[2025-12-23 14:58:16,583] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 24/128 step: 13
[2025-12-23 14:58:17,065] m-LoRA: Adapter lora_gsm8k_55 epoch: 1/1 iteration: 48/128 step: 13
[2025-12-23 14:58:17,114] m-LoRA: Adapter lora_gsm8k_53 loss: 1.227449655532837
[2025-12-23 14:58:17,333] m-LoRA: Adapter lora_gsm8k_50 loss: 1.6770505905151367
[2025-12-23 14:58:17,486] m-LoRA: Adapter lora_gsm8k_51 loss: 0.5879579186439514
[2025-12-23 14:58:17,607] m-LoRA: Adapter lora_gsm8k_54 loss: 0.6666713356971741
[2025-12-23 14:58:18,122] m-LoRA: Adapter lora_gsm8k_55 loss: 0.7796322107315063
[2025-12-23 14:58:18,127] m-LoRA: Adapter lora_gsm8k_53 epoch: 1/1 iteration: 56/128 step: 15
[2025-12-23 14:58:18,301] m-LoRA: Adapter lora_gsm8k_50 epoch: 1/1 iteration: 116/128 step: 30
[2025-12-23 14:58:18,396] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 44/128 step: 23
[2025-12-23 14:58:18,471] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 26/128 step: 14
[2025-12-23 14:58:19,116] m-LoRA: Adapter lora_gsm8k_55 epoch: 1/1 iteration: 52/128 step: 14
[2025-12-23 14:58:19,174] m-LoRA: Adapter lora_gsm8k_53 loss: 1.053431510925293
[2025-12-23 14:58:19,455] m-LoRA: Adapter lora_gsm8k_50 loss: 1.5674476623535156
[2025-12-23 14:58:19,587] m-LoRA: Adapter lora_gsm8k_51 loss: 0.5319514870643616
[2025-12-23 14:58:19,870] m-LoRA: Adapter lora_gsm8k_54 loss: 0.9045724868774414
[2025-12-23 14:58:20,346] m-LoRA: Adapter lora_gsm8k_55 loss: 0.662577748298645
[2025-12-23 14:58:20,350] m-LoRA: Adapter lora_gsm8k_53 epoch: 1/1 iteration: 60/128 step: 16
[2025-12-23 14:58:20,526] m-LoRA: Adapter lora_gsm8k_50 epoch: 1/1 iteration: 120/128 step: 31
[2025-12-23 14:58:20,622] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 46/128 step: 24
[2025-12-23 14:58:20,711] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 28/128 step: 15
[2025-12-23 14:58:21,302] m-LoRA: Adapter lora_gsm8k_55 epoch: 1/1 iteration: 56/128 step: 15
[2025-12-23 14:58:21,343] m-LoRA: Adapter lora_gsm8k_53 loss: 1.0825532674789429
[2025-12-23 14:58:21,663] m-LoRA: Adapter lora_gsm8k_50 loss: 1.3706947565078735
[2025-12-23 14:58:21,782] m-LoRA: Adapter lora_gsm8k_51 loss: 0.7050880789756775
[2025-12-23 14:58:21,969] m-LoRA: Adapter lora_gsm8k_54 loss: 0.42503467202186584
[2025-12-23 14:58:22,435] m-LoRA: Adapter lora_gsm8k_55 loss: 0.5512464642524719
[2025-12-23 14:58:22,440] m-LoRA: Adapter lora_gsm8k_53 epoch: 1/1 iteration: 64/128 step: 17
[2025-12-23 14:58:22,591] m-LoRA: Adapter lora_gsm8k_50 epoch: 1/1 iteration: 124/128 step: 32
[2025-12-23 14:58:22,689] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 48/128 step: 25
[2025-12-23 14:58:22,837] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 30/128 step: 16
[2025-12-23 14:58:23,458] m-LoRA: Adapter lora_gsm8k_53 loss: 1.1238268613815308
[2025-12-23 14:58:23,461] m-LoRA: Adapter lora_gsm8k_55 epoch: 1/1 iteration: 60/128 step: 16
[2025-12-23 14:58:23,776] m-LoRA: Adapter lora_gsm8k_50 loss: 1.383315086364746
[2025-12-23 14:58:23,910] m-LoRA: Adapter lora_gsm8k_51 loss: 0.5641523599624634
[2025-12-23 14:58:24,086] m-LoRA: Adapter lora_gsm8k_54 loss: 0.642235279083252
[2025-12-23 14:58:24,417] m-LoRA: Adapter lora_gsm8k_55 loss: 0.6222772598266602
[2025-12-23 14:58:24,420] m-LoRA: Adapter lora_gsm8k_53 epoch: 1/1 iteration: 68/128 step: 18
[2025-12-23 14:58:24,586] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_50']
[2025-12-23 14:58:24,810] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_56']
[2025-12-23 14:58:24,982] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:58:25,076] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 50/128 step: 26
[2025-12-23 14:58:25,147] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 32/128 step: 17
[2025-12-23 14:58:25,667] m-LoRA: Adapter lora_gsm8k_55 epoch: 1/1 iteration: 64/128 step: 17
[2025-12-23 14:58:25,731] m-LoRA: Adapter lora_gsm8k_53 loss: 1.1699774265289307
[2025-12-23 14:58:25,738] m-LoRA: Adapter lora_gsm8k_56 loss: 2.6541354656219482
[2025-12-23 14:58:25,997] m-LoRA: Adapter lora_gsm8k_51 loss: 0.538581907749176
[2025-12-23 14:58:26,093] m-LoRA: Adapter lora_gsm8k_54 loss: 0.7179161906242371
[2025-12-23 14:58:26,419] m-LoRA: Adapter lora_gsm8k_55 loss: 0.6101858019828796
[2025-12-23 14:58:26,580] m-LoRA: Adapter lora_gsm8k_53 epoch: 1/1 iteration: 72/128 step: 19
[2025-12-23 14:58:26,698] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 2/128 step: 2
[2025-12-23 14:58:26,798] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 52/128 step: 27
[2025-12-23 14:58:26,893] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 34/128 step: 18
[2025-12-23 14:58:27,443] m-LoRA: Adapter lora_gsm8k_55 epoch: 1/1 iteration: 68/128 step: 18
[2025-12-23 14:58:27,511] m-LoRA: Adapter lora_gsm8k_53 loss: 0.9276048541069031
[2025-12-23 14:58:27,517] m-LoRA: Adapter lora_gsm8k_56 loss: 1.758661150932312
[2025-12-23 14:58:27,708] m-LoRA: Adapter lora_gsm8k_51 loss: 0.523855447769165
[2025-12-23 14:58:27,885] m-LoRA: Adapter lora_gsm8k_54 loss: 0.6536531448364258
[2025-12-23 14:58:28,243] m-LoRA: Adapter lora_gsm8k_55 loss: 0.6891794204711914
[2025-12-23 14:58:28,452] m-LoRA: Adapter lora_gsm8k_53 epoch: 1/1 iteration: 76/128 step: 20
[2025-12-23 14:58:28,551] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 4/128 step: 3
[2025-12-23 14:58:28,629] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 54/128 step: 28
[2025-12-23 14:58:28,704] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 36/128 step: 19
[2025-12-23 14:58:29,342] m-LoRA: Adapter lora_gsm8k_55 epoch: 1/1 iteration: 72/128 step: 19
[2025-12-23 14:58:29,402] m-LoRA: Adapter lora_gsm8k_53 loss: 0.8169460296630859
[2025-12-23 14:58:29,408] m-LoRA: Adapter lora_gsm8k_56 loss: 1.8553316593170166
[2025-12-23 14:58:29,619] m-LoRA: Adapter lora_gsm8k_51 loss: 0.5364069938659668
[2025-12-23 14:58:29,760] m-LoRA: Adapter lora_gsm8k_54 loss: 0.6450586915016174
[2025-12-23 14:58:30,265] m-LoRA: Adapter lora_gsm8k_55 loss: 0.5988753437995911
[2025-12-23 14:58:30,402] m-LoRA: Adapter lora_gsm8k_53 epoch: 1/1 iteration: 80/128 step: 21
[2025-12-23 14:58:30,495] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 6/128 step: 4
[2025-12-23 14:58:30,587] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 56/128 step: 29
[2025-12-23 14:58:30,665] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 38/128 step: 20
[2025-12-23 14:58:31,428] m-LoRA: Adapter lora_gsm8k_55 epoch: 1/1 iteration: 76/128 step: 20
[2025-12-23 14:58:31,490] m-LoRA: Adapter lora_gsm8k_53 loss: 0.8529261946678162
[2025-12-23 14:58:31,502] m-LoRA: Adapter lora_gsm8k_51 loss: 0.6436402201652527
[2025-12-23 14:58:31,709] m-LoRA: Adapter lora_gsm8k_56 loss: 1.6361076831817627
[2025-12-23 14:58:31,867] m-LoRA: Adapter lora_gsm8k_54 loss: 0.685727596282959
[2025-12-23 14:58:32,158] m-LoRA: Adapter lora_gsm8k_55 loss: 0.5832970142364502
[2025-12-23 14:58:32,416] m-LoRA: Adapter lora_gsm8k_53 epoch: 1/1 iteration: 84/128 step: 22
[2025-12-23 14:58:32,511] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 58/128 step: 30
[2025-12-23 14:58:32,600] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 8/128 step: 5
[2025-12-23 14:58:32,689] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 40/128 step: 21
[2025-12-23 14:58:33,290] m-LoRA: Adapter lora_gsm8k_55 epoch: 1/1 iteration: 80/128 step: 21
[2025-12-23 14:58:33,356] m-LoRA: Adapter lora_gsm8k_53 loss: 0.7898929119110107
[2025-12-23 14:58:33,369] m-LoRA: Adapter lora_gsm8k_51 loss: 0.6223980188369751
[2025-12-23 14:58:33,563] m-LoRA: Adapter lora_gsm8k_56 loss: 1.8163597583770752
[2025-12-23 14:58:33,802] m-LoRA: Adapter lora_gsm8k_54 loss: 0.559412956237793
[2025-12-23 14:58:34,210] m-LoRA: Adapter lora_gsm8k_55 loss: 0.7278444170951843
[2025-12-23 14:58:34,215] m-LoRA: Adapter lora_gsm8k_53 epoch: 1/1 iteration: 88/128 step: 23
[2025-12-23 14:58:34,391] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 60/128 step: 31
[2025-12-23 14:58:34,491] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 10/128 step: 6
[2025-12-23 14:58:34,610] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 42/128 step: 22
[2025-12-23 14:58:35,173] m-LoRA: Adapter lora_gsm8k_55 epoch: 1/1 iteration: 84/128 step: 22
[2025-12-23 14:58:35,228] m-LoRA: Adapter lora_gsm8k_53 loss: 0.8923129439353943
[2025-12-23 14:58:35,249] m-LoRA: Adapter lora_gsm8k_51 loss: 0.6968467235565186
[2025-12-23 14:58:35,496] m-LoRA: Adapter lora_gsm8k_56 loss: 1.5290095806121826
[2025-12-23 14:58:35,660] m-LoRA: Adapter lora_gsm8k_54 loss: 0.7047916650772095
[2025-12-23 14:58:36,012] m-LoRA: Adapter lora_gsm8k_55 loss: 0.6294459700584412
[2025-12-23 14:58:36,164] m-LoRA: Adapter lora_gsm8k_53 epoch: 1/1 iteration: 92/128 step: 24
[2025-12-23 14:58:36,267] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 62/128 step: 32
[2025-12-23 14:58:36,354] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 12/128 step: 7
[2025-12-23 14:58:36,480] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 44/128 step: 23
[2025-12-23 14:58:37,130] m-LoRA: Adapter lora_gsm8k_55 epoch: 1/1 iteration: 88/128 step: 23
[2025-12-23 14:58:37,184] m-LoRA: Adapter lora_gsm8k_53 loss: 0.5989586710929871
[2025-12-23 14:58:37,222] m-LoRA: Adapter lora_gsm8k_51 loss: 0.5792616605758667
[2025-12-23 14:58:37,464] m-LoRA: Adapter lora_gsm8k_56 loss: 1.2302424907684326
[2025-12-23 14:58:37,555] m-LoRA: Adapter lora_gsm8k_54 loss: 0.6098620891571045
[2025-12-23 14:58:37,879] m-LoRA: Adapter lora_gsm8k_55 loss: 0.45155397057533264
[2025-12-23 14:58:38,186] m-LoRA: Adapter lora_gsm8k_53 epoch: 1/1 iteration: 96/128 step: 25
[2025-12-23 14:58:38,298] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 64/128 step: 33
[2025-12-23 14:58:38,375] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 14/128 step: 8
[2025-12-23 14:58:38,452] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 46/128 step: 24
[2025-12-23 14:58:39,001] m-LoRA: Adapter lora_gsm8k_55 epoch: 1/1 iteration: 92/128 step: 24
[2025-12-23 14:58:39,056] m-LoRA: Adapter lora_gsm8k_53 loss: 0.6219915747642517
[2025-12-23 14:58:39,172] m-LoRA: Adapter lora_gsm8k_51 loss: 0.5418393015861511
[2025-12-23 14:58:39,368] m-LoRA: Adapter lora_gsm8k_56 loss: 1.539057970046997
[2025-12-23 14:58:39,498] m-LoRA: Adapter lora_gsm8k_54 loss: 0.4274163544178009
[2025-12-23 14:58:39,785] m-LoRA: Adapter lora_gsm8k_55 loss: 0.5553288459777832
[2025-12-23 14:58:39,906] m-LoRA: Adapter lora_gsm8k_53 epoch: 1/1 iteration: 100/128 step: 26
[2025-12-23 14:58:39,990] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 66/128 step: 34
[2025-12-23 14:58:40,070] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 16/128 step: 9
[2025-12-23 14:58:40,158] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 48/128 step: 25
[2025-12-23 14:58:40,787] m-LoRA: Adapter lora_gsm8k_55 epoch: 1/1 iteration: 96/128 step: 25
[2025-12-23 14:58:40,848] m-LoRA: Adapter lora_gsm8k_53 loss: 0.6153032779693604
[2025-12-23 14:58:40,854] m-LoRA: Adapter lora_gsm8k_51 loss: 0.6002957820892334
[2025-12-23 14:58:41,040] m-LoRA: Adapter lora_gsm8k_56 loss: 1.2482613325119019
[2025-12-23 14:58:41,147] m-LoRA: Adapter lora_gsm8k_54 loss: 0.5082278847694397
[2025-12-23 14:58:41,485] m-LoRA: Adapter lora_gsm8k_55 loss: 0.6010977625846863
[2025-12-23 14:58:41,738] m-LoRA: Adapter lora_gsm8k_53 epoch: 1/1 iteration: 104/128 step: 27
[2025-12-23 14:58:41,834] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 68/128 step: 35
[2025-12-23 14:58:41,907] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 18/128 step: 10
[2025-12-23 14:58:41,999] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 50/128 step: 26
[2025-12-23 14:58:42,957] m-LoRA: Adapter lora_gsm8k_55 epoch: 1/1 iteration: 100/128 step: 26
[2025-12-23 14:58:43,021] m-LoRA: Adapter lora_gsm8k_53 loss: 0.7479273676872253
[2025-12-23 14:58:43,038] m-LoRA: Adapter lora_gsm8k_51 loss: 0.6939135193824768
[2025-12-23 14:58:43,280] m-LoRA: Adapter lora_gsm8k_56 loss: 0.966089129447937
[2025-12-23 14:58:43,441] m-LoRA: Adapter lora_gsm8k_54 loss: 0.47094249725341797
[2025-12-23 14:58:43,790] m-LoRA: Adapter lora_gsm8k_55 loss: 0.4914339780807495
[2025-12-23 14:58:43,958] m-LoRA: Adapter lora_gsm8k_53 epoch: 1/1 iteration: 108/128 step: 28
[2025-12-23 14:58:44,055] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 70/128 step: 36
[2025-12-23 14:58:44,128] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 20/128 step: 11
[2025-12-23 14:58:44,259] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 52/128 step: 27
[2025-12-23 14:58:44,739] m-LoRA: Adapter lora_gsm8k_55 epoch: 1/1 iteration: 104/128 step: 27
[2025-12-23 14:58:44,791] m-LoRA: Adapter lora_gsm8k_53 loss: 0.7086940407752991
[2025-12-23 14:58:44,869] m-LoRA: Adapter lora_gsm8k_51 loss: 0.5171062350273132
[2025-12-23 14:58:45,085] m-LoRA: Adapter lora_gsm8k_56 loss: 1.0237141847610474
[2025-12-23 14:58:45,277] m-LoRA: Adapter lora_gsm8k_54 loss: 0.5108733177185059
[2025-12-23 14:58:45,668] m-LoRA: Adapter lora_gsm8k_55 loss: 0.5987147688865662
[2025-12-23 14:58:45,672] m-LoRA: Adapter lora_gsm8k_53 epoch: 1/1 iteration: 112/128 step: 29
[2025-12-23 14:58:45,874] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 72/128 step: 37
[2025-12-23 14:58:45,981] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 22/128 step: 12
[2025-12-23 14:58:46,059] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 54/128 step: 28
[2025-12-23 14:58:46,700] m-LoRA: Adapter lora_gsm8k_55 epoch: 1/1 iteration: 108/128 step: 28
[2025-12-23 14:58:46,752] m-LoRA: Adapter lora_gsm8k_53 loss: 0.6775016188621521
[2025-12-23 14:58:46,801] m-LoRA: Adapter lora_gsm8k_51 loss: 0.5373256206512451
[2025-12-23 14:58:47,038] m-LoRA: Adapter lora_gsm8k_56 loss: 0.8747774362564087
[2025-12-23 14:58:47,217] m-LoRA: Adapter lora_gsm8k_54 loss: 0.7253909707069397
[2025-12-23 14:58:47,552] m-LoRA: Adapter lora_gsm8k_55 loss: 0.42047518491744995
[2025-12-23 14:58:47,736] m-LoRA: Adapter lora_gsm8k_53 epoch: 1/1 iteration: 116/128 step: 30
[2025-12-23 14:58:47,844] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 74/128 step: 38
[2025-12-23 14:58:47,922] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 24/128 step: 13
[2025-12-23 14:58:48,036] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 56/128 step: 29
[2025-12-23 14:58:48,633] m-LoRA: Adapter lora_gsm8k_55 epoch: 1/1 iteration: 112/128 step: 29
[2025-12-23 14:58:48,686] m-LoRA: Adapter lora_gsm8k_53 loss: 0.5828092694282532
[2025-12-23 14:58:48,909] m-LoRA: Adapter lora_gsm8k_51 loss: 0.5388756990432739
[2025-12-23 14:58:49,088] m-LoRA: Adapter lora_gsm8k_56 loss: 0.7851942181587219
[2025-12-23 14:58:49,295] m-LoRA: Adapter lora_gsm8k_54 loss: 0.6526540517807007
[2025-12-23 14:58:49,732] m-LoRA: Adapter lora_gsm8k_55 loss: 0.5381419658660889
[2025-12-23 14:58:49,736] m-LoRA: Adapter lora_gsm8k_53 epoch: 1/1 iteration: 120/128 step: 31
[2025-12-23 14:58:49,903] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 76/128 step: 39
[2025-12-23 14:58:49,999] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 26/128 step: 14
[2025-12-23 14:58:50,105] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 58/128 step: 30
[2025-12-23 14:58:50,727] m-LoRA: Adapter lora_gsm8k_55 epoch: 1/1 iteration: 116/128 step: 30
[2025-12-23 14:58:50,792] m-LoRA: Adapter lora_gsm8k_53 loss: 0.5716399550437927
[2025-12-23 14:58:50,798] m-LoRA: Adapter lora_gsm8k_51 loss: 0.47585800290107727
[2025-12-23 14:58:50,981] m-LoRA: Adapter lora_gsm8k_56 loss: 0.9180071353912354
[2025-12-23 14:58:51,132] m-LoRA: Adapter lora_gsm8k_54 loss: 0.6929255127906799
[2025-12-23 14:58:51,450] m-LoRA: Adapter lora_gsm8k_55 loss: 0.5965044498443604
[2025-12-23 14:58:51,672] m-LoRA: Adapter lora_gsm8k_53 epoch: 1/1 iteration: 124/128 step: 32
[2025-12-23 14:58:51,774] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 78/128 step: 40
[2025-12-23 14:58:51,849] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 28/128 step: 15
[2025-12-23 14:58:51,924] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 60/128 step: 31
[2025-12-23 14:58:52,510] m-LoRA: Adapter lora_gsm8k_55 epoch: 1/1 iteration: 120/128 step: 31
[2025-12-23 14:58:52,571] m-LoRA: Adapter lora_gsm8k_53 loss: 0.5146950483322144
[2025-12-23 14:58:52,576] m-LoRA: Adapter lora_gsm8k_51 loss: 0.7118767499923706
[2025-12-23 14:58:52,734] m-LoRA: Adapter lora_gsm8k_56 loss: 0.6452000141143799
[2025-12-23 14:58:52,924] m-LoRA: Adapter lora_gsm8k_54 loss: 0.5177977085113525
[2025-12-23 14:58:53,204] m-LoRA: Adapter lora_gsm8k_55 loss: 0.5485293865203857
[2025-12-23 14:58:53,435] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_53']
[2025-12-23 14:58:53,625] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_57']
[2025-12-23 14:58:53,708] m-LoRA: Adapter lora_gsm8k_57 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:58:53,860] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 80/128 step: 41
[2025-12-23 14:58:53,951] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 30/128 step: 16
[2025-12-23 14:58:54,048] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 62/128 step: 32
[2025-12-23 14:58:54,128] m-LoRA: Adapter lora_gsm8k_55 epoch: 1/1 iteration: 124/128 step: 32
[2025-12-23 14:58:55,811] m-LoRA: Adapter lora_gsm8k_57 loss: 2.527010917663574
[2025-12-23 14:58:55,829] m-LoRA: Adapter lora_gsm8k_51 loss: 0.5340942144393921
[2025-12-23 14:58:56,069] m-LoRA: Adapter lora_gsm8k_56 loss: 1.0105582475662231
[2025-12-23 14:58:56,214] m-LoRA: Adapter lora_gsm8k_54 loss: 0.6997339725494385
[2025-12-23 14:58:56,559] m-LoRA: Adapter lora_gsm8k_55 loss: 0.5700849890708923
[2025-12-23 14:58:58,382] m-LoRA: Adapter lora_gsm8k_57 epoch: 1/1 iteration: 16/128 step: 2
[2025-12-23 14:58:58,589] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 82/128 step: 42
[2025-12-23 14:58:58,688] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 32/128 step: 17
[2025-12-23 14:58:58,787] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 64/128 step: 33
[2025-12-23 14:58:58,880] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_55']
[2025-12-23 14:58:59,484] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_58']
[2025-12-23 14:58:59,566] m-LoRA: Adapter lora_gsm8k_58 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:59:01,945] m-LoRA: Adapter lora_gsm8k_57 loss: 1.1926639080047607
[2025-12-23 14:59:01,965] m-LoRA: Adapter lora_gsm8k_51 loss: 0.5692631602287292
[2025-12-23 14:59:01,995] m-LoRA: Adapter lora_gsm8k_56 loss: 0.6630595922470093
[2025-12-23 14:59:02,255] m-LoRA: Adapter lora_gsm8k_54 loss: 0.6520320773124695
[2025-12-23 14:59:02,595] m-LoRA: Adapter lora_gsm8k_58 loss: 2.1239402294158936
[2025-12-23 14:59:05,039] m-LoRA: Adapter lora_gsm8k_57 epoch: 1/1 iteration: 32/128 step: 3
[2025-12-23 14:59:05,271] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 84/128 step: 43
[2025-12-23 14:59:05,348] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 34/128 step: 18
[2025-12-23 14:59:05,426] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 66/128 step: 34
[2025-12-23 14:59:05,505] m-LoRA: Adapter lora_gsm8k_58 epoch: 1/1 iteration: 4/128 step: 2
[2025-12-23 14:59:07,237] m-LoRA: Adapter lora_gsm8k_57 loss: 1.4406986236572266
[2025-12-23 14:59:07,252] m-LoRA: Adapter lora_gsm8k_51 loss: 0.5381522178649902
[2025-12-23 14:59:07,603] m-LoRA: Adapter lora_gsm8k_56 loss: 0.7265354990959167
[2025-12-23 14:59:07,749] m-LoRA: Adapter lora_gsm8k_54 loss: 0.512801468372345
[2025-12-23 14:59:07,991] m-LoRA: Adapter lora_gsm8k_58 loss: 2.7252984046936035
[2025-12-23 14:59:09,781] m-LoRA: Adapter lora_gsm8k_57 epoch: 1/1 iteration: 48/128 step: 4
[2025-12-23 14:59:09,979] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 86/128 step: 44
[2025-12-23 14:59:10,039] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 36/128 step: 19
[2025-12-23 14:59:10,117] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 68/128 step: 35
[2025-12-23 14:59:10,190] m-LoRA: Adapter lora_gsm8k_58 epoch: 1/1 iteration: 8/128 step: 3
[2025-12-23 14:59:12,167] m-LoRA: Adapter lora_gsm8k_57 loss: 1.2374459505081177
[2025-12-23 14:59:12,185] m-LoRA: Adapter lora_gsm8k_51 loss: 0.7462121248245239
[2025-12-23 14:59:12,446] m-LoRA: Adapter lora_gsm8k_56 loss: 0.5635388493537903
[2025-12-23 14:59:12,536] m-LoRA: Adapter lora_gsm8k_54 loss: 0.4749829173088074
[2025-12-23 14:59:12,838] m-LoRA: Adapter lora_gsm8k_58 loss: 2.4368975162506104
[2025-12-23 14:59:14,930] m-LoRA: Adapter lora_gsm8k_57 epoch: 1/1 iteration: 64/128 step: 5
[2025-12-23 14:59:15,195] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 88/128 step: 45
[2025-12-23 14:59:15,290] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 38/128 step: 20
[2025-12-23 14:59:15,379] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 70/128 step: 36
[2025-12-23 14:59:15,474] m-LoRA: Adapter lora_gsm8k_58 epoch: 1/1 iteration: 12/128 step: 4
[2025-12-23 14:59:17,851] m-LoRA: Adapter lora_gsm8k_57 loss: 0.9955979585647583
[2025-12-23 14:59:17,872] m-LoRA: Adapter lora_gsm8k_51 loss: 0.7471577525138855
[2025-12-23 14:59:17,903] m-LoRA: Adapter lora_gsm8k_56 loss: 0.45818933844566345
[2025-12-23 14:59:18,191] m-LoRA: Adapter lora_gsm8k_54 loss: 0.718034029006958
[2025-12-23 14:59:18,387] m-LoRA: Adapter lora_gsm8k_58 loss: 2.4171431064605713
[2025-12-23 14:59:21,229] m-LoRA: Adapter lora_gsm8k_57 epoch: 1/1 iteration: 80/128 step: 6
[2025-12-23 14:59:21,483] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 90/128 step: 46
[2025-12-23 14:59:21,593] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 40/128 step: 21
[2025-12-23 14:59:21,685] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 72/128 step: 37
[2025-12-23 14:59:21,761] m-LoRA: Adapter lora_gsm8k_58 epoch: 1/1 iteration: 16/128 step: 5
[2025-12-23 14:59:23,562] m-LoRA: Adapter lora_gsm8k_57 loss: 1.2684080600738525
[2025-12-23 14:59:23,588] m-LoRA: Adapter lora_gsm8k_51 loss: 0.7033067345619202
[2025-12-23 14:59:23,622] m-LoRA: Adapter lora_gsm8k_56 loss: 0.8482598066329956
[2025-12-23 14:59:23,807] m-LoRA: Adapter lora_gsm8k_54 loss: 0.6702212691307068
[2025-12-23 14:59:24,284] m-LoRA: Adapter lora_gsm8k_58 loss: 1.9485461711883545
[2025-12-23 14:59:26,026] m-LoRA: Adapter lora_gsm8k_57 epoch: 1/1 iteration: 96/128 step: 7
[2025-12-23 14:59:26,254] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 92/128 step: 47
[2025-12-23 14:59:26,352] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 42/128 step: 22
[2025-12-23 14:59:26,429] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 74/128 step: 38
[2025-12-23 14:59:26,508] m-LoRA: Adapter lora_gsm8k_58 epoch: 1/1 iteration: 20/128 step: 6
[2025-12-23 14:59:28,332] m-LoRA: Adapter lora_gsm8k_57 loss: 1.1723557710647583
[2025-12-23 14:59:28,352] m-LoRA: Adapter lora_gsm8k_51 loss: 0.5066511034965515
[2025-12-23 14:59:28,597] m-LoRA: Adapter lora_gsm8k_56 loss: 0.6600823402404785
[2025-12-23 14:59:28,761] m-LoRA: Adapter lora_gsm8k_54 loss: 0.5639343857765198
[2025-12-23 14:59:28,998] m-LoRA: Adapter lora_gsm8k_58 loss: 2.126803159713745
[2025-12-23 14:59:30,933] m-LoRA: Adapter lora_gsm8k_57 epoch: 1/1 iteration: 112/128 step: 8
[2025-12-23 14:59:31,139] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 94/128 step: 48
[2025-12-23 14:59:31,215] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 44/128 step: 23
[2025-12-23 14:59:31,285] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 76/128 step: 39
[2025-12-23 14:59:31,356] m-LoRA: Adapter lora_gsm8k_58 epoch: 1/1 iteration: 24/128 step: 7
[2025-12-23 14:59:33,371] m-LoRA: Adapter lora_gsm8k_57 loss: 0.9958555102348328
[2025-12-23 14:59:33,394] m-LoRA: Adapter lora_gsm8k_51 loss: 0.8579577803611755
[2025-12-23 14:59:33,657] m-LoRA: Adapter lora_gsm8k_56 loss: 0.5227219462394714
[2025-12-23 14:59:33,762] m-LoRA: Adapter lora_gsm8k_54 loss: 0.6931089758872986
[2025-12-23 14:59:34,053] m-LoRA: Adapter lora_gsm8k_58 loss: 1.889131784439087
[2025-12-23 14:59:36,172] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_57']
[2025-12-23 14:59:36,544] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_59']
[2025-12-23 14:59:36,578] m-LoRA: Adapter lora_gsm8k_59 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 14:59:36,712] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 96/128 step: 49
[2025-12-23 14:59:36,768] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 46/128 step: 24
[2025-12-23 14:59:36,837] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 78/128 step: 40
[2025-12-23 14:59:36,907] m-LoRA: Adapter lora_gsm8k_58 epoch: 1/1 iteration: 28/128 step: 8
[2025-12-23 14:59:37,944] m-LoRA: Adapter lora_gsm8k_59 loss: 2.342026948928833
[2025-12-23 14:59:37,964] m-LoRA: Adapter lora_gsm8k_51 loss: 0.4470125734806061
[2025-12-23 14:59:38,121] m-LoRA: Adapter lora_gsm8k_56 loss: 0.6844379901885986
[2025-12-23 14:59:38,286] m-LoRA: Adapter lora_gsm8k_54 loss: 0.5578422546386719
[2025-12-23 14:59:38,531] m-LoRA: Adapter lora_gsm8k_58 loss: 2.0410726070404053
[2025-12-23 14:59:39,639] m-LoRA: Adapter lora_gsm8k_59 epoch: 1/1 iteration: 8/128 step: 2
[2025-12-23 14:59:39,773] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 98/128 step: 50
[2025-12-23 14:59:39,857] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 48/128 step: 25
[2025-12-23 14:59:39,929] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 80/128 step: 41
[2025-12-23 14:59:40,009] m-LoRA: Adapter lora_gsm8k_58 epoch: 1/1 iteration: 32/128 step: 9
[2025-12-23 14:59:40,801] m-LoRA: Adapter lora_gsm8k_59 loss: 1.8362507820129395
[2025-12-23 14:59:40,816] m-LoRA: Adapter lora_gsm8k_51 loss: 0.6173370480537415
[2025-12-23 14:59:41,030] m-LoRA: Adapter lora_gsm8k_56 loss: 0.6664154529571533
[2025-12-23 14:59:41,146] m-LoRA: Adapter lora_gsm8k_54 loss: 0.6146233081817627
[2025-12-23 14:59:41,455] m-LoRA: Adapter lora_gsm8k_58 loss: 1.4968737363815308
[2025-12-23 14:59:42,146] m-LoRA: Adapter lora_gsm8k_59 epoch: 1/1 iteration: 16/128 step: 3
[2025-12-23 14:59:42,265] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 100/128 step: 51
[2025-12-23 14:59:42,365] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 50/128 step: 26
[2025-12-23 14:59:42,459] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 82/128 step: 42
[2025-12-23 14:59:42,630] m-LoRA: Adapter lora_gsm8k_58 epoch: 1/1 iteration: 36/128 step: 10
[2025-12-23 14:59:43,264] m-LoRA: Adapter lora_gsm8k_59 loss: 1.7764464616775513
[2025-12-23 14:59:43,308] m-LoRA: Adapter lora_gsm8k_51 loss: 0.6296417713165283
[2025-12-23 14:59:43,491] m-LoRA: Adapter lora_gsm8k_56 loss: 0.6210044026374817
[2025-12-23 14:59:43,613] m-LoRA: Adapter lora_gsm8k_54 loss: 0.7157185077667236
[2025-12-23 14:59:43,934] m-LoRA: Adapter lora_gsm8k_58 loss: 1.6439151763916016
[2025-12-23 14:59:44,534] m-LoRA: Adapter lora_gsm8k_59 epoch: 1/1 iteration: 24/128 step: 4
[2025-12-23 14:59:44,641] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 102/128 step: 52
[2025-12-23 14:59:44,734] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 52/128 step: 27
[2025-12-23 14:59:44,809] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 84/128 step: 43
[2025-12-23 14:59:44,930] m-LoRA: Adapter lora_gsm8k_58 epoch: 1/1 iteration: 40/128 step: 11
[2025-12-23 14:59:45,677] m-LoRA: Adapter lora_gsm8k_59 loss: 1.5333514213562012
[2025-12-23 14:59:45,693] m-LoRA: Adapter lora_gsm8k_51 loss: 0.6555043458938599
[2025-12-23 14:59:45,907] m-LoRA: Adapter lora_gsm8k_56 loss: 0.678905189037323
[2025-12-23 14:59:46,009] m-LoRA: Adapter lora_gsm8k_54 loss: 0.5772707462310791
[2025-12-23 14:59:46,330] m-LoRA: Adapter lora_gsm8k_58 loss: 1.5135501623153687
[2025-12-23 14:59:46,994] m-LoRA: Adapter lora_gsm8k_59 epoch: 1/1 iteration: 32/128 step: 5
[2025-12-23 14:59:47,103] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 104/128 step: 53
[2025-12-23 14:59:47,180] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 54/128 step: 28
[2025-12-23 14:59:47,267] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 86/128 step: 44
[2025-12-23 14:59:47,439] m-LoRA: Adapter lora_gsm8k_58 epoch: 1/1 iteration: 44/128 step: 12
[2025-12-23 14:59:48,183] m-LoRA: Adapter lora_gsm8k_59 loss: 1.2746074199676514
[2025-12-23 14:59:48,219] m-LoRA: Adapter lora_gsm8k_51 loss: 0.5816716551780701
[2025-12-23 14:59:48,463] m-LoRA: Adapter lora_gsm8k_56 loss: 0.5608308911323547
[2025-12-23 14:59:48,576] m-LoRA: Adapter lora_gsm8k_54 loss: 0.46642690896987915
[2025-12-23 14:59:48,899] m-LoRA: Adapter lora_gsm8k_58 loss: 1.3015470504760742
[2025-12-23 14:59:49,643] m-LoRA: Adapter lora_gsm8k_59 epoch: 1/1 iteration: 40/128 step: 6
[2025-12-23 14:59:49,736] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 106/128 step: 54
[2025-12-23 14:59:49,814] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 56/128 step: 29
[2025-12-23 14:59:49,886] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 88/128 step: 45
[2025-12-23 14:59:50,062] m-LoRA: Adapter lora_gsm8k_58 epoch: 1/1 iteration: 48/128 step: 13
[2025-12-23 14:59:50,840] m-LoRA: Adapter lora_gsm8k_59 loss: 1.275719404220581
[2025-12-23 14:59:50,862] m-LoRA: Adapter lora_gsm8k_51 loss: 0.5779724717140198
[2025-12-23 14:59:51,102] m-LoRA: Adapter lora_gsm8k_56 loss: 0.4924580156803131
[2025-12-23 14:59:51,249] m-LoRA: Adapter lora_gsm8k_54 loss: 0.5174434781074524
[2025-12-23 14:59:51,504] m-LoRA: Adapter lora_gsm8k_58 loss: 1.8631128072738647
[2025-12-23 14:59:52,289] m-LoRA: Adapter lora_gsm8k_59 epoch: 1/1 iteration: 48/128 step: 7
[2025-12-23 14:59:52,404] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 108/128 step: 55
[2025-12-23 14:59:52,493] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 58/128 step: 30
[2025-12-23 14:59:52,567] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 90/128 step: 46
[2025-12-23 14:59:52,670] m-LoRA: Adapter lora_gsm8k_58 epoch: 1/1 iteration: 52/128 step: 14
[2025-12-23 14:59:53,621] m-LoRA: Adapter lora_gsm8k_59 loss: 1.194907307624817
[2025-12-23 14:59:53,639] m-LoRA: Adapter lora_gsm8k_51 loss: 0.7324875593185425
[2025-12-23 14:59:53,879] m-LoRA: Adapter lora_gsm8k_56 loss: 0.5241581797599792
[2025-12-23 14:59:53,976] m-LoRA: Adapter lora_gsm8k_54 loss: 0.49222877621650696
[2025-12-23 14:59:54,207] m-LoRA: Adapter lora_gsm8k_58 loss: 2.141183614730835
[2025-12-23 14:59:55,169] m-LoRA: Adapter lora_gsm8k_59 epoch: 1/1 iteration: 56/128 step: 8
[2025-12-23 14:59:55,266] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 110/128 step: 56
[2025-12-23 14:59:55,355] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 60/128 step: 31
[2025-12-23 14:59:55,438] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 92/128 step: 47
[2025-12-23 14:59:55,512] m-LoRA: Adapter lora_gsm8k_58 epoch: 1/1 iteration: 56/128 step: 15
[2025-12-23 14:59:56,340] m-LoRA: Adapter lora_gsm8k_59 loss: 1.3624540567398071
[2025-12-23 14:59:56,360] m-LoRA: Adapter lora_gsm8k_51 loss: 0.4921729266643524
[2025-12-23 14:59:56,538] m-LoRA: Adapter lora_gsm8k_56 loss: 0.651108980178833
[2025-12-23 14:59:56,694] m-LoRA: Adapter lora_gsm8k_54 loss: 0.568336009979248
[2025-12-23 14:59:57,042] m-LoRA: Adapter lora_gsm8k_58 loss: 1.487364649772644
[2025-12-23 14:59:57,759] m-LoRA: Adapter lora_gsm8k_59 epoch: 1/1 iteration: 64/128 step: 9
[2025-12-23 14:59:57,872] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 112/128 step: 57
[2025-12-23 14:59:57,950] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 62/128 step: 32
[2025-12-23 14:59:58,028] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 94/128 step: 48
[2025-12-23 14:59:58,179] m-LoRA: Adapter lora_gsm8k_58 epoch: 1/1 iteration: 60/128 step: 16
[2025-12-23 14:59:59,000] m-LoRA: Adapter lora_gsm8k_59 loss: 1.2297825813293457
[2025-12-23 14:59:59,017] m-LoRA: Adapter lora_gsm8k_51 loss: 0.7111170291900635
[2025-12-23 14:59:59,263] m-LoRA: Adapter lora_gsm8k_56 loss: 0.6444613933563232
[2025-12-23 14:59:59,420] m-LoRA: Adapter lora_gsm8k_54 loss: 0.5340966582298279
[2025-12-23 15:00:00,027] m-LoRA: Adapter lora_gsm8k_58 loss: 1.517233967781067
[2025-12-23 15:00:00,552] m-LoRA: Adapter lora_gsm8k_59 epoch: 1/1 iteration: 72/128 step: 10
[2025-12-23 15:00:00,661] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 114/128 step: 58
[2025-12-23 15:00:00,737] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 64/128 step: 33
[2025-12-23 15:00:00,823] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 96/128 step: 49
[2025-12-23 15:00:01,022] m-LoRA: Adapter lora_gsm8k_58 epoch: 1/1 iteration: 64/128 step: 17
[2025-12-23 15:00:01,745] m-LoRA: Adapter lora_gsm8k_59 loss: 1.385932445526123
[2025-12-23 15:00:01,757] m-LoRA: Adapter lora_gsm8k_51 loss: 0.581844687461853
[2025-12-23 15:00:01,978] m-LoRA: Adapter lora_gsm8k_56 loss: 0.5268647074699402
[2025-12-23 15:00:02,126] m-LoRA: Adapter lora_gsm8k_54 loss: 0.5871526598930359
[2025-12-23 15:00:02,539] m-LoRA: Adapter lora_gsm8k_58 loss: 1.3384454250335693
[2025-12-23 15:00:03,095] m-LoRA: Adapter lora_gsm8k_59 epoch: 1/1 iteration: 80/128 step: 11
[2025-12-23 15:00:03,213] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 116/128 step: 59
[2025-12-23 15:00:03,312] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 66/128 step: 34
[2025-12-23 15:00:03,405] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 98/128 step: 50
[2025-12-23 15:00:04,269] m-LoRA: Adapter lora_gsm8k_58 epoch: 1/1 iteration: 68/128 step: 18
[2025-12-23 15:00:04,326] m-LoRA: Adapter lora_gsm8k_59 loss: 1.227882742881775
[2025-12-23 15:00:04,461] m-LoRA: Adapter lora_gsm8k_51 loss: 0.7071153521537781
[2025-12-23 15:00:04,656] m-LoRA: Adapter lora_gsm8k_56 loss: 0.5714895725250244
[2025-12-23 15:00:04,758] m-LoRA: Adapter lora_gsm8k_54 loss: 0.9462087750434875
[2025-12-23 15:00:05,053] m-LoRA: Adapter lora_gsm8k_58 loss: 1.589821219444275
[2025-12-23 15:00:05,720] m-LoRA: Adapter lora_gsm8k_59 epoch: 1/1 iteration: 88/128 step: 12
[2025-12-23 15:00:05,834] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 118/128 step: 60
[2025-12-23 15:00:05,910] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 68/128 step: 35
[2025-12-23 15:00:05,985] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 100/128 step: 51
[2025-12-23 15:00:06,159] m-LoRA: Adapter lora_gsm8k_58 epoch: 1/1 iteration: 72/128 step: 19
[2025-12-23 15:00:07,063] m-LoRA: Adapter lora_gsm8k_59 loss: 1.1139309406280518
[2025-12-23 15:00:07,083] m-LoRA: Adapter lora_gsm8k_51 loss: 0.5872927904129028
[2025-12-23 15:00:07,338] m-LoRA: Adapter lora_gsm8k_56 loss: 0.5007302165031433
[2025-12-23 15:00:07,465] m-LoRA: Adapter lora_gsm8k_54 loss: 0.7301095128059387
[2025-12-23 15:00:07,767] m-LoRA: Adapter lora_gsm8k_58 loss: 1.365993857383728
[2025-12-23 15:00:08,717] m-LoRA: Adapter lora_gsm8k_59 epoch: 1/1 iteration: 96/128 step: 13
[2025-12-23 15:00:08,866] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 120/128 step: 61
[2025-12-23 15:00:08,963] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 70/128 step: 36
[2025-12-23 15:00:09,049] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 102/128 step: 52
[2025-12-23 15:00:09,170] m-LoRA: Adapter lora_gsm8k_58 epoch: 1/1 iteration: 76/128 step: 20
[2025-12-23 15:00:10,294] m-LoRA: Adapter lora_gsm8k_59 loss: 0.8544203639030457
[2025-12-23 15:00:10,312] m-LoRA: Adapter lora_gsm8k_51 loss: 0.6290490031242371
[2025-12-23 15:00:10,488] m-LoRA: Adapter lora_gsm8k_56 loss: 0.6878935098648071
[2025-12-23 15:00:10,784] m-LoRA: Adapter lora_gsm8k_54 loss: 0.7063418030738831
[2025-12-23 15:00:11,136] m-LoRA: Adapter lora_gsm8k_58 loss: 1.3321634531021118
[2025-12-23 15:00:12,183] m-LoRA: Adapter lora_gsm8k_59 epoch: 1/1 iteration: 104/128 step: 14
[2025-12-23 15:00:12,311] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 122/128 step: 62
[2025-12-23 15:00:12,385] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 72/128 step: 37
[2025-12-23 15:00:12,463] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 104/128 step: 53
[2025-12-23 15:00:12,581] m-LoRA: Adapter lora_gsm8k_58 epoch: 1/1 iteration: 80/128 step: 21
[2025-12-23 15:00:13,386] m-LoRA: Adapter lora_gsm8k_59 loss: 1.1517363786697388
[2025-12-23 15:00:13,404] m-LoRA: Adapter lora_gsm8k_51 loss: 0.4587165415287018
[2025-12-23 15:00:13,621] m-LoRA: Adapter lora_gsm8k_56 loss: 0.48912468552589417
[2025-12-23 15:00:13,809] m-LoRA: Adapter lora_gsm8k_54 loss: 0.6309065818786621
[2025-12-23 15:00:14,119] m-LoRA: Adapter lora_gsm8k_58 loss: 1.4470359086990356
[2025-12-23 15:00:14,794] m-LoRA: Adapter lora_gsm8k_59 epoch: 1/1 iteration: 112/128 step: 15
[2025-12-23 15:00:14,900] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 124/128 step: 63
[2025-12-23 15:00:14,973] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 74/128 step: 38
[2025-12-23 15:00:15,027] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 106/128 step: 54
[2025-12-23 15:00:15,234] m-LoRA: Adapter lora_gsm8k_58 epoch: 1/1 iteration: 84/128 step: 22
[2025-12-23 15:00:16,142] m-LoRA: Adapter lora_gsm8k_59 loss: 1.0505739450454712
[2025-12-23 15:00:16,159] m-LoRA: Adapter lora_gsm8k_51 loss: 0.7464253902435303
[2025-12-23 15:00:16,309] m-LoRA: Adapter lora_gsm8k_56 loss: 0.5111415386199951
[2025-12-23 15:00:16,523] m-LoRA: Adapter lora_gsm8k_54 loss: 0.6689493060112
[2025-12-23 15:00:16,864] m-LoRA: Adapter lora_gsm8k_58 loss: 1.2991124391555786
[2025-12-23 15:00:17,707] m-LoRA: Adapter lora_gsm8k_59 epoch: 1/1 iteration: 120/128 step: 16
[2025-12-23 15:00:17,804] m-LoRA: Adapter lora_gsm8k_51 epoch: 1/1 iteration: 126/128 step: 64
[2025-12-23 15:00:17,881] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 76/128 step: 39
[2025-12-23 15:00:17,951] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 108/128 step: 55
[2025-12-23 15:00:18,150] m-LoRA: Adapter lora_gsm8k_58 epoch: 1/1 iteration: 88/128 step: 23
[2025-12-23 15:00:18,893] m-LoRA: Adapter lora_gsm8k_59 loss: 1.257650375366211
[2025-12-23 15:00:18,912] m-LoRA: Adapter lora_gsm8k_51 loss: 0.5990322828292847
[2025-12-23 15:00:19,156] m-LoRA: Adapter lora_gsm8k_56 loss: 0.5755925178527832
[2025-12-23 15:00:19,295] m-LoRA: Adapter lora_gsm8k_54 loss: 0.5802831053733826
[2025-12-23 15:00:19,619] m-LoRA: Adapter lora_gsm8k_58 loss: 1.3330661058425903
[2025-12-23 15:00:20,275] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_59']
[2025-12-23 15:00:20,463] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_60']
[2025-12-23 15:00:20,625] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 15:00:20,707] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_51']
[2025-12-23 15:00:20,950] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_61']
[2025-12-23 15:00:20,983] m-LoRA: Adapter lora_gsm8k_61 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 15:00:21,042] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 78/128 step: 40
[2025-12-23 15:00:21,095] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 110/128 step: 56
[2025-12-23 15:00:21,250] m-LoRA: Adapter lora_gsm8k_58 epoch: 1/1 iteration: 92/128 step: 24
[2025-12-23 15:00:21,513] m-LoRA: Adapter lora_gsm8k_60 loss: 2.4052469730377197
[2025-12-23 15:00:21,833] m-LoRA: Adapter lora_gsm8k_61 loss: 2.665879249572754
[2025-12-23 15:00:22,016] m-LoRA: Adapter lora_gsm8k_56 loss: 0.5256856679916382
[2025-12-23 15:00:22,169] m-LoRA: Adapter lora_gsm8k_54 loss: 0.6349249482154846
[2025-12-23 15:00:22,172] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 2/128 step: 2
[2025-12-23 15:00:22,501] m-LoRA: Adapter lora_gsm8k_58 loss: 1.169353723526001
[2025-12-23 15:00:22,632] m-LoRA: Adapter lora_gsm8k_61 epoch: 1/1 iteration: 4/128 step: 2
[2025-12-23 15:00:22,767] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 80/128 step: 41
[2025-12-23 15:00:22,861] m-LoRA: Adapter lora_gsm8k_60 loss: 1.5637396574020386
[2025-12-23 15:00:22,864] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 112/128 step: 57
[2025-12-23 15:00:23,730] m-LoRA: Adapter lora_gsm8k_58 epoch: 1/1 iteration: 96/128 step: 25
[2025-12-23 15:00:23,862] m-LoRA: Adapter lora_gsm8k_61 loss: 1.8066354990005493
[2025-12-23 15:00:23,867] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 4/128 step: 3
[2025-12-23 15:00:23,911] m-LoRA: Adapter lora_gsm8k_56 loss: 0.633294403553009
[2025-12-23 15:00:24,048] m-LoRA: Adapter lora_gsm8k_54 loss: 0.7459508776664734
[2025-12-23 15:00:24,415] m-LoRA: Adapter lora_gsm8k_58 loss: 1.5129340887069702
[2025-12-23 15:00:24,574] m-LoRA: Adapter lora_gsm8k_60 loss: 2.111915349960327
[2025-12-23 15:00:24,730] m-LoRA: Adapter lora_gsm8k_61 epoch: 1/1 iteration: 8/128 step: 3
[2025-12-23 15:00:24,817] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 82/128 step: 42
[2025-12-23 15:00:24,921] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 114/128 step: 58
[2025-12-23 15:00:25,515] m-LoRA: Adapter lora_gsm8k_58 epoch: 1/1 iteration: 100/128 step: 26
[2025-12-23 15:00:25,561] m-LoRA: Adapter lora_gsm8k_61 loss: 1.583125114440918
[2025-12-23 15:00:25,719] m-LoRA: Adapter lora_gsm8k_56 loss: 0.41262564063072205
[2025-12-23 15:00:25,721] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 6/128 step: 4
[2025-12-23 15:00:25,783] m-LoRA: Adapter lora_gsm8k_54 loss: 0.6005439758300781
[2025-12-23 15:00:26,249] m-LoRA: Adapter lora_gsm8k_58 loss: 1.342405080795288
[2025-12-23 15:00:26,274] m-LoRA: Adapter lora_gsm8k_60 loss: 1.7843493223190308
[2025-12-23 15:00:26,479] m-LoRA: Adapter lora_gsm8k_61 epoch: 1/1 iteration: 12/128 step: 4
[2025-12-23 15:00:26,578] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 84/128 step: 43
[2025-12-23 15:00:26,654] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 116/128 step: 59
[2025-12-23 15:00:27,259] m-LoRA: Adapter lora_gsm8k_58 epoch: 1/1 iteration: 104/128 step: 27
[2025-12-23 15:00:27,307] m-LoRA: Adapter lora_gsm8k_61 loss: 1.4889930486679077
[2025-12-23 15:00:27,450] m-LoRA: Adapter lora_gsm8k_56 loss: 0.4001976251602173
[2025-12-23 15:00:27,451] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 8/128 step: 5
[2025-12-23 15:00:27,593] m-LoRA: Adapter lora_gsm8k_54 loss: 0.4725676476955414
[2025-12-23 15:00:28,151] m-LoRA: Adapter lora_gsm8k_58 loss: 0.9410234093666077
[2025-12-23 15:00:28,426] m-LoRA: Adapter lora_gsm8k_60 loss: 1.4955352544784546
[2025-12-23 15:00:28,428] m-LoRA: Adapter lora_gsm8k_61 epoch: 1/1 iteration: 16/128 step: 5
[2025-12-23 15:00:28,539] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 86/128 step: 44
[2025-12-23 15:00:28,631] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 118/128 step: 60
[2025-12-23 15:00:29,251] m-LoRA: Adapter lora_gsm8k_58 epoch: 1/1 iteration: 108/128 step: 28
[2025-12-23 15:00:29,312] m-LoRA: Adapter lora_gsm8k_61 loss: 1.5407105684280396
[2025-12-23 15:00:29,451] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 10/128 step: 6
[2025-12-23 15:00:29,492] m-LoRA: Adapter lora_gsm8k_56 loss: 0.5231207013130188
[2025-12-23 15:00:29,606] m-LoRA: Adapter lora_gsm8k_54 loss: 0.7480179667472839
[2025-12-23 15:00:29,932] m-LoRA: Adapter lora_gsm8k_58 loss: 1.5911139249801636
[2025-12-23 15:00:30,129] m-LoRA: Adapter lora_gsm8k_60 loss: 1.4371604919433594
[2025-12-23 15:00:30,131] m-LoRA: Adapter lora_gsm8k_61 epoch: 1/1 iteration: 20/128 step: 6
[2025-12-23 15:00:30,242] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 88/128 step: 45
[2025-12-23 15:00:30,328] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 120/128 step: 61
[2025-12-23 15:00:30,821] m-LoRA: Adapter lora_gsm8k_58 epoch: 1/1 iteration: 112/128 step: 29
[2025-12-23 15:00:30,864] m-LoRA: Adapter lora_gsm8k_61 loss: 1.7414379119873047
[2025-12-23 15:00:31,020] m-LoRA: Adapter lora_gsm8k_56 loss: 0.4927433729171753
[2025-12-23 15:00:31,023] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 12/128 step: 7
[2025-12-23 15:00:31,121] m-LoRA: Adapter lora_gsm8k_54 loss: 0.5057224035263062
[2025-12-23 15:00:31,530] m-LoRA: Adapter lora_gsm8k_58 loss: 1.3935126066207886
[2025-12-23 15:00:31,757] m-LoRA: Adapter lora_gsm8k_60 loss: 1.357435703277588
[2025-12-23 15:00:31,759] m-LoRA: Adapter lora_gsm8k_61 epoch: 1/1 iteration: 24/128 step: 7
[2025-12-23 15:00:31,884] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 90/128 step: 46
[2025-12-23 15:00:31,976] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 122/128 step: 62
[2025-12-23 15:00:32,578] m-LoRA: Adapter lora_gsm8k_58 epoch: 1/1 iteration: 116/128 step: 30
[2025-12-23 15:00:32,624] m-LoRA: Adapter lora_gsm8k_61 loss: 1.263208031654358
[2025-12-23 15:00:32,807] m-LoRA: Adapter lora_gsm8k_56 loss: 0.7605236768722534
[2025-12-23 15:00:32,810] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 14/128 step: 8
[2025-12-23 15:00:32,938] m-LoRA: Adapter lora_gsm8k_54 loss: 0.4673374891281128
[2025-12-23 15:00:33,319] m-LoRA: Adapter lora_gsm8k_58 loss: 1.1810764074325562
[2025-12-23 15:00:33,706] m-LoRA: Adapter lora_gsm8k_60 loss: 1.1694334745407104
[2025-12-23 15:00:33,709] m-LoRA: Adapter lora_gsm8k_61 epoch: 1/1 iteration: 28/128 step: 8
[2025-12-23 15:00:33,849] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 92/128 step: 47
[2025-12-23 15:00:33,947] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 124/128 step: 63
[2025-12-23 15:00:34,216] m-LoRA: Adapter lora_gsm8k_58 epoch: 1/1 iteration: 120/128 step: 31
[2025-12-23 15:00:34,573] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 16/128 step: 9
[2025-12-23 15:00:34,610] m-LoRA: Adapter lora_gsm8k_61 loss: 1.430487871170044
[2025-12-23 15:00:34,794] m-LoRA: Adapter lora_gsm8k_56 loss: 0.7448520660400391
[2025-12-23 15:00:34,944] m-LoRA: Adapter lora_gsm8k_54 loss: 0.5640778541564941
[2025-12-23 15:00:35,304] m-LoRA: Adapter lora_gsm8k_58 loss: 1.3042523860931396
[2025-12-23 15:00:35,593] m-LoRA: Adapter lora_gsm8k_60 loss: 1.0136486291885376
[2025-12-23 15:00:35,595] m-LoRA: Adapter lora_gsm8k_61 epoch: 1/1 iteration: 32/128 step: 9
[2025-12-23 15:00:35,736] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 94/128 step: 48
[2025-12-23 15:00:35,838] m-LoRA: Adapter lora_gsm8k_54 epoch: 1/1 iteration: 126/128 step: 64
[2025-12-23 15:00:36,165] m-LoRA: Adapter lora_gsm8k_58 epoch: 1/1 iteration: 124/128 step: 32
[2025-12-23 15:00:36,537] m-LoRA: Adapter lora_gsm8k_61 loss: 1.31472647190094
[2025-12-23 15:00:36,542] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 18/128 step: 10
[2025-12-23 15:00:36,716] m-LoRA: Adapter lora_gsm8k_56 loss: 0.6511520743370056
[2025-12-23 15:00:36,872] m-LoRA: Adapter lora_gsm8k_54 loss: 0.6603272557258606
[2025-12-23 15:00:37,189] m-LoRA: Adapter lora_gsm8k_58 loss: 1.4501864910125732
[2025-12-23 15:00:37,488] m-LoRA: Adapter lora_gsm8k_60 loss: 1.2433854341506958
[2025-12-23 15:00:37,490] m-LoRA: Adapter lora_gsm8k_61 epoch: 1/1 iteration: 36/128 step: 10
[2025-12-23 15:00:37,631] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 96/128 step: 49
[2025-12-23 15:00:37,733] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_54']
[2025-12-23 15:00:38,356] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_62']
[2025-12-23 15:00:38,438] m-LoRA: Adapter lora_gsm8k_62 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 15:00:38,575] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_58']
[2025-12-23 15:00:38,865] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_63']
[2025-12-23 15:00:39,030] m-LoRA: Adapter lora_gsm8k_63 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 15:00:39,062] m-LoRA: Adapter lora_gsm8k_61 loss: 1.2829391956329346
[2025-12-23 15:00:39,259] m-LoRA: Adapter lora_gsm8k_56 loss: 0.7351497411727905
[2025-12-23 15:00:39,262] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 20/128 step: 11
[2025-12-23 15:00:40,390] m-LoRA: Adapter lora_gsm8k_61 epoch: 1/1 iteration: 40/128 step: 11
[2025-12-23 15:00:40,438] m-LoRA: Adapter lora_gsm8k_62 loss: 2.1955337524414062
[2025-12-23 15:00:40,769] m-LoRA: Adapter lora_gsm8k_63 loss: 2.4945907592773438
[2025-12-23 15:00:40,773] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 98/128 step: 50
[2025-12-23 15:00:40,804] m-LoRA: Adapter lora_gsm8k_60 loss: 1.1429522037506104
[2025-12-23 15:00:41,184] m-LoRA: Adapter lora_gsm8k_61 loss: 1.1124184131622314
[2025-12-23 15:00:41,963] m-LoRA: Adapter lora_gsm8k_56 loss: 0.6941239237785339
[2025-12-23 15:00:42,261] m-LoRA: Adapter lora_gsm8k_62 epoch: 1/1 iteration: 8/128 step: 2
[2025-12-23 15:00:42,468] m-LoRA: Adapter lora_gsm8k_63 epoch: 1/1 iteration: 4/128 step: 2
[2025-12-23 15:00:42,576] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 22/128 step: 12
[2025-12-23 15:00:42,775] m-LoRA: Adapter lora_gsm8k_61 epoch: 1/1 iteration: 44/128 step: 12
[2025-12-23 15:00:42,852] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 100/128 step: 51
[2025-12-23 15:00:43,646] m-LoRA: Adapter lora_gsm8k_62 loss: 2.2931127548217773
[2025-12-23 15:00:43,862] m-LoRA: Adapter lora_gsm8k_63 loss: 1.6546226739883423
[2025-12-23 15:00:43,996] m-LoRA: Adapter lora_gsm8k_60 loss: 0.7090387344360352
[2025-12-23 15:00:44,291] m-LoRA: Adapter lora_gsm8k_61 loss: 1.3930636644363403
[2025-12-23 15:00:44,392] m-LoRA: Adapter lora_gsm8k_56 loss: 0.5239688754081726
[2025-12-23 15:00:45,097] m-LoRA: Adapter lora_gsm8k_62 epoch: 1/1 iteration: 16/128 step: 3
[2025-12-23 15:00:45,236] m-LoRA: Adapter lora_gsm8k_63 epoch: 1/1 iteration: 8/128 step: 3
[2025-12-23 15:00:45,303] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 24/128 step: 13
[2025-12-23 15:00:45,478] m-LoRA: Adapter lora_gsm8k_61 epoch: 1/1 iteration: 48/128 step: 13
[2025-12-23 15:00:45,562] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 102/128 step: 52
[2025-12-23 15:00:46,291] m-LoRA: Adapter lora_gsm8k_62 loss: 2.348512649536133
[2025-12-23 15:00:46,532] m-LoRA: Adapter lora_gsm8k_63 loss: 1.2921855449676514
[2025-12-23 15:00:46,707] m-LoRA: Adapter lora_gsm8k_60 loss: 0.8309775590896606
[2025-12-23 15:00:46,979] m-LoRA: Adapter lora_gsm8k_61 loss: 1.2959771156311035
[2025-12-23 15:00:47,074] m-LoRA: Adapter lora_gsm8k_56 loss: 0.6091910004615784
[2025-12-23 15:00:47,701] m-LoRA: Adapter lora_gsm8k_62 epoch: 1/1 iteration: 24/128 step: 4
[2025-12-23 15:00:47,837] m-LoRA: Adapter lora_gsm8k_63 epoch: 1/1 iteration: 12/128 step: 4
[2025-12-23 15:00:47,935] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 26/128 step: 14
[2025-12-23 15:00:48,232] m-LoRA: Adapter lora_gsm8k_61 epoch: 1/1 iteration: 52/128 step: 14
[2025-12-23 15:00:48,323] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 104/128 step: 53
[2025-12-23 15:00:49,156] m-LoRA: Adapter lora_gsm8k_62 loss: 2.026171922683716
[2025-12-23 15:00:49,420] m-LoRA: Adapter lora_gsm8k_63 loss: 1.2404348850250244
[2025-12-23 15:00:49,493] m-LoRA: Adapter lora_gsm8k_60 loss: 0.9689472317695618
[2025-12-23 15:00:49,768] m-LoRA: Adapter lora_gsm8k_61 loss: 1.4792163372039795
[2025-12-23 15:00:50,013] m-LoRA: Adapter lora_gsm8k_56 loss: 0.7374110221862793
[2025-12-23 15:00:50,682] m-LoRA: Adapter lora_gsm8k_62 epoch: 1/1 iteration: 32/128 step: 5
[2025-12-23 15:00:50,808] m-LoRA: Adapter lora_gsm8k_63 epoch: 1/1 iteration: 16/128 step: 5
[2025-12-23 15:00:50,891] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 28/128 step: 15
[2025-12-23 15:00:50,978] m-LoRA: Adapter lora_gsm8k_61 epoch: 1/1 iteration: 56/128 step: 15
[2025-12-23 15:00:51,094] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 106/128 step: 54
[2025-12-23 15:00:51,782] m-LoRA: Adapter lora_gsm8k_62 loss: 1.9720797538757324
[2025-12-23 15:00:52,170] m-LoRA: Adapter lora_gsm8k_63 loss: 1.0419774055480957
[2025-12-23 15:00:52,258] m-LoRA: Adapter lora_gsm8k_60 loss: 0.6489526629447937
[2025-12-23 15:00:52,565] m-LoRA: Adapter lora_gsm8k_61 loss: 1.1140024662017822
[2025-12-23 15:00:52,865] m-LoRA: Adapter lora_gsm8k_56 loss: 0.5768063068389893
[2025-12-23 15:00:53,099] m-LoRA: Adapter lora_gsm8k_62 epoch: 1/1 iteration: 40/128 step: 6
[2025-12-23 15:00:53,540] m-LoRA: Adapter lora_gsm8k_63 epoch: 1/1 iteration: 20/128 step: 6
[2025-12-23 15:00:53,636] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 30/128 step: 16
[2025-12-23 15:00:54,265] m-LoRA: Adapter lora_gsm8k_61 epoch: 1/1 iteration: 60/128 step: 16
[2025-12-23 15:00:54,309] m-LoRA: Adapter lora_gsm8k_62 loss: 1.9838987588882446
[2025-12-23 15:00:54,607] m-LoRA: Adapter lora_gsm8k_63 loss: 1.347705602645874
[2025-12-23 15:00:54,610] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 108/128 step: 55
[2025-12-23 15:00:54,681] m-LoRA: Adapter lora_gsm8k_60 loss: 0.6243190169334412
[2025-12-23 15:00:54,977] m-LoRA: Adapter lora_gsm8k_61 loss: 1.118428111076355
[2025-12-23 15:00:55,481] m-LoRA: Adapter lora_gsm8k_56 loss: 0.7091032266616821
[2025-12-23 15:00:55,618] m-LoRA: Adapter lora_gsm8k_62 epoch: 1/1 iteration: 48/128 step: 7
[2025-12-23 15:00:55,738] m-LoRA: Adapter lora_gsm8k_63 epoch: 1/1 iteration: 24/128 step: 7
[2025-12-23 15:00:55,839] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 32/128 step: 17
[2025-12-23 15:00:56,058] m-LoRA: Adapter lora_gsm8k_61 epoch: 1/1 iteration: 64/128 step: 17
[2025-12-23 15:00:56,807] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 110/128 step: 56
[2025-12-23 15:00:56,846] m-LoRA: Adapter lora_gsm8k_62 loss: 1.586982011795044
[2025-12-23 15:00:57,136] m-LoRA: Adapter lora_gsm8k_63 loss: 1.3425997495651245
[2025-12-23 15:00:57,253] m-LoRA: Adapter lora_gsm8k_60 loss: 0.5359396934509277
[2025-12-23 15:00:57,577] m-LoRA: Adapter lora_gsm8k_61 loss: 0.9625890851020813
[2025-12-23 15:00:57,727] m-LoRA: Adapter lora_gsm8k_56 loss: 0.4843641221523285
[2025-12-23 15:00:58,193] m-LoRA: Adapter lora_gsm8k_62 epoch: 1/1 iteration: 56/128 step: 8
[2025-12-23 15:00:58,300] m-LoRA: Adapter lora_gsm8k_63 epoch: 1/1 iteration: 28/128 step: 8
[2025-12-23 15:00:58,400] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 34/128 step: 18
[2025-12-23 15:00:58,699] m-LoRA: Adapter lora_gsm8k_61 epoch: 1/1 iteration: 68/128 step: 18
[2025-12-23 15:00:59,337] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 112/128 step: 57
[2025-12-23 15:00:59,368] m-LoRA: Adapter lora_gsm8k_62 loss: 1.78565514087677
[2025-12-23 15:00:59,643] m-LoRA: Adapter lora_gsm8k_63 loss: 1.2104313373565674
[2025-12-23 15:00:59,734] m-LoRA: Adapter lora_gsm8k_60 loss: 0.6977766156196594
[2025-12-23 15:01:00,216] m-LoRA: Adapter lora_gsm8k_61 loss: 0.9591911435127258
[2025-12-23 15:01:00,383] m-LoRA: Adapter lora_gsm8k_56 loss: 0.5296345353126526
[2025-12-23 15:01:00,643] m-LoRA: Adapter lora_gsm8k_62 epoch: 1/1 iteration: 64/128 step: 9
[2025-12-23 15:01:00,921] m-LoRA: Adapter lora_gsm8k_63 epoch: 1/1 iteration: 32/128 step: 9
[2025-12-23 15:01:01,056] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 36/128 step: 19
[2025-12-23 15:01:01,883] m-LoRA: Adapter lora_gsm8k_61 epoch: 1/1 iteration: 72/128 step: 19
[2025-12-23 15:01:01,936] m-LoRA: Adapter lora_gsm8k_62 loss: 1.6021300554275513
[2025-12-23 15:01:02,237] m-LoRA: Adapter lora_gsm8k_63 loss: 1.1839330196380615
[2025-12-23 15:01:02,241] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 114/128 step: 58
[2025-12-23 15:01:02,326] m-LoRA: Adapter lora_gsm8k_60 loss: 0.5527960658073425
[2025-12-23 15:01:02,679] m-LoRA: Adapter lora_gsm8k_61 loss: 0.9262619614601135
[2025-12-23 15:01:03,098] m-LoRA: Adapter lora_gsm8k_56 loss: 0.6337668299674988
[2025-12-23 15:01:03,259] m-LoRA: Adapter lora_gsm8k_62 epoch: 1/1 iteration: 72/128 step: 10
[2025-12-23 15:01:03,428] m-LoRA: Adapter lora_gsm8k_63 epoch: 1/1 iteration: 36/128 step: 10
[2025-12-23 15:01:03,528] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 38/128 step: 20
[2025-12-23 15:01:04,423] m-LoRA: Adapter lora_gsm8k_61 epoch: 1/1 iteration: 76/128 step: 20
[2025-12-23 15:01:04,465] m-LoRA: Adapter lora_gsm8k_62 loss: 1.6051348447799683
[2025-12-23 15:01:04,746] m-LoRA: Adapter lora_gsm8k_63 loss: 1.1191126108169556
[2025-12-23 15:01:04,750] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 116/128 step: 59
[2025-12-23 15:01:04,838] m-LoRA: Adapter lora_gsm8k_60 loss: 0.6013743877410889
[2025-12-23 15:01:05,152] m-LoRA: Adapter lora_gsm8k_61 loss: 0.9007007479667664
[2025-12-23 15:01:05,604] m-LoRA: Adapter lora_gsm8k_56 loss: 0.657206654548645
[2025-12-23 15:01:05,756] m-LoRA: Adapter lora_gsm8k_62 epoch: 1/1 iteration: 80/128 step: 11
[2025-12-23 15:01:05,853] m-LoRA: Adapter lora_gsm8k_63 epoch: 1/1 iteration: 40/128 step: 11
[2025-12-23 15:01:05,937] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 40/128 step: 21
[2025-12-23 15:01:06,259] m-LoRA: Adapter lora_gsm8k_61 epoch: 1/1 iteration: 80/128 step: 21
[2025-12-23 15:01:06,347] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 118/128 step: 60
[2025-12-23 15:01:07,041] m-LoRA: Adapter lora_gsm8k_62 loss: 1.345524549484253
[2025-12-23 15:01:07,299] m-LoRA: Adapter lora_gsm8k_63 loss: 0.9290757179260254
[2025-12-23 15:01:07,378] m-LoRA: Adapter lora_gsm8k_60 loss: 0.5816434621810913
[2025-12-23 15:01:07,733] m-LoRA: Adapter lora_gsm8k_61 loss: 0.909990668296814
[2025-12-23 15:01:07,932] m-LoRA: Adapter lora_gsm8k_56 loss: 0.43327125906944275
[2025-12-23 15:01:08,380] m-LoRA: Adapter lora_gsm8k_62 epoch: 1/1 iteration: 88/128 step: 12
[2025-12-23 15:01:08,547] m-LoRA: Adapter lora_gsm8k_63 epoch: 1/1 iteration: 44/128 step: 12
[2025-12-23 15:01:08,652] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 42/128 step: 22
[2025-12-23 15:01:08,880] m-LoRA: Adapter lora_gsm8k_61 epoch: 1/1 iteration: 84/128 step: 22
[2025-12-23 15:01:09,530] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 120/128 step: 61
[2025-12-23 15:01:09,575] m-LoRA: Adapter lora_gsm8k_62 loss: 1.3840038776397705
[2025-12-23 15:01:09,859] m-LoRA: Adapter lora_gsm8k_63 loss: 0.6446653008460999
[2025-12-23 15:01:09,887] m-LoRA: Adapter lora_gsm8k_60 loss: 0.6974319815635681
[2025-12-23 15:01:10,166] m-LoRA: Adapter lora_gsm8k_61 loss: 0.8859586119651794
[2025-12-23 15:01:10,297] m-LoRA: Adapter lora_gsm8k_56 loss: 0.40662261843681335
[2025-12-23 15:01:10,921] m-LoRA: Adapter lora_gsm8k_62 epoch: 1/1 iteration: 96/128 step: 13
[2025-12-23 15:01:11,032] m-LoRA: Adapter lora_gsm8k_63 epoch: 1/1 iteration: 48/128 step: 13
[2025-12-23 15:01:11,102] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 44/128 step: 23
[2025-12-23 15:01:11,310] m-LoRA: Adapter lora_gsm8k_61 epoch: 1/1 iteration: 88/128 step: 23
[2025-12-23 15:01:11,408] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 122/128 step: 62
[2025-12-23 15:01:12,151] m-LoRA: Adapter lora_gsm8k_62 loss: 1.53019118309021
[2025-12-23 15:01:12,369] m-LoRA: Adapter lora_gsm8k_63 loss: 0.8782587051391602
[2025-12-23 15:01:12,489] m-LoRA: Adapter lora_gsm8k_60 loss: 0.7049179077148438
[2025-12-23 15:01:12,816] m-LoRA: Adapter lora_gsm8k_61 loss: 0.8074292540550232
[2025-12-23 15:01:12,932] m-LoRA: Adapter lora_gsm8k_56 loss: 0.5971343517303467
[2025-12-23 15:01:13,608] m-LoRA: Adapter lora_gsm8k_62 epoch: 1/1 iteration: 104/128 step: 14
[2025-12-23 15:01:13,737] m-LoRA: Adapter lora_gsm8k_63 epoch: 1/1 iteration: 52/128 step: 14
[2025-12-23 15:01:13,836] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 46/128 step: 24
[2025-12-23 15:01:14,024] m-LoRA: Adapter lora_gsm8k_61 epoch: 1/1 iteration: 92/128 step: 24
[2025-12-23 15:01:14,723] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 124/128 step: 63
[2025-12-23 15:01:14,764] m-LoRA: Adapter lora_gsm8k_62 loss: 1.4652462005615234
[2025-12-23 15:01:15,011] m-LoRA: Adapter lora_gsm8k_63 loss: 0.7174723148345947
[2025-12-23 15:01:15,119] m-LoRA: Adapter lora_gsm8k_60 loss: 0.7748898267745972
[2025-12-23 15:01:15,490] m-LoRA: Adapter lora_gsm8k_61 loss: 0.8512212634086609
[2025-12-23 15:01:15,587] m-LoRA: Adapter lora_gsm8k_56 loss: 0.8442202806472778
[2025-12-23 15:01:16,186] m-LoRA: Adapter lora_gsm8k_62 epoch: 1/1 iteration: 112/128 step: 15
[2025-12-23 15:01:16,325] m-LoRA: Adapter lora_gsm8k_63 epoch: 1/1 iteration: 56/128 step: 15
[2025-12-23 15:01:16,435] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 48/128 step: 25
[2025-12-23 15:01:16,751] m-LoRA: Adapter lora_gsm8k_61 epoch: 1/1 iteration: 96/128 step: 25
[2025-12-23 15:01:17,579] m-LoRA: Adapter lora_gsm8k_56 epoch: 1/1 iteration: 126/128 step: 64
[2025-12-23 15:01:17,619] m-LoRA: Adapter lora_gsm8k_62 loss: 1.2245869636535645
[2025-12-23 15:01:17,939] m-LoRA: Adapter lora_gsm8k_63 loss: 0.6598619222640991
[2025-12-23 15:01:17,950] m-LoRA: Adapter lora_gsm8k_60 loss: 0.49973422288894653
[2025-12-23 15:01:18,297] m-LoRA: Adapter lora_gsm8k_61 loss: 0.787726104259491
[2025-12-23 15:01:18,372] m-LoRA: Adapter lora_gsm8k_56 loss: 0.5464209318161011
[2025-12-23 15:01:19,264] m-LoRA: Adapter lora_gsm8k_62 epoch: 1/1 iteration: 120/128 step: 16
[2025-12-23 15:01:19,417] m-LoRA: Adapter lora_gsm8k_63 epoch: 1/1 iteration: 60/128 step: 16
[2025-12-23 15:01:19,522] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 50/128 step: 26
[2025-12-23 15:01:19,614] m-LoRA: Adapter lora_gsm8k_61 epoch: 1/1 iteration: 100/128 step: 26
[2025-12-23 15:01:19,727] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_56']
[2025-12-23 15:01:20,313] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_64']
[2025-12-23 15:01:20,622] m-LoRA: Adapter lora_gsm8k_64 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 15:01:20,661] m-LoRA: Adapter lora_gsm8k_62 loss: 1.1721552610397339
[2025-12-23 15:01:20,916] m-LoRA: Adapter lora_gsm8k_63 loss: 0.792052149772644
[2025-12-23 15:01:21,067] m-LoRA: Adapter lora_gsm8k_60 loss: 0.5480062365531921
[2025-12-23 15:01:21,553] m-LoRA: Adapter lora_gsm8k_61 loss: 0.5314380526542664
[2025-12-23 15:01:21,954] m-LoRA: Adapter lora_gsm8k_64 loss: 2.421182155609131
[2025-12-23 15:01:22,276] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_62']
[2025-12-23 15:01:22,554] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_65']
[2025-12-23 15:01:22,635] m-LoRA: Adapter lora_gsm8k_65 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 15:01:22,756] m-LoRA: Adapter lora_gsm8k_63 epoch: 1/1 iteration: 64/128 step: 17
[2025-12-23 15:01:22,846] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 52/128 step: 27
[2025-12-23 15:01:23,100] m-LoRA: Adapter lora_gsm8k_61 epoch: 1/1 iteration: 104/128 step: 27
[2025-12-23 15:01:23,286] m-LoRA: Adapter lora_gsm8k_64 epoch: 1/1 iteration: 4/128 step: 2
[2025-12-23 15:01:23,932] m-LoRA: Adapter lora_gsm8k_65 loss: 2.476853370666504
[2025-12-23 15:01:24,196] m-LoRA: Adapter lora_gsm8k_63 loss: 0.6107549071311951
[2025-12-23 15:01:24,350] m-LoRA: Adapter lora_gsm8k_60 loss: 0.7643914818763733
[2025-12-23 15:01:24,774] m-LoRA: Adapter lora_gsm8k_61 loss: 0.5947809219360352
[2025-12-23 15:01:25,098] m-LoRA: Adapter lora_gsm8k_64 loss: 1.9059292078018188
[2025-12-23 15:01:25,320] m-LoRA: Adapter lora_gsm8k_65 epoch: 1/1 iteration: 8/128 step: 2
[2025-12-23 15:01:25,503] m-LoRA: Adapter lora_gsm8k_63 epoch: 1/1 iteration: 68/128 step: 18
[2025-12-23 15:01:25,623] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 54/128 step: 28
[2025-12-23 15:01:26,675] m-LoRA: Adapter lora_gsm8k_61 epoch: 1/1 iteration: 108/128 step: 28
[2025-12-23 15:01:26,733] m-LoRA: Adapter lora_gsm8k_65 loss: 2.4819016456604004
[2025-12-23 15:01:27,094] m-LoRA: Adapter lora_gsm8k_63 loss: 0.5752737522125244
[2025-12-23 15:01:27,099] m-LoRA: Adapter lora_gsm8k_64 epoch: 1/1 iteration: 8/128 step: 3
[2025-12-23 15:01:27,132] m-LoRA: Adapter lora_gsm8k_60 loss: 0.6712357401847839
[2025-12-23 15:01:27,347] m-LoRA: Adapter lora_gsm8k_61 loss: 0.701733410358429
[2025-12-23 15:01:28,308] m-LoRA: Adapter lora_gsm8k_64 loss: 1.4355186223983765
[2025-12-23 15:01:28,312] m-LoRA: Adapter lora_gsm8k_65 epoch: 1/1 iteration: 16/128 step: 3
[2025-12-23 15:01:28,520] m-LoRA: Adapter lora_gsm8k_63 epoch: 1/1 iteration: 72/128 step: 19
[2025-12-23 15:01:28,625] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 56/128 step: 29
[2025-12-23 15:01:28,700] m-LoRA: Adapter lora_gsm8k_61 epoch: 1/1 iteration: 112/128 step: 29
[2025-12-23 15:01:29,076] m-LoRA: Adapter lora_gsm8k_64 epoch: 1/1 iteration: 12/128 step: 4
[2025-12-23 15:01:29,941] m-LoRA: Adapter lora_gsm8k_65 loss: 2.0664408206939697
[2025-12-23 15:01:30,152] m-LoRA: Adapter lora_gsm8k_63 loss: 0.6492027640342712
[2025-12-23 15:01:30,357] m-LoRA: Adapter lora_gsm8k_60 loss: 0.6131519675254822
[2025-12-23 15:01:30,665] m-LoRA: Adapter lora_gsm8k_61 loss: 0.5350708365440369
[2025-12-23 15:01:30,968] m-LoRA: Adapter lora_gsm8k_64 loss: 1.4423295259475708
[2025-12-23 15:01:31,676] m-LoRA: Adapter lora_gsm8k_65 epoch: 1/1 iteration: 24/128 step: 4
[2025-12-23 15:01:31,822] m-LoRA: Adapter lora_gsm8k_63 epoch: 1/1 iteration: 76/128 step: 20
[2025-12-23 15:01:31,886] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 58/128 step: 30
[2025-12-23 15:01:32,075] m-LoRA: Adapter lora_gsm8k_61 epoch: 1/1 iteration: 116/128 step: 30
[2025-12-23 15:01:32,928] m-LoRA: Adapter lora_gsm8k_64 epoch: 1/1 iteration: 16/128 step: 5
[2025-12-23 15:01:32,971] m-LoRA: Adapter lora_gsm8k_65 loss: 2.151707172393799
[2025-12-23 15:01:33,243] m-LoRA: Adapter lora_gsm8k_63 loss: 0.7280519008636475
[2025-12-23 15:01:33,370] m-LoRA: Adapter lora_gsm8k_60 loss: 0.5647068619728088
[2025-12-23 15:01:33,699] m-LoRA: Adapter lora_gsm8k_61 loss: 0.7321287989616394
[2025-12-23 15:01:33,952] m-LoRA: Adapter lora_gsm8k_64 loss: 1.3245521783828735
[2025-12-23 15:01:34,268] m-LoRA: Adapter lora_gsm8k_65 epoch: 1/1 iteration: 32/128 step: 5
[2025-12-23 15:01:34,412] m-LoRA: Adapter lora_gsm8k_63 epoch: 1/1 iteration: 80/128 step: 21
[2025-12-23 15:01:34,560] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 60/128 step: 31
[2025-12-23 15:01:34,815] m-LoRA: Adapter lora_gsm8k_61 epoch: 1/1 iteration: 120/128 step: 31
[2025-12-23 15:01:35,723] m-LoRA: Adapter lora_gsm8k_64 epoch: 1/1 iteration: 20/128 step: 6
[2025-12-23 15:01:35,777] m-LoRA: Adapter lora_gsm8k_65 loss: 1.8890504837036133
[2025-12-23 15:01:36,054] m-LoRA: Adapter lora_gsm8k_63 loss: 0.6683571338653564
[2025-12-23 15:01:36,148] m-LoRA: Adapter lora_gsm8k_60 loss: 0.45951226353645325
[2025-12-23 15:01:36,535] m-LoRA: Adapter lora_gsm8k_61 loss: 0.5903356671333313
[2025-12-23 15:01:36,778] m-LoRA: Adapter lora_gsm8k_64 loss: 1.4265031814575195
[2025-12-23 15:01:37,221] m-LoRA: Adapter lora_gsm8k_65 epoch: 1/1 iteration: 40/128 step: 6
[2025-12-23 15:01:37,393] m-LoRA: Adapter lora_gsm8k_63 epoch: 1/1 iteration: 84/128 step: 22
[2025-12-23 15:01:37,532] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 62/128 step: 32
[2025-12-23 15:01:37,753] m-LoRA: Adapter lora_gsm8k_61 epoch: 1/1 iteration: 124/128 step: 32
[2025-12-23 15:01:37,966] m-LoRA: Adapter lora_gsm8k_64 epoch: 1/1 iteration: 24/128 step: 7
[2025-12-23 15:01:38,556] m-LoRA: Adapter lora_gsm8k_65 loss: 1.9630060195922852
[2025-12-23 15:01:38,835] m-LoRA: Adapter lora_gsm8k_63 loss: 0.5862149596214294
[2025-12-23 15:01:38,933] m-LoRA: Adapter lora_gsm8k_60 loss: 0.5071037411689758
[2025-12-23 15:01:39,162] m-LoRA: Adapter lora_gsm8k_61 loss: 0.5505366921424866
[2025-12-23 15:01:39,450] m-LoRA: Adapter lora_gsm8k_64 loss: 1.4849021434783936
[2025-12-23 15:01:39,679] m-LoRA: Adapter lora_gsm8k_65 epoch: 1/1 iteration: 48/128 step: 7
[2025-12-23 15:01:39,944] m-LoRA: Adapter lora_gsm8k_63 epoch: 1/1 iteration: 88/128 step: 23
[2025-12-23 15:01:40,029] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 64/128 step: 33
[2025-12-23 15:01:40,095] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_61']
[2025-12-23 15:01:40,262] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_66']
[2025-12-23 15:01:40,339] m-LoRA: Adapter lora_gsm8k_66 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 15:01:41,165] m-LoRA: Adapter lora_gsm8k_64 epoch: 1/1 iteration: 28/128 step: 8
[2025-12-23 15:01:41,220] m-LoRA: Adapter lora_gsm8k_65 loss: 1.4738364219665527
[2025-12-23 15:01:41,480] m-LoRA: Adapter lora_gsm8k_63 loss: 0.7547852396965027
[2025-12-23 15:01:41,579] m-LoRA: Adapter lora_gsm8k_60 loss: 0.8578545451164246
[2025-12-23 15:01:42,822] m-LoRA: Adapter lora_gsm8k_65 epoch: 1/1 iteration: 56/128 step: 8
[2025-12-23 15:01:42,919] m-LoRA: Adapter lora_gsm8k_66 loss: 2.4905989170074463
[2025-12-23 15:01:43,024] m-LoRA: Adapter lora_gsm8k_63 epoch: 1/1 iteration: 92/128 step: 24
[2025-12-23 15:01:43,242] m-LoRA: Adapter lora_gsm8k_64 loss: 1.2417171001434326
[2025-12-23 15:01:43,246] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 66/128 step: 34
[2025-12-23 15:01:43,896] m-LoRA: Adapter lora_gsm8k_65 loss: 1.6051504611968994
[2025-12-23 15:01:45,142] m-LoRA: Adapter lora_gsm8k_63 loss: 0.6147754192352295
[2025-12-23 15:01:45,214] m-LoRA: Adapter lora_gsm8k_60 loss: 0.8042733073234558
[2025-12-23 15:01:45,501] m-LoRA: Adapter lora_gsm8k_66 epoch: 1/1 iteration: 16/128 step: 2
[2025-12-23 15:01:45,697] m-LoRA: Adapter lora_gsm8k_64 epoch: 1/1 iteration: 32/128 step: 9
[2025-12-23 15:01:45,827] m-LoRA: Adapter lora_gsm8k_65 epoch: 1/1 iteration: 64/128 step: 9
[2025-12-23 15:01:46,057] m-LoRA: Adapter lora_gsm8k_63 epoch: 1/1 iteration: 96/128 step: 25
[2025-12-23 15:01:46,138] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 68/128 step: 35
[2025-12-23 15:01:47,967] m-LoRA: Adapter lora_gsm8k_66 loss: 1.3688113689422607
[2025-12-23 15:01:48,038] m-LoRA: Adapter lora_gsm8k_64 loss: 1.1504324674606323
[2025-12-23 15:01:48,721] m-LoRA: Adapter lora_gsm8k_65 loss: 1.5597779750823975
[2025-12-23 15:01:48,936] m-LoRA: Adapter lora_gsm8k_63 loss: 0.6055924296379089
[2025-12-23 15:01:49,055] m-LoRA: Adapter lora_gsm8k_60 loss: 0.744837760925293
[2025-12-23 15:01:50,782] m-LoRA: Adapter lora_gsm8k_66 epoch: 1/1 iteration: 32/128 step: 3
[2025-12-23 15:01:51,056] m-LoRA: Adapter lora_gsm8k_64 epoch: 1/1 iteration: 36/128 step: 10
[2025-12-23 15:01:51,157] m-LoRA: Adapter lora_gsm8k_65 epoch: 1/1 iteration: 72/128 step: 10
[2025-12-23 15:01:51,305] m-LoRA: Adapter lora_gsm8k_63 epoch: 1/1 iteration: 100/128 step: 26
[2025-12-23 15:01:51,407] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 70/128 step: 36
[2025-12-23 15:01:53,765] m-LoRA: Adapter lora_gsm8k_66 loss: 1.04208242893219
[2025-12-23 15:01:53,805] m-LoRA: Adapter lora_gsm8k_64 loss: 1.203819990158081
[2025-12-23 15:01:54,780] m-LoRA: Adapter lora_gsm8k_65 loss: 1.0919374227523804
[2025-12-23 15:01:55,050] m-LoRA: Adapter lora_gsm8k_63 loss: 0.5102799534797668
[2025-12-23 15:01:55,140] m-LoRA: Adapter lora_gsm8k_60 loss: 0.6105043888092041
[2025-12-23 15:01:57,218] m-LoRA: Adapter lora_gsm8k_66 epoch: 1/1 iteration: 48/128 step: 4
[2025-12-23 15:01:57,443] m-LoRA: Adapter lora_gsm8k_64 epoch: 1/1 iteration: 40/128 step: 11
[2025-12-23 15:01:57,619] m-LoRA: Adapter lora_gsm8k_65 epoch: 1/1 iteration: 80/128 step: 11
[2025-12-23 15:01:57,789] m-LoRA: Adapter lora_gsm8k_63 epoch: 1/1 iteration: 104/128 step: 27
[2025-12-23 15:01:57,912] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 72/128 step: 37
[2025-12-23 15:01:59,759] m-LoRA: Adapter lora_gsm8k_66 loss: 1.2725800275802612
[2025-12-23 15:01:59,875] m-LoRA: Adapter lora_gsm8k_64 loss: 0.9562371373176575
[2025-12-23 15:02:00,474] m-LoRA: Adapter lora_gsm8k_65 loss: 1.4798431396484375
[2025-12-23 15:02:00,934] m-LoRA: Adapter lora_gsm8k_63 loss: 0.6175392866134644
[2025-12-23 15:02:01,070] m-LoRA: Adapter lora_gsm8k_60 loss: 0.8149217367172241
[2025-12-23 15:02:02,366] m-LoRA: Adapter lora_gsm8k_66 epoch: 1/1 iteration: 64/128 step: 5
[2025-12-23 15:02:02,595] m-LoRA: Adapter lora_gsm8k_64 epoch: 1/1 iteration: 44/128 step: 12
[2025-12-23 15:02:02,696] m-LoRA: Adapter lora_gsm8k_65 epoch: 1/1 iteration: 88/128 step: 12
[2025-12-23 15:02:02,834] m-LoRA: Adapter lora_gsm8k_63 epoch: 1/1 iteration: 108/128 step: 28
[2025-12-23 15:02:02,921] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 74/128 step: 38
[2025-12-23 15:02:04,432] m-LoRA: Adapter lora_gsm8k_66 loss: 1.380258321762085
[2025-12-23 15:02:04,671] m-LoRA: Adapter lora_gsm8k_64 loss: 0.9598600268363953
[2025-12-23 15:02:05,263] m-LoRA: Adapter lora_gsm8k_65 loss: 1.4262847900390625
[2025-12-23 15:02:05,611] m-LoRA: Adapter lora_gsm8k_63 loss: 0.5272126197814941
[2025-12-23 15:02:05,740] m-LoRA: Adapter lora_gsm8k_60 loss: 0.5746867656707764
[2025-12-23 15:02:06,844] m-LoRA: Adapter lora_gsm8k_66 epoch: 1/1 iteration: 80/128 step: 6
[2025-12-23 15:02:07,047] m-LoRA: Adapter lora_gsm8k_64 epoch: 1/1 iteration: 48/128 step: 13
[2025-12-23 15:02:07,228] m-LoRA: Adapter lora_gsm8k_65 epoch: 1/1 iteration: 96/128 step: 13
[2025-12-23 15:02:07,478] m-LoRA: Adapter lora_gsm8k_63 epoch: 1/1 iteration: 112/128 step: 29
[2025-12-23 15:02:07,626] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 76/128 step: 39
[2025-12-23 15:02:09,594] m-LoRA: Adapter lora_gsm8k_66 loss: 1.074981927871704
[2025-12-23 15:02:09,626] m-LoRA: Adapter lora_gsm8k_64 loss: 0.8065398931503296
[2025-12-23 15:02:10,452] m-LoRA: Adapter lora_gsm8k_65 loss: 1.260947585105896
[2025-12-23 15:02:10,623] m-LoRA: Adapter lora_gsm8k_63 loss: 0.6369743943214417
[2025-12-23 15:02:10,716] m-LoRA: Adapter lora_gsm8k_60 loss: 0.7801148891448975
[2025-12-23 15:02:12,576] m-LoRA: Adapter lora_gsm8k_66 epoch: 1/1 iteration: 96/128 step: 7
[2025-12-23 15:02:12,793] m-LoRA: Adapter lora_gsm8k_64 epoch: 1/1 iteration: 52/128 step: 14
[2025-12-23 15:02:12,893] m-LoRA: Adapter lora_gsm8k_65 epoch: 1/1 iteration: 104/128 step: 14
[2025-12-23 15:02:13,045] m-LoRA: Adapter lora_gsm8k_63 epoch: 1/1 iteration: 116/128 step: 30
[2025-12-23 15:02:13,124] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 78/128 step: 40
[2025-12-23 15:02:15,005] m-LoRA: Adapter lora_gsm8k_66 loss: 1.1207348108291626
[2025-12-23 15:02:15,281] m-LoRA: Adapter lora_gsm8k_64 loss: 0.6420083045959473
[2025-12-23 15:02:15,866] m-LoRA: Adapter lora_gsm8k_65 loss: 1.377719759941101
[2025-12-23 15:02:16,038] m-LoRA: Adapter lora_gsm8k_63 loss: 0.5644215941429138
[2025-12-23 15:02:16,176] m-LoRA: Adapter lora_gsm8k_60 loss: 0.5423166155815125
[2025-12-23 15:02:17,672] m-LoRA: Adapter lora_gsm8k_66 epoch: 1/1 iteration: 112/128 step: 8
[2025-12-23 15:02:17,866] m-LoRA: Adapter lora_gsm8k_64 epoch: 1/1 iteration: 56/128 step: 15
[2025-12-23 15:02:17,987] m-LoRA: Adapter lora_gsm8k_65 epoch: 1/1 iteration: 112/128 step: 15
[2025-12-23 15:02:18,161] m-LoRA: Adapter lora_gsm8k_63 epoch: 1/1 iteration: 120/128 step: 31
[2025-12-23 15:02:18,283] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 80/128 step: 41
[2025-12-23 15:02:19,778] m-LoRA: Adapter lora_gsm8k_66 loss: 1.2573096752166748
[2025-12-23 15:02:20,021] m-LoRA: Adapter lora_gsm8k_64 loss: 0.6970283389091492
[2025-12-23 15:02:20,774] m-LoRA: Adapter lora_gsm8k_65 loss: 1.1635898351669312
[2025-12-23 15:02:21,068] m-LoRA: Adapter lora_gsm8k_63 loss: 0.5334612131118774
[2025-12-23 15:02:21,286] m-LoRA: Adapter lora_gsm8k_60 loss: 0.5093827843666077
[2025-12-23 15:02:22,097] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_66']
[2025-12-23 15:02:22,439] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_67']
[2025-12-23 15:02:22,521] m-LoRA: Adapter lora_gsm8k_67 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 15:02:22,610] m-LoRA: Adapter lora_gsm8k_64 epoch: 1/1 iteration: 60/128 step: 16
[2025-12-23 15:02:23,254] m-LoRA: Adapter lora_gsm8k_67 loss: 2.238607406616211
[2025-12-23 15:02:23,258] m-LoRA: Adapter lora_gsm8k_65 epoch: 1/1 iteration: 120/128 step: 16
[2025-12-23 15:02:23,539] m-LoRA: Adapter lora_gsm8k_64 loss: 0.5715616941452026
[2025-12-23 15:02:23,544] m-LoRA: Adapter lora_gsm8k_63 epoch: 1/1 iteration: 124/128 step: 32
[2025-12-23 15:02:23,656] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 82/128 step: 42
[2025-12-23 15:02:23,987] m-LoRA: Adapter lora_gsm8k_67 epoch: 1/1 iteration: 4/128 step: 2
[2025-12-23 15:02:24,582] m-LoRA: Adapter lora_gsm8k_64 epoch: 1/1 iteration: 64/128 step: 17
[2025-12-23 15:02:24,637] m-LoRA: Adapter lora_gsm8k_65 loss: 1.4053709506988525
[2025-12-23 15:02:24,805] m-LoRA: Adapter lora_gsm8k_63 loss: 0.4909331202507019
[2025-12-23 15:02:25,023] m-LoRA: Adapter lora_gsm8k_60 loss: 0.8997491598129272
[2025-12-23 15:02:25,349] m-LoRA: Adapter lora_gsm8k_67 loss: 1.4951177835464478
[2025-12-23 15:02:25,594] m-LoRA: Adapter lora_gsm8k_64 loss: 0.5518102645874023
[2025-12-23 15:02:26,000] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_65']
[2025-12-23 15:02:26,246] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_68']
[2025-12-23 15:02:26,326] m-LoRA: Adapter lora_gsm8k_68 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 15:02:26,446] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_63']
[2025-12-23 15:02:26,658] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_69']
[2025-12-23 15:02:26,736] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 15:02:26,827] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 84/128 step: 43
[2025-12-23 15:02:27,482] m-LoRA: Adapter lora_gsm8k_68 loss: 2.6322381496429443
[2025-12-23 15:02:27,488] m-LoRA: Adapter lora_gsm8k_67 epoch: 1/1 iteration: 8/128 step: 3
[2025-12-23 15:02:27,645] m-LoRA: Adapter lora_gsm8k_69 loss: 2.0641720294952393
[2025-12-23 15:02:27,647] m-LoRA: Adapter lora_gsm8k_64 epoch: 1/1 iteration: 68/128 step: 18
[2025-12-23 15:02:27,736] m-LoRA: Adapter lora_gsm8k_60 loss: 0.5581563115119934
[2025-12-23 15:02:28,024] m-LoRA: Adapter lora_gsm8k_67 loss: 1.5866129398345947
[2025-12-23 15:02:28,924] m-LoRA: Adapter lora_gsm8k_64 loss: 0.5094583630561829
[2025-12-23 15:02:28,928] m-LoRA: Adapter lora_gsm8k_68 epoch: 1/1 iteration: 8/128 step: 2
[2025-12-23 15:02:29,114] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 2/128 step: 2
[2025-12-23 15:02:29,204] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 86/128 step: 44
[2025-12-23 15:02:29,289] m-LoRA: Adapter lora_gsm8k_67 epoch: 1/1 iteration: 12/128 step: 4
[2025-12-23 15:02:30,432] m-LoRA: Adapter lora_gsm8k_64 epoch: 1/1 iteration: 72/128 step: 19
[2025-12-23 15:02:30,528] m-LoRA: Adapter lora_gsm8k_68 loss: 2.136536121368408
[2025-12-23 15:02:30,537] m-LoRA: Adapter lora_gsm8k_69 loss: 1.5156856775283813
[2025-12-23 15:02:30,714] m-LoRA: Adapter lora_gsm8k_60 loss: 0.5915344953536987
[2025-12-23 15:02:31,051] m-LoRA: Adapter lora_gsm8k_67 loss: 1.5338393449783325
[2025-12-23 15:02:31,346] m-LoRA: Adapter lora_gsm8k_64 loss: 0.5919084548950195
[2025-12-23 15:02:32,223] m-LoRA: Adapter lora_gsm8k_68 epoch: 1/1 iteration: 16/128 step: 3
[2025-12-23 15:02:32,372] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 4/128 step: 3
[2025-12-23 15:02:32,444] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 88/128 step: 45
[2025-12-23 15:02:32,517] m-LoRA: Adapter lora_gsm8k_67 epoch: 1/1 iteration: 16/128 step: 5
[2025-12-23 15:02:32,722] m-LoRA: Adapter lora_gsm8k_64 epoch: 1/1 iteration: 76/128 step: 20
[2025-12-23 15:02:33,802] m-LoRA: Adapter lora_gsm8k_68 loss: 2.0266425609588623
[2025-12-23 15:02:33,815] m-LoRA: Adapter lora_gsm8k_69 loss: 2.0697760581970215
[2025-12-23 15:02:34,055] m-LoRA: Adapter lora_gsm8k_60 loss: 0.5318518280982971
[2025-12-23 15:02:34,303] m-LoRA: Adapter lora_gsm8k_67 loss: 1.577580451965332
[2025-12-23 15:02:34,568] m-LoRA: Adapter lora_gsm8k_64 loss: 0.6683610081672668
[2025-12-23 15:02:35,645] m-LoRA: Adapter lora_gsm8k_68 epoch: 1/1 iteration: 24/128 step: 4
[2025-12-23 15:02:35,815] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 6/128 step: 4
[2025-12-23 15:02:35,887] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 90/128 step: 46
[2025-12-23 15:02:35,960] m-LoRA: Adapter lora_gsm8k_67 epoch: 1/1 iteration: 20/128 step: 6
[2025-12-23 15:02:36,041] m-LoRA: Adapter lora_gsm8k_64 epoch: 1/1 iteration: 80/128 step: 21
[2025-12-23 15:02:37,014] m-LoRA: Adapter lora_gsm8k_68 loss: 2.0348236560821533
[2025-12-23 15:02:37,031] m-LoRA: Adapter lora_gsm8k_69 loss: 1.6804604530334473
[2025-12-23 15:02:37,254] m-LoRA: Adapter lora_gsm8k_60 loss: 0.544693112373352
[2025-12-23 15:02:37,560] m-LoRA: Adapter lora_gsm8k_67 loss: 1.2769602537155151
[2025-12-23 15:02:37,874] m-LoRA: Adapter lora_gsm8k_64 loss: 0.7016732692718506
[2025-12-23 15:02:38,574] m-LoRA: Adapter lora_gsm8k_68 epoch: 1/1 iteration: 32/128 step: 5
[2025-12-23 15:02:38,716] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 8/128 step: 5
[2025-12-23 15:02:38,804] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 92/128 step: 47
[2025-12-23 15:02:38,878] m-LoRA: Adapter lora_gsm8k_67 epoch: 1/1 iteration: 24/128 step: 7
[2025-12-23 15:02:39,087] m-LoRA: Adapter lora_gsm8k_64 epoch: 1/1 iteration: 84/128 step: 22
[2025-12-23 15:02:39,819] m-LoRA: Adapter lora_gsm8k_68 loss: 1.9759056568145752
[2025-12-23 15:02:39,840] m-LoRA: Adapter lora_gsm8k_69 loss: 1.4121074676513672
[2025-12-23 15:02:40,078] m-LoRA: Adapter lora_gsm8k_60 loss: 0.6017377376556396
[2025-12-23 15:02:40,347] m-LoRA: Adapter lora_gsm8k_67 loss: 1.4018301963806152
[2025-12-23 15:02:40,819] m-LoRA: Adapter lora_gsm8k_64 loss: 0.6367893218994141
[2025-12-23 15:02:41,206] m-LoRA: Adapter lora_gsm8k_68 epoch: 1/1 iteration: 40/128 step: 6
[2025-12-23 15:02:41,361] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 10/128 step: 6
[2025-12-23 15:02:41,439] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 94/128 step: 48
[2025-12-23 15:02:41,617] m-LoRA: Adapter lora_gsm8k_67 epoch: 1/1 iteration: 28/128 step: 8
[2025-12-23 15:02:42,646] m-LoRA: Adapter lora_gsm8k_64 epoch: 1/1 iteration: 88/128 step: 23
[2025-12-23 15:02:42,714] m-LoRA: Adapter lora_gsm8k_68 loss: 1.830307126045227
[2025-12-23 15:02:42,722] m-LoRA: Adapter lora_gsm8k_69 loss: 1.4763209819793701
[2025-12-23 15:02:42,991] m-LoRA: Adapter lora_gsm8k_60 loss: 0.8033291697502136
[2025-12-23 15:02:43,369] m-LoRA: Adapter lora_gsm8k_67 loss: 0.8857649564743042
[2025-12-23 15:02:43,669] m-LoRA: Adapter lora_gsm8k_64 loss: 0.5796829462051392
[2025-12-23 15:02:44,106] m-LoRA: Adapter lora_gsm8k_68 epoch: 1/1 iteration: 48/128 step: 7
[2025-12-23 15:02:44,218] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 12/128 step: 7
[2025-12-23 15:02:44,308] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 96/128 step: 49
[2025-12-23 15:02:44,657] m-LoRA: Adapter lora_gsm8k_67 epoch: 1/1 iteration: 32/128 step: 9
[2025-12-23 15:02:45,411] m-LoRA: Adapter lora_gsm8k_64 epoch: 1/1 iteration: 92/128 step: 24
[2025-12-23 15:02:45,480] m-LoRA: Adapter lora_gsm8k_68 loss: 1.6846857070922852
[2025-12-23 15:02:45,488] m-LoRA: Adapter lora_gsm8k_69 loss: 1.260454535484314
[2025-12-23 15:02:45,702] m-LoRA: Adapter lora_gsm8k_60 loss: 0.47825536131858826
[2025-12-23 15:02:45,976] m-LoRA: Adapter lora_gsm8k_67 loss: 1.0735061168670654
[2025-12-23 15:02:46,216] m-LoRA: Adapter lora_gsm8k_64 loss: 0.6463183760643005
[2025-12-23 15:02:46,780] m-LoRA: Adapter lora_gsm8k_68 epoch: 1/1 iteration: 56/128 step: 8
[2025-12-23 15:02:46,885] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 14/128 step: 8
[2025-12-23 15:02:46,970] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 98/128 step: 50
[2025-12-23 15:02:47,043] m-LoRA: Adapter lora_gsm8k_67 epoch: 1/1 iteration: 36/128 step: 10
[2025-12-23 15:02:47,927] m-LoRA: Adapter lora_gsm8k_64 epoch: 1/1 iteration: 96/128 step: 25
[2025-12-23 15:02:48,000] m-LoRA: Adapter lora_gsm8k_68 loss: 1.6357059478759766
[2025-12-23 15:02:48,008] m-LoRA: Adapter lora_gsm8k_69 loss: 1.320083737373352
[2025-12-23 15:02:48,273] m-LoRA: Adapter lora_gsm8k_60 loss: 0.4750320315361023
[2025-12-23 15:02:48,719] m-LoRA: Adapter lora_gsm8k_67 loss: 0.8003525137901306
[2025-12-23 15:02:49,103] m-LoRA: Adapter lora_gsm8k_64 loss: 0.541973888874054
[2025-12-23 15:02:49,323] m-LoRA: Adapter lora_gsm8k_68 epoch: 1/1 iteration: 64/128 step: 9
[2025-12-23 15:02:49,454] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 16/128 step: 9
[2025-12-23 15:02:49,556] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 100/128 step: 51
[2025-12-23 15:02:49,911] m-LoRA: Adapter lora_gsm8k_67 epoch: 1/1 iteration: 40/128 step: 11
[2025-12-23 15:02:50,833] m-LoRA: Adapter lora_gsm8k_64 epoch: 1/1 iteration: 100/128 step: 26
[2025-12-23 15:02:50,932] m-LoRA: Adapter lora_gsm8k_68 loss: 1.3305741548538208
[2025-12-23 15:02:50,941] m-LoRA: Adapter lora_gsm8k_69 loss: 1.2229751348495483
[2025-12-23 15:02:51,116] m-LoRA: Adapter lora_gsm8k_60 loss: 0.5751798152923584
[2025-12-23 15:02:51,506] m-LoRA: Adapter lora_gsm8k_67 loss: 0.9218037724494934
[2025-12-23 15:02:51,795] m-LoRA: Adapter lora_gsm8k_64 loss: 0.49292370676994324
[2025-12-23 15:02:52,500] m-LoRA: Adapter lora_gsm8k_68 epoch: 1/1 iteration: 72/128 step: 10
[2025-12-23 15:02:52,645] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 18/128 step: 10
[2025-12-23 15:02:52,737] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 102/128 step: 52
[2025-12-23 15:02:52,814] m-LoRA: Adapter lora_gsm8k_67 epoch: 1/1 iteration: 44/128 step: 12
[2025-12-23 15:02:53,049] m-LoRA: Adapter lora_gsm8k_64 epoch: 1/1 iteration: 104/128 step: 27
[2025-12-23 15:02:53,951] m-LoRA: Adapter lora_gsm8k_68 loss: 1.286808967590332
[2025-12-23 15:02:53,970] m-LoRA: Adapter lora_gsm8k_69 loss: 1.0712469816207886
[2025-12-23 15:02:54,116] m-LoRA: Adapter lora_gsm8k_60 loss: 0.6265875101089478
[2025-12-23 15:02:54,489] m-LoRA: Adapter lora_gsm8k_67 loss: 0.6426147222518921
[2025-12-23 15:02:54,766] m-LoRA: Adapter lora_gsm8k_64 loss: 0.7273719906806946
[2025-12-23 15:02:55,579] m-LoRA: Adapter lora_gsm8k_68 epoch: 1/1 iteration: 80/128 step: 11
[2025-12-23 15:02:55,705] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 20/128 step: 11
[2025-12-23 15:02:55,776] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 104/128 step: 53
[2025-12-23 15:02:55,850] m-LoRA: Adapter lora_gsm8k_67 epoch: 1/1 iteration: 48/128 step: 13
[2025-12-23 15:02:55,979] m-LoRA: Adapter lora_gsm8k_64 epoch: 1/1 iteration: 108/128 step: 28
[2025-12-23 15:02:56,684] m-LoRA: Adapter lora_gsm8k_68 loss: 1.5358588695526123
[2025-12-23 15:02:56,701] m-LoRA: Adapter lora_gsm8k_69 loss: 0.9697672724723816
[2025-12-23 15:02:56,947] m-LoRA: Adapter lora_gsm8k_60 loss: 0.5005375146865845
[2025-12-23 15:02:57,176] m-LoRA: Adapter lora_gsm8k_67 loss: 0.7292507290840149
[2025-12-23 15:02:57,573] m-LoRA: Adapter lora_gsm8k_64 loss: 0.4630691707134247
[2025-12-23 15:02:57,911] m-LoRA: Adapter lora_gsm8k_68 epoch: 1/1 iteration: 88/128 step: 12
[2025-12-23 15:02:58,043] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 22/128 step: 12
[2025-12-23 15:02:58,138] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 106/128 step: 54
[2025-12-23 15:02:58,260] m-LoRA: Adapter lora_gsm8k_67 epoch: 1/1 iteration: 52/128 step: 14
[2025-12-23 15:02:58,684] m-LoRA: Adapter lora_gsm8k_64 epoch: 1/1 iteration: 112/128 step: 29
[2025-12-23 15:02:59,501] m-LoRA: Adapter lora_gsm8k_68 loss: 1.315487027168274
[2025-12-23 15:02:59,537] m-LoRA: Adapter lora_gsm8k_69 loss: 0.6903959512710571
[2025-12-23 15:02:59,783] m-LoRA: Adapter lora_gsm8k_60 loss: 0.9786438345909119
[2025-12-23 15:03:00,073] m-LoRA: Adapter lora_gsm8k_67 loss: 0.6086796522140503
[2025-12-23 15:03:00,356] m-LoRA: Adapter lora_gsm8k_64 loss: 0.5246425867080688
[2025-12-23 15:03:01,089] m-LoRA: Adapter lora_gsm8k_68 epoch: 1/1 iteration: 96/128 step: 13
[2025-12-23 15:03:01,216] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 24/128 step: 13
[2025-12-23 15:03:01,310] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 108/128 step: 55
[2025-12-23 15:03:01,387] m-LoRA: Adapter lora_gsm8k_67 epoch: 1/1 iteration: 56/128 step: 15
[2025-12-23 15:03:01,591] m-LoRA: Adapter lora_gsm8k_64 epoch: 1/1 iteration: 116/128 step: 30
[2025-12-23 15:03:02,306] m-LoRA: Adapter lora_gsm8k_68 loss: 1.5423728227615356
[2025-12-23 15:03:02,323] m-LoRA: Adapter lora_gsm8k_69 loss: 0.6636264324188232
[2025-12-23 15:03:02,530] m-LoRA: Adapter lora_gsm8k_60 loss: 0.6366333961486816
[2025-12-23 15:03:02,914] m-LoRA: Adapter lora_gsm8k_67 loss: 0.6157809495925903
[2025-12-23 15:03:03,265] m-LoRA: Adapter lora_gsm8k_64 loss: 0.6219010949134827
[2025-12-23 15:03:03,701] m-LoRA: Adapter lora_gsm8k_68 epoch: 1/1 iteration: 104/128 step: 14
[2025-12-23 15:03:03,825] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 26/128 step: 14
[2025-12-23 15:03:03,915] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 110/128 step: 56
[2025-12-23 15:03:04,120] m-LoRA: Adapter lora_gsm8k_67 epoch: 1/1 iteration: 60/128 step: 16
[2025-12-23 15:03:04,879] m-LoRA: Adapter lora_gsm8k_64 epoch: 1/1 iteration: 120/128 step: 31
[2025-12-23 15:03:04,941] m-LoRA: Adapter lora_gsm8k_68 loss: 1.6312954425811768
[2025-12-23 15:03:04,948] m-LoRA: Adapter lora_gsm8k_69 loss: 0.718721866607666
[2025-12-23 15:03:05,184] m-LoRA: Adapter lora_gsm8k_60 loss: 0.3695736229419708
[2025-12-23 15:03:05,524] m-LoRA: Adapter lora_gsm8k_67 loss: 0.5839097499847412
[2025-12-23 15:03:05,827] m-LoRA: Adapter lora_gsm8k_64 loss: 0.5551697611808777
[2025-12-23 15:03:06,247] m-LoRA: Adapter lora_gsm8k_68 epoch: 1/1 iteration: 112/128 step: 15
[2025-12-23 15:03:06,363] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 28/128 step: 15
[2025-12-23 15:03:06,454] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 112/128 step: 57
[2025-12-23 15:03:06,577] m-LoRA: Adapter lora_gsm8k_67 epoch: 1/1 iteration: 64/128 step: 17
[2025-12-23 15:03:07,400] m-LoRA: Adapter lora_gsm8k_68 loss: 1.7323217391967773
[2025-12-23 15:03:07,406] m-LoRA: Adapter lora_gsm8k_64 epoch: 1/1 iteration: 124/128 step: 32
[2025-12-23 15:03:07,458] m-LoRA: Adapter lora_gsm8k_69 loss: 0.6803948879241943
[2025-12-23 15:03:07,545] m-LoRA: Adapter lora_gsm8k_60 loss: 0.5621240139007568
[2025-12-23 15:03:07,871] m-LoRA: Adapter lora_gsm8k_67 loss: 0.5320996046066284
[2025-12-23 15:03:08,242] m-LoRA: Adapter lora_gsm8k_64 loss: 0.5297838449478149
[2025-12-23 15:03:08,539] m-LoRA: Adapter lora_gsm8k_68 epoch: 1/1 iteration: 120/128 step: 16
[2025-12-23 15:03:08,714] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 30/128 step: 16
[2025-12-23 15:03:08,821] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 114/128 step: 58
[2025-12-23 15:03:08,922] m-LoRA: Adapter lora_gsm8k_67 epoch: 1/1 iteration: 68/128 step: 18
[2025-12-23 15:03:09,953] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_64']
[2025-12-23 15:03:10,695] m-LoRA: Adapter lora_gsm8k_68 loss: 1.4234774112701416
[2025-12-23 15:03:10,701] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_70']
[2025-12-23 15:03:11,036] m-LoRA: Adapter lora_gsm8k_70 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 15:03:11,063] m-LoRA: Adapter lora_gsm8k_69 loss: 0.6926020383834839
[2025-12-23 15:03:11,067] m-LoRA: Adapter lora_gsm8k_60 loss: 0.5977927446365356
[2025-12-23 15:03:11,071] m-LoRA: Adapter lora_gsm8k_67 loss: 0.5954765677452087
[2025-12-23 15:03:12,146] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_68']
[2025-12-23 15:03:12,409] m-LoRA: Adapter lora_gsm8k_70 loss: 2.410759210586548
[2025-12-23 15:03:12,412] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_71']
[2025-12-23 15:03:12,686] m-LoRA: Adapter lora_gsm8k_71 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 15:03:12,782] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 32/128 step: 17
[2025-12-23 15:03:12,837] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 116/128 step: 59
[2025-12-23 15:03:12,942] m-LoRA: Adapter lora_gsm8k_67 epoch: 1/1 iteration: 72/128 step: 19
[2025-12-23 15:03:13,199] m-LoRA: Adapter lora_gsm8k_70 epoch: 1/1 iteration: 4/128 step: 2
[2025-12-23 15:03:14,056] m-LoRA: Adapter lora_gsm8k_71 loss: 2.1446375846862793
[2025-12-23 15:03:14,074] m-LoRA: Adapter lora_gsm8k_69 loss: 0.6726542711257935
[2025-12-23 15:03:14,299] m-LoRA: Adapter lora_gsm8k_60 loss: 0.5786588191986084
[2025-12-23 15:03:14,543] m-LoRA: Adapter lora_gsm8k_67 loss: 0.46766918897628784
[2025-12-23 15:03:14,798] m-LoRA: Adapter lora_gsm8k_70 loss: 1.5941143035888672
[2025-12-23 15:03:15,691] m-LoRA: Adapter lora_gsm8k_71 epoch: 1/1 iteration: 8/128 step: 2
[2025-12-23 15:03:15,836] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 34/128 step: 18
[2025-12-23 15:03:15,921] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 118/128 step: 60
[2025-12-23 15:03:16,006] m-LoRA: Adapter lora_gsm8k_67 epoch: 1/1 iteration: 76/128 step: 20
[2025-12-23 15:03:16,089] m-LoRA: Adapter lora_gsm8k_70 epoch: 1/1 iteration: 8/128 step: 3
[2025-12-23 15:03:16,914] m-LoRA: Adapter lora_gsm8k_71 loss: 1.418882966041565
[2025-12-23 15:03:17,044] m-LoRA: Adapter lora_gsm8k_69 loss: 0.6163488626480103
[2025-12-23 15:03:17,236] m-LoRA: Adapter lora_gsm8k_60 loss: 0.6662176251411438
[2025-12-23 15:03:17,514] m-LoRA: Adapter lora_gsm8k_67 loss: 0.7320234179496765
[2025-12-23 15:03:17,863] m-LoRA: Adapter lora_gsm8k_70 loss: 1.5765361785888672
[2025-12-23 15:03:18,344] m-LoRA: Adapter lora_gsm8k_71 epoch: 1/1 iteration: 16/128 step: 3
[2025-12-23 15:03:18,463] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 36/128 step: 19
[2025-12-23 15:03:18,549] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 120/128 step: 61
[2025-12-23 15:03:18,647] m-LoRA: Adapter lora_gsm8k_67 epoch: 1/1 iteration: 80/128 step: 21
[2025-12-23 15:03:19,437] m-LoRA: Adapter lora_gsm8k_70 epoch: 1/1 iteration: 12/128 step: 4
[2025-12-23 15:03:19,506] m-LoRA: Adapter lora_gsm8k_71 loss: 1.4920415878295898
[2025-12-23 15:03:19,513] m-LoRA: Adapter lora_gsm8k_69 loss: 0.6907187104225159
[2025-12-23 15:03:19,808] m-LoRA: Adapter lora_gsm8k_60 loss: 0.6335574984550476
[2025-12-23 15:03:20,096] m-LoRA: Adapter lora_gsm8k_67 loss: 0.6283684372901917
[2025-12-23 15:03:20,409] m-LoRA: Adapter lora_gsm8k_70 loss: 1.4703224897384644
[2025-12-23 15:03:20,790] m-LoRA: Adapter lora_gsm8k_71 epoch: 1/1 iteration: 24/128 step: 4
[2025-12-23 15:03:20,912] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 38/128 step: 20
[2025-12-23 15:03:21,001] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 122/128 step: 62
[2025-12-23 15:03:21,195] m-LoRA: Adapter lora_gsm8k_67 epoch: 1/1 iteration: 84/128 step: 22
[2025-12-23 15:03:22,288] m-LoRA: Adapter lora_gsm8k_71 loss: 1.2193942070007324
[2025-12-23 15:03:22,296] m-LoRA: Adapter lora_gsm8k_70 epoch: 1/1 iteration: 16/128 step: 5
[2025-12-23 15:03:22,322] m-LoRA: Adapter lora_gsm8k_69 loss: 0.5214198231697083
[2025-12-23 15:03:22,460] m-LoRA: Adapter lora_gsm8k_60 loss: 0.709693968296051
[2025-12-23 15:03:22,855] m-LoRA: Adapter lora_gsm8k_67 loss: 0.3986435830593109
[2025-12-23 15:03:23,118] m-LoRA: Adapter lora_gsm8k_70 loss: 1.4281001091003418
[2025-12-23 15:03:23,920] m-LoRA: Adapter lora_gsm8k_71 epoch: 1/1 iteration: 32/128 step: 5
[2025-12-23 15:03:24,068] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 40/128 step: 21
[2025-12-23 15:03:24,133] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 124/128 step: 63
[2025-12-23 15:03:24,203] m-LoRA: Adapter lora_gsm8k_67 epoch: 1/1 iteration: 88/128 step: 23
[2025-12-23 15:03:24,410] m-LoRA: Adapter lora_gsm8k_70 epoch: 1/1 iteration: 20/128 step: 6
[2025-12-23 15:03:25,116] m-LoRA: Adapter lora_gsm8k_71 loss: 1.3028556108474731
[2025-12-23 15:03:25,133] m-LoRA: Adapter lora_gsm8k_69 loss: 0.535809338092804
[2025-12-23 15:03:25,376] m-LoRA: Adapter lora_gsm8k_60 loss: 0.5322831869125366
[2025-12-23 15:03:25,701] m-LoRA: Adapter lora_gsm8k_67 loss: 0.599365234375
[2025-12-23 15:03:26,058] m-LoRA: Adapter lora_gsm8k_70 loss: 1.2459661960601807
[2025-12-23 15:03:26,472] m-LoRA: Adapter lora_gsm8k_71 epoch: 1/1 iteration: 40/128 step: 6
[2025-12-23 15:03:26,590] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 42/128 step: 22
[2025-12-23 15:03:26,681] m-LoRA: Adapter lora_gsm8k_60 epoch: 1/1 iteration: 126/128 step: 64
[2025-12-23 15:03:26,871] m-LoRA: Adapter lora_gsm8k_67 epoch: 1/1 iteration: 92/128 step: 24
[2025-12-23 15:03:27,174] m-LoRA: Adapter lora_gsm8k_70 epoch: 1/1 iteration: 24/128 step: 7
[2025-12-23 15:03:27,887] m-LoRA: Adapter lora_gsm8k_71 loss: 1.0993030071258545
[2025-12-23 15:03:27,904] m-LoRA: Adapter lora_gsm8k_69 loss: 0.4193940758705139
[2025-12-23 15:03:28,161] m-LoRA: Adapter lora_gsm8k_60 loss: 0.5797666311264038
[2025-12-23 15:03:28,440] m-LoRA: Adapter lora_gsm8k_67 loss: 0.4945446848869324
[2025-12-23 15:03:28,862] m-LoRA: Adapter lora_gsm8k_70 loss: 1.0514793395996094
[2025-12-23 15:03:29,371] m-LoRA: Adapter lora_gsm8k_71 epoch: 1/1 iteration: 48/128 step: 7
[2025-12-23 15:03:29,508] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 44/128 step: 23
[2025-12-23 15:03:29,601] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_60']
[2025-12-23 15:03:30,190] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_72']
[2025-12-23 15:03:30,357] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 15:03:30,457] m-LoRA: Adapter lora_gsm8k_67 epoch: 1/1 iteration: 96/128 step: 25
[2025-12-23 15:03:30,619] m-LoRA: Adapter lora_gsm8k_70 epoch: 1/1 iteration: 28/128 step: 8
[2025-12-23 15:03:31,242] m-LoRA: Adapter lora_gsm8k_71 loss: 1.125275731086731
[2025-12-23 15:03:31,298] m-LoRA: Adapter lora_gsm8k_69 loss: 0.552009105682373
[2025-12-23 15:03:31,488] m-LoRA: Adapter lora_gsm8k_72 loss: 2.5401217937469482
[2025-12-23 15:03:31,793] m-LoRA: Adapter lora_gsm8k_67 loss: 0.5625494718551636
[2025-12-23 15:03:32,135] m-LoRA: Adapter lora_gsm8k_70 loss: 0.9201151728630066
[2025-12-23 15:03:32,602] m-LoRA: Adapter lora_gsm8k_71 epoch: 1/1 iteration: 56/128 step: 8
[2025-12-23 15:03:32,740] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 46/128 step: 24
[2025-12-23 15:03:32,841] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 2/128 step: 2
[2025-12-23 15:03:32,987] m-LoRA: Adapter lora_gsm8k_67 epoch: 1/1 iteration: 100/128 step: 26
[2025-12-23 15:03:33,852] m-LoRA: Adapter lora_gsm8k_70 epoch: 1/1 iteration: 32/128 step: 9
[2025-12-23 15:03:33,935] m-LoRA: Adapter lora_gsm8k_71 loss: 0.9593896865844727
[2025-12-23 15:03:33,943] m-LoRA: Adapter lora_gsm8k_69 loss: 0.5753363966941833
[2025-12-23 15:03:34,081] m-LoRA: Adapter lora_gsm8k_72 loss: 2.0428829193115234
[2025-12-23 15:03:34,462] m-LoRA: Adapter lora_gsm8k_67 loss: 0.6334664821624756
[2025-12-23 15:03:34,884] m-LoRA: Adapter lora_gsm8k_70 loss: 1.0888476371765137
[2025-12-23 15:03:35,324] m-LoRA: Adapter lora_gsm8k_71 epoch: 1/1 iteration: 64/128 step: 9
[2025-12-23 15:03:35,452] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 48/128 step: 25
[2025-12-23 15:03:35,540] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 4/128 step: 3
[2025-12-23 15:03:35,617] m-LoRA: Adapter lora_gsm8k_67 epoch: 1/1 iteration: 104/128 step: 27
[2025-12-23 15:03:36,608] m-LoRA: Adapter lora_gsm8k_71 loss: 0.7106590867042542
[2025-12-23 15:03:36,616] m-LoRA: Adapter lora_gsm8k_70 epoch: 1/1 iteration: 36/128 step: 10
[2025-12-23 15:03:36,678] m-LoRA: Adapter lora_gsm8k_69 loss: 0.6454176306724548
[2025-12-23 15:03:36,766] m-LoRA: Adapter lora_gsm8k_72 loss: 2.1657588481903076
[2025-12-23 15:03:37,185] m-LoRA: Adapter lora_gsm8k_67 loss: 0.673254668712616
[2025-12-23 15:03:37,451] m-LoRA: Adapter lora_gsm8k_70 loss: 1.0011560916900635
[2025-12-23 15:03:38,083] m-LoRA: Adapter lora_gsm8k_71 epoch: 1/1 iteration: 72/128 step: 10
[2025-12-23 15:03:38,238] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 50/128 step: 26
[2025-12-23 15:03:38,326] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 6/128 step: 4
[2025-12-23 15:03:38,417] m-LoRA: Adapter lora_gsm8k_67 epoch: 1/1 iteration: 108/128 step: 28
[2025-12-23 15:03:38,635] m-LoRA: Adapter lora_gsm8k_70 epoch: 1/1 iteration: 40/128 step: 11
[2025-12-23 15:03:39,582] m-LoRA: Adapter lora_gsm8k_71 loss: 0.5647892355918884
[2025-12-23 15:03:39,598] m-LoRA: Adapter lora_gsm8k_69 loss: 0.5915332436561584
[2025-12-23 15:03:39,830] m-LoRA: Adapter lora_gsm8k_72 loss: 1.3998295068740845
[2025-12-23 15:03:40,092] m-LoRA: Adapter lora_gsm8k_67 loss: 0.4942445456981659
[2025-12-23 15:03:40,402] m-LoRA: Adapter lora_gsm8k_70 loss: 0.706790566444397
[2025-12-23 15:03:41,356] m-LoRA: Adapter lora_gsm8k_71 epoch: 1/1 iteration: 80/128 step: 11
[2025-12-23 15:03:41,547] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 52/128 step: 27
[2025-12-23 15:03:41,638] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 8/128 step: 5
[2025-12-23 15:03:41,713] m-LoRA: Adapter lora_gsm8k_67 epoch: 1/1 iteration: 112/128 step: 29
[2025-12-23 15:03:41,793] m-LoRA: Adapter lora_gsm8k_70 epoch: 1/1 iteration: 44/128 step: 12
[2025-12-23 15:03:43,009] m-LoRA: Adapter lora_gsm8k_71 loss: 0.4916479289531708
[2025-12-23 15:03:43,031] m-LoRA: Adapter lora_gsm8k_69 loss: 0.46893957257270813
[2025-12-23 15:03:43,216] m-LoRA: Adapter lora_gsm8k_72 loss: 1.3880934715270996
[2025-12-23 15:03:43,611] m-LoRA: Adapter lora_gsm8k_67 loss: 0.505422055721283
[2025-12-23 15:03:43,868] m-LoRA: Adapter lora_gsm8k_70 loss: 0.8612790107727051
[2025-12-23 15:03:44,908] m-LoRA: Adapter lora_gsm8k_71 epoch: 1/1 iteration: 88/128 step: 12
[2025-12-23 15:03:45,049] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 54/128 step: 28
[2025-12-23 15:03:45,130] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 10/128 step: 6
[2025-12-23 15:03:45,215] m-LoRA: Adapter lora_gsm8k_67 epoch: 1/1 iteration: 116/128 step: 30
[2025-12-23 15:03:45,290] m-LoRA: Adapter lora_gsm8k_70 epoch: 1/1 iteration: 48/128 step: 13
[2025-12-23 15:03:46,062] m-LoRA: Adapter lora_gsm8k_71 loss: 0.5115857720375061
[2025-12-23 15:03:46,190] m-LoRA: Adapter lora_gsm8k_69 loss: 0.5798767805099487
[2025-12-23 15:03:46,341] m-LoRA: Adapter lora_gsm8k_72 loss: 1.4126461744308472
[2025-12-23 15:03:46,764] m-LoRA: Adapter lora_gsm8k_67 loss: 0.5026777386665344
[2025-12-23 15:03:47,165] m-LoRA: Adapter lora_gsm8k_70 loss: 0.6091434955596924
[2025-12-23 15:03:47,340] m-LoRA: Adapter lora_gsm8k_71 epoch: 1/1 iteration: 96/128 step: 13
[2025-12-23 15:03:47,476] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 56/128 step: 29
[2025-12-23 15:03:47,574] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 12/128 step: 7
[2025-12-23 15:03:48,540] m-LoRA: Adapter lora_gsm8k_71 loss: 0.6439835429191589
[2025-12-23 15:03:48,547] m-LoRA: Adapter lora_gsm8k_67 epoch: 1/1 iteration: 120/128 step: 31
[2025-12-23 15:03:48,806] m-LoRA: Adapter lora_gsm8k_72 loss: 1.4163470268249512
[2025-12-23 15:03:48,808] m-LoRA: Adapter lora_gsm8k_70 epoch: 1/1 iteration: 52/128 step: 14
[2025-12-23 15:03:48,865] m-LoRA: Adapter lora_gsm8k_69 loss: 0.5220238566398621
[2025-12-23 15:03:49,222] m-LoRA: Adapter lora_gsm8k_67 loss: 0.5450180172920227
[2025-12-23 15:03:49,855] m-LoRA: Adapter lora_gsm8k_70 loss: 0.7755753993988037
[2025-12-23 15:03:49,976] m-LoRA: Adapter lora_gsm8k_71 epoch: 1/1 iteration: 104/128 step: 14
[2025-12-23 15:03:50,111] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 14/128 step: 8
[2025-12-23 15:03:50,208] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 58/128 step: 30
[2025-12-23 15:03:50,306] m-LoRA: Adapter lora_gsm8k_67 epoch: 1/1 iteration: 124/128 step: 32
[2025-12-23 15:03:51,197] m-LoRA: Adapter lora_gsm8k_70 epoch: 1/1 iteration: 56/128 step: 15
[2025-12-23 15:03:51,260] m-LoRA: Adapter lora_gsm8k_71 loss: 0.48600825667381287
[2025-12-23 15:03:51,271] m-LoRA: Adapter lora_gsm8k_72 loss: 1.0576894283294678
[2025-12-23 15:03:51,510] m-LoRA: Adapter lora_gsm8k_69 loss: 0.7191029191017151
[2025-12-23 15:03:51,850] m-LoRA: Adapter lora_gsm8k_67 loss: 0.6623302698135376
[2025-12-23 15:03:52,099] m-LoRA: Adapter lora_gsm8k_70 loss: 0.6666132211685181
[2025-12-23 15:03:52,813] m-LoRA: Adapter lora_gsm8k_71 epoch: 1/1 iteration: 112/128 step: 15
[2025-12-23 15:03:52,967] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 16/128 step: 9
[2025-12-23 15:03:53,070] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 60/128 step: 31
[2025-12-23 15:03:53,164] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_67']
[2025-12-23 15:03:53,476] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_73']
[2025-12-23 15:03:53,561] m-LoRA: Adapter lora_gsm8k_73 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 15:03:53,751] m-LoRA: Adapter lora_gsm8k_70 epoch: 1/1 iteration: 60/128 step: 16
[2025-12-23 15:03:54,098] m-LoRA: Adapter lora_gsm8k_71 loss: 0.49673813581466675
[2025-12-23 15:03:54,249] m-LoRA: Adapter lora_gsm8k_72 loss: 0.8329356908798218
[2025-12-23 15:03:54,508] m-LoRA: Adapter lora_gsm8k_69 loss: 0.49375462532043457
[2025-12-23 15:03:56,262] m-LoRA: Adapter lora_gsm8k_71 epoch: 1/1 iteration: 120/128 step: 16
[2025-12-23 15:03:56,544] m-LoRA: Adapter lora_gsm8k_73 loss: 2.3848893642425537
[2025-12-23 15:03:56,559] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 18/128 step: 10
[2025-12-23 15:03:56,651] m-LoRA: Adapter lora_gsm8k_70 loss: 0.5838462710380554
[2025-12-23 15:03:56,655] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 62/128 step: 32
[2025-12-23 15:03:57,532] m-LoRA: Adapter lora_gsm8k_71 loss: 0.5271151065826416
[2025-12-23 15:03:57,564] m-LoRA: Adapter lora_gsm8k_72 loss: 0.881159245967865
[2025-12-23 15:03:57,742] m-LoRA: Adapter lora_gsm8k_69 loss: 0.658205509185791
[2025-12-23 15:03:59,923] m-LoRA: Adapter lora_gsm8k_73 epoch: 1/1 iteration: 16/128 step: 2
[2025-12-23 15:04:00,172] m-LoRA: Adapter lora_gsm8k_70 epoch: 1/1 iteration: 64/128 step: 17
[2025-12-23 15:04:00,283] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_71']
[2025-12-23 15:04:00,756] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_74']
[2025-12-23 15:04:00,923] m-LoRA: Adapter lora_gsm8k_74 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 15:04:01,030] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 20/128 step: 11
[2025-12-23 15:04:01,090] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 64/128 step: 33
[2025-12-23 15:04:02,713] m-LoRA: Adapter lora_gsm8k_73 loss: 1.4444708824157715
[2025-12-23 15:04:02,749] m-LoRA: Adapter lora_gsm8k_70 loss: 0.5277708768844604
[2025-12-23 15:04:03,575] m-LoRA: Adapter lora_gsm8k_74 loss: 2.304445266723633
[2025-12-23 15:04:03,587] m-LoRA: Adapter lora_gsm8k_72 loss: 0.8992302417755127
[2025-12-23 15:04:03,749] m-LoRA: Adapter lora_gsm8k_69 loss: 0.6842054128646851
[2025-12-23 15:04:05,330] m-LoRA: Adapter lora_gsm8k_73 epoch: 1/1 iteration: 32/128 step: 3
[2025-12-23 15:04:05,569] m-LoRA: Adapter lora_gsm8k_70 epoch: 1/1 iteration: 68/128 step: 18
[2025-12-23 15:04:05,888] m-LoRA: Adapter lora_gsm8k_74 epoch: 1/1 iteration: 8/128 step: 2
[2025-12-23 15:04:06,059] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 22/128 step: 12
[2025-12-23 15:04:06,129] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 66/128 step: 34
[2025-12-23 15:04:07,559] m-LoRA: Adapter lora_gsm8k_73 loss: 1.3141727447509766
[2025-12-23 15:04:07,627] m-LoRA: Adapter lora_gsm8k_70 loss: 0.5392898321151733
[2025-12-23 15:04:08,353] m-LoRA: Adapter lora_gsm8k_74 loss: 1.4839622974395752
[2025-12-23 15:04:08,474] m-LoRA: Adapter lora_gsm8k_72 loss: 0.8482253551483154
[2025-12-23 15:04:08,570] m-LoRA: Adapter lora_gsm8k_69 loss: 0.6142758727073669
[2025-12-23 15:04:10,094] m-LoRA: Adapter lora_gsm8k_73 epoch: 1/1 iteration: 48/128 step: 4
[2025-12-23 15:04:10,343] m-LoRA: Adapter lora_gsm8k_70 epoch: 1/1 iteration: 72/128 step: 19
[2025-12-23 15:04:10,444] m-LoRA: Adapter lora_gsm8k_74 epoch: 1/1 iteration: 16/128 step: 3
[2025-12-23 15:04:10,632] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 24/128 step: 13
[2025-12-23 15:04:10,730] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 68/128 step: 35
[2025-12-23 15:04:12,670] m-LoRA: Adapter lora_gsm8k_73 loss: 1.194714069366455
[2025-12-23 15:04:12,706] m-LoRA: Adapter lora_gsm8k_70 loss: 0.5137967467308044
[2025-12-23 15:04:13,689] m-LoRA: Adapter lora_gsm8k_74 loss: 1.061288833618164
[2025-12-23 15:04:13,709] m-LoRA: Adapter lora_gsm8k_72 loss: 0.8191915154457092
[2025-12-23 15:04:13,865] m-LoRA: Adapter lora_gsm8k_69 loss: 0.5638935565948486
[2025-12-23 15:04:15,609] m-LoRA: Adapter lora_gsm8k_73 epoch: 1/1 iteration: 64/128 step: 5
[2025-12-23 15:04:15,846] m-LoRA: Adapter lora_gsm8k_70 epoch: 1/1 iteration: 76/128 step: 20
[2025-12-23 15:04:16,227] m-LoRA: Adapter lora_gsm8k_74 epoch: 1/1 iteration: 24/128 step: 4
[2025-12-23 15:04:16,383] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 26/128 step: 14
[2025-12-23 15:04:16,438] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 70/128 step: 36
[2025-12-23 15:04:18,044] m-LoRA: Adapter lora_gsm8k_73 loss: 1.2337638139724731
[2025-12-23 15:04:18,172] m-LoRA: Adapter lora_gsm8k_70 loss: 0.7320550680160522
[2025-12-23 15:04:18,922] m-LoRA: Adapter lora_gsm8k_74 loss: 1.2017273902893066
[2025-12-23 15:04:18,940] m-LoRA: Adapter lora_gsm8k_72 loss: 0.8238791823387146
[2025-12-23 15:04:19,129] m-LoRA: Adapter lora_gsm8k_69 loss: 0.5644216537475586
[2025-12-23 15:04:20,785] m-LoRA: Adapter lora_gsm8k_73 epoch: 1/1 iteration: 80/128 step: 6
[2025-12-23 15:04:21,017] m-LoRA: Adapter lora_gsm8k_70 epoch: 1/1 iteration: 80/128 step: 21
[2025-12-23 15:04:21,172] m-LoRA: Adapter lora_gsm8k_74 epoch: 1/1 iteration: 32/128 step: 5
[2025-12-23 15:04:21,330] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 28/128 step: 15
[2025-12-23 15:04:21,476] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 72/128 step: 37
[2025-12-23 15:04:22,987] m-LoRA: Adapter lora_gsm8k_73 loss: 1.3252261877059937
[2025-12-23 15:04:23,299] m-LoRA: Adapter lora_gsm8k_70 loss: 0.4397984445095062
[2025-12-23 15:04:23,915] m-LoRA: Adapter lora_gsm8k_74 loss: 1.4233523607254028
[2025-12-23 15:04:23,928] m-LoRA: Adapter lora_gsm8k_72 loss: 0.7478631138801575
[2025-12-23 15:04:24,082] m-LoRA: Adapter lora_gsm8k_69 loss: 0.6685888171195984
[2025-12-23 15:04:25,501] m-LoRA: Adapter lora_gsm8k_73 epoch: 1/1 iteration: 96/128 step: 7
[2025-12-23 15:04:25,734] m-LoRA: Adapter lora_gsm8k_70 epoch: 1/1 iteration: 84/128 step: 22
[2025-12-23 15:04:26,036] m-LoRA: Adapter lora_gsm8k_74 epoch: 1/1 iteration: 40/128 step: 6
[2025-12-23 15:04:26,193] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 30/128 step: 16
[2025-12-23 15:04:26,273] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 74/128 step: 38
[2025-12-23 15:04:27,984] m-LoRA: Adapter lora_gsm8k_73 loss: 1.0484164953231812
[2025-12-23 15:04:28,279] m-LoRA: Adapter lora_gsm8k_70 loss: 0.482287734746933
[2025-12-23 15:04:28,900] m-LoRA: Adapter lora_gsm8k_74 loss: 1.136902093887329
[2025-12-23 15:04:28,916] m-LoRA: Adapter lora_gsm8k_72 loss: 0.5912132859230042
[2025-12-23 15:04:29,062] m-LoRA: Adapter lora_gsm8k_69 loss: 0.6042166948318481
[2025-12-23 15:04:30,768] m-LoRA: Adapter lora_gsm8k_73 epoch: 1/1 iteration: 112/128 step: 8
[2025-12-23 15:04:30,990] m-LoRA: Adapter lora_gsm8k_70 epoch: 1/1 iteration: 88/128 step: 23
[2025-12-23 15:04:31,133] m-LoRA: Adapter lora_gsm8k_74 epoch: 1/1 iteration: 48/128 step: 7
[2025-12-23 15:04:31,328] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 32/128 step: 17
[2025-12-23 15:04:31,452] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 76/128 step: 39
[2025-12-23 15:04:33,551] m-LoRA: Adapter lora_gsm8k_73 loss: 0.9376310706138611
[2025-12-23 15:04:33,584] m-LoRA: Adapter lora_gsm8k_70 loss: 0.6619015336036682
[2025-12-23 15:04:34,490] m-LoRA: Adapter lora_gsm8k_74 loss: 0.9687334299087524
[2025-12-23 15:04:34,502] m-LoRA: Adapter lora_gsm8k_72 loss: 0.6256244778633118
[2025-12-23 15:04:34,742] m-LoRA: Adapter lora_gsm8k_69 loss: 0.593313992023468
[2025-12-23 15:04:36,676] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_73']
[2025-12-23 15:04:37,069] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_75']
[2025-12-23 15:04:37,108] m-LoRA: Adapter lora_gsm8k_75 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 15:04:37,218] m-LoRA: Adapter lora_gsm8k_70 epoch: 1/1 iteration: 92/128 step: 24
[2025-12-23 15:04:37,335] m-LoRA: Adapter lora_gsm8k_74 epoch: 1/1 iteration: 56/128 step: 8
[2025-12-23 15:04:37,496] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 34/128 step: 18
[2025-12-23 15:04:37,601] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 78/128 step: 40
[2025-12-23 15:04:38,419] m-LoRA: Adapter lora_gsm8k_75 loss: 2.501110076904297
[2025-12-23 15:04:38,723] m-LoRA: Adapter lora_gsm8k_70 loss: 0.6133566498756409
[2025-12-23 15:04:39,203] m-LoRA: Adapter lora_gsm8k_74 loss: 1.3464469909667969
[2025-12-23 15:04:39,500] m-LoRA: Adapter lora_gsm8k_72 loss: 0.6529715657234192
[2025-12-23 15:04:39,576] m-LoRA: Adapter lora_gsm8k_69 loss: 0.5160070657730103
[2025-12-23 15:04:39,766] m-LoRA: Adapter lora_gsm8k_75 epoch: 1/1 iteration: 8/128 step: 2
[2025-12-23 15:04:40,060] m-LoRA: Adapter lora_gsm8k_70 epoch: 1/1 iteration: 96/128 step: 25
[2025-12-23 15:04:41,080] m-LoRA: Adapter lora_gsm8k_74 epoch: 1/1 iteration: 64/128 step: 9
[2025-12-23 15:04:41,137] m-LoRA: Adapter lora_gsm8k_75 loss: 1.759601354598999
[2025-12-23 15:04:41,370] m-LoRA: Adapter lora_gsm8k_70 loss: 0.6066234707832336
[2025-12-23 15:04:41,375] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 36/128 step: 19
[2025-12-23 15:04:41,477] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 80/128 step: 41
[2025-12-23 15:04:42,204] m-LoRA: Adapter lora_gsm8k_74 loss: 1.145943284034729
[2025-12-23 15:04:42,480] m-LoRA: Adapter lora_gsm8k_72 loss: 0.46473371982574463
[2025-12-23 15:04:42,750] m-LoRA: Adapter lora_gsm8k_69 loss: 0.5358332991600037
[2025-12-23 15:04:42,752] m-LoRA: Adapter lora_gsm8k_75 epoch: 1/1 iteration: 16/128 step: 3
[2025-12-23 15:04:42,903] m-LoRA: Adapter lora_gsm8k_70 epoch: 1/1 iteration: 100/128 step: 26
[2025-12-23 15:04:43,871] m-LoRA: Adapter lora_gsm8k_75 loss: 1.666387915611267
[2025-12-23 15:04:43,876] m-LoRA: Adapter lora_gsm8k_74 epoch: 1/1 iteration: 72/128 step: 10
[2025-12-23 15:04:44,204] m-LoRA: Adapter lora_gsm8k_70 loss: 0.6188420057296753
[2025-12-23 15:04:44,208] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 38/128 step: 20
[2025-12-23 15:04:44,338] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 82/128 step: 42
[2025-12-23 15:04:45,500] m-LoRA: Adapter lora_gsm8k_75 epoch: 1/1 iteration: 24/128 step: 4
[2025-12-23 15:04:45,667] m-LoRA: Adapter lora_gsm8k_74 loss: 0.9813796281814575
[2025-12-23 15:04:45,673] m-LoRA: Adapter lora_gsm8k_70 epoch: 1/1 iteration: 104/128 step: 27
[2025-12-23 15:04:45,705] m-LoRA: Adapter lora_gsm8k_72 loss: 0.6639471650123596
[2025-12-23 15:04:45,970] m-LoRA: Adapter lora_gsm8k_69 loss: 0.6854772567749023
[2025-12-23 15:04:46,652] m-LoRA: Adapter lora_gsm8k_75 loss: 1.4083119630813599
[2025-12-23 15:04:47,025] m-LoRA: Adapter lora_gsm8k_70 loss: 0.5674458742141724
[2025-12-23 15:04:47,238] m-LoRA: Adapter lora_gsm8k_74 epoch: 1/1 iteration: 80/128 step: 11
[2025-12-23 15:04:47,398] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 40/128 step: 21
[2025-12-23 15:04:47,499] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 84/128 step: 43
[2025-12-23 15:04:48,630] m-LoRA: Adapter lora_gsm8k_75 epoch: 1/1 iteration: 32/128 step: 5
[2025-12-23 15:04:48,780] m-LoRA: Adapter lora_gsm8k_74 loss: 0.8455477356910706
[2025-12-23 15:04:48,787] m-LoRA: Adapter lora_gsm8k_70 epoch: 1/1 iteration: 108/128 step: 28
[2025-12-23 15:04:48,838] m-LoRA: Adapter lora_gsm8k_72 loss: 0.4449070394039154
[2025-12-23 15:04:48,932] m-LoRA: Adapter lora_gsm8k_69 loss: 0.7878702282905579
[2025-12-23 15:04:49,700] m-LoRA: Adapter lora_gsm8k_75 loss: 1.4307804107666016
[2025-12-23 15:04:50,065] m-LoRA: Adapter lora_gsm8k_70 loss: 0.504788339138031
[2025-12-23 15:04:50,335] m-LoRA: Adapter lora_gsm8k_74 epoch: 1/1 iteration: 88/128 step: 12
[2025-12-23 15:04:50,487] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 42/128 step: 22
[2025-12-23 15:04:50,581] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 86/128 step: 44
[2025-12-23 15:04:51,486] m-LoRA: Adapter lora_gsm8k_75 epoch: 1/1 iteration: 40/128 step: 6
[2025-12-23 15:04:51,646] m-LoRA: Adapter lora_gsm8k_74 loss: 0.8622547388076782
[2025-12-23 15:04:51,653] m-LoRA: Adapter lora_gsm8k_70 epoch: 1/1 iteration: 112/128 step: 29
[2025-12-23 15:04:51,693] m-LoRA: Adapter lora_gsm8k_72 loss: 0.7204990386962891
[2025-12-23 15:04:51,772] m-LoRA: Adapter lora_gsm8k_69 loss: 0.7319545149803162
[2025-12-23 15:04:52,619] m-LoRA: Adapter lora_gsm8k_75 loss: 1.3916841745376587
[2025-12-23 15:04:52,880] m-LoRA: Adapter lora_gsm8k_70 loss: 0.6085483431816101
[2025-12-23 15:04:52,989] m-LoRA: Adapter lora_gsm8k_74 epoch: 1/1 iteration: 96/128 step: 13
[2025-12-23 15:04:53,122] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 44/128 step: 23
[2025-12-23 15:04:53,226] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 88/128 step: 45
[2025-12-23 15:04:54,357] m-LoRA: Adapter lora_gsm8k_74 loss: 0.8197646141052246
[2025-12-23 15:04:54,363] m-LoRA: Adapter lora_gsm8k_75 epoch: 1/1 iteration: 48/128 step: 7
[2025-12-23 15:04:54,551] m-LoRA: Adapter lora_gsm8k_69 loss: 0.4868331551551819
[2025-12-23 15:04:54,553] m-LoRA: Adapter lora_gsm8k_70 epoch: 1/1 iteration: 116/128 step: 30
[2025-12-23 15:04:54,619] m-LoRA: Adapter lora_gsm8k_72 loss: 0.5272781252861023
[2025-12-23 15:04:55,487] m-LoRA: Adapter lora_gsm8k_75 loss: 1.316912055015564
[2025-12-23 15:04:55,896] m-LoRA: Adapter lora_gsm8k_70 loss: 0.5360475182533264
[2025-12-23 15:04:55,901] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 90/128 step: 46
[2025-12-23 15:04:56,061] m-LoRA: Adapter lora_gsm8k_74 epoch: 1/1 iteration: 104/128 step: 14
[2025-12-23 15:04:56,196] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 46/128 step: 24
[2025-12-23 15:04:56,666] m-LoRA: Adapter lora_gsm8k_69 loss: 0.6213281750679016
[2025-12-23 15:04:56,770] m-LoRA: Adapter lora_gsm8k_75 epoch: 1/1 iteration: 56/128 step: 8
[2025-12-23 15:04:57,515] m-LoRA: Adapter lora_gsm8k_70 epoch: 1/1 iteration: 120/128 step: 31
[2025-12-23 15:04:57,668] m-LoRA: Adapter lora_gsm8k_74 loss: 0.7610391974449158
[2025-12-23 15:04:57,675] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 92/128 step: 47
[2025-12-23 15:04:57,714] m-LoRA: Adapter lora_gsm8k_72 loss: 0.46381014585494995
[2025-12-23 15:04:58,267] m-LoRA: Adapter lora_gsm8k_75 loss: 1.3292375802993774
[2025-12-23 15:04:58,575] m-LoRA: Adapter lora_gsm8k_70 loss: 0.5817304849624634
[2025-12-23 15:04:58,688] m-LoRA: Adapter lora_gsm8k_69 loss: 0.6245161294937134
[2025-12-23 15:04:59,010] m-LoRA: Adapter lora_gsm8k_74 epoch: 1/1 iteration: 112/128 step: 15
[2025-12-23 15:04:59,168] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 48/128 step: 25
[2025-12-23 15:04:59,593] m-LoRA: Adapter lora_gsm8k_75 epoch: 1/1 iteration: 64/128 step: 9
[2025-12-23 15:05:00,335] m-LoRA: Adapter lora_gsm8k_70 epoch: 1/1 iteration: 124/128 step: 32
[2025-12-23 15:05:00,460] m-LoRA: Adapter lora_gsm8k_74 loss: 0.5839813351631165
[2025-12-23 15:05:00,466] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 94/128 step: 48
[2025-12-23 15:05:00,504] m-LoRA: Adapter lora_gsm8k_72 loss: 0.5441902875900269
[2025-12-23 15:05:01,116] m-LoRA: Adapter lora_gsm8k_75 loss: 1.426693081855774
[2025-12-23 15:05:01,422] m-LoRA: Adapter lora_gsm8k_70 loss: 0.6449339389801025
[2025-12-23 15:05:01,577] m-LoRA: Adapter lora_gsm8k_69 loss: 0.6004994511604309
[2025-12-23 15:05:01,866] m-LoRA: Adapter lora_gsm8k_74 epoch: 1/1 iteration: 120/128 step: 16
[2025-12-23 15:05:02,004] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 50/128 step: 26
[2025-12-23 15:05:02,459] m-LoRA: Adapter lora_gsm8k_75 epoch: 1/1 iteration: 72/128 step: 10
[2025-12-23 15:05:03,151] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_70']
[2025-12-23 15:05:03,826] m-LoRA: Adapter lora_gsm8k_74 loss: 0.5985929369926453
[2025-12-23 15:05:03,894] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_76']
[2025-12-23 15:05:04,192] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 15:05:04,249] m-LoRA: Adapter lora_gsm8k_72 loss: 0.7203645706176758
[2025-12-23 15:05:04,251] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 96/128 step: 49
[2025-12-23 15:05:04,290] m-LoRA: Adapter lora_gsm8k_75 loss: 1.2214707136154175
[2025-12-23 15:05:05,058] m-LoRA: Adapter lora_gsm8k_76 loss: 2.3789167404174805
[2025-12-23 15:05:05,308] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_74']
[2025-12-23 15:05:05,770] m-LoRA: Adapter lora_gsm8k_69 loss: 0.5551806092262268
[2025-12-23 15:05:05,773] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_77']
[2025-12-23 15:05:05,905] m-LoRA: Adapter lora_gsm8k_77 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 15:05:06,031] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 52/128 step: 27
[2025-12-23 15:05:06,283] m-LoRA: Adapter lora_gsm8k_75 epoch: 1/1 iteration: 80/128 step: 11
[2025-12-23 15:05:06,399] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 2/128 step: 2
[2025-12-23 15:05:07,028] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 98/128 step: 50
[2025-12-23 15:05:07,085] m-LoRA: Adapter lora_gsm8k_77 loss: 2.4068329334259033
[2025-12-23 15:05:07,098] m-LoRA: Adapter lora_gsm8k_72 loss: 0.5002501606941223
[2025-12-23 15:05:07,872] m-LoRA: Adapter lora_gsm8k_75 loss: 1.174294114112854
[2025-12-23 15:05:08,090] m-LoRA: Adapter lora_gsm8k_76 loss: 1.7737958431243896
[2025-12-23 15:05:08,220] m-LoRA: Adapter lora_gsm8k_69 loss: 0.6112825870513916
[2025-12-23 15:05:08,411] m-LoRA: Adapter lora_gsm8k_77 epoch: 1/1 iteration: 8/128 step: 2
[2025-12-23 15:05:08,513] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 54/128 step: 28
[2025-12-23 15:05:09,904] m-LoRA: Adapter lora_gsm8k_77 loss: 1.3590337038040161
[2025-12-23 15:05:09,912] m-LoRA: Adapter lora_gsm8k_75 epoch: 1/1 iteration: 88/128 step: 12
[2025-12-23 15:05:10,100] m-LoRA: Adapter lora_gsm8k_72 loss: 0.6745743751525879
[2025-12-23 15:05:10,103] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 4/128 step: 3
[2025-12-23 15:05:10,206] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 100/128 step: 51
[2025-12-23 15:05:11,956] m-LoRA: Adapter lora_gsm8k_77 epoch: 1/1 iteration: 16/128 step: 3
[2025-12-23 15:05:12,105] m-LoRA: Adapter lora_gsm8k_75 loss: 0.9772316813468933
[2025-12-23 15:05:12,114] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 56/128 step: 29
[2025-12-23 15:05:12,140] m-LoRA: Adapter lora_gsm8k_76 loss: 1.384173035621643
[2025-12-23 15:05:12,152] m-LoRA: Adapter lora_gsm8k_69 loss: 0.6744100451469421
[2025-12-23 15:05:13,201] m-LoRA: Adapter lora_gsm8k_77 loss: 1.2848248481750488
[2025-12-23 15:05:13,467] m-LoRA: Adapter lora_gsm8k_72 loss: 0.6608501672744751
[2025-12-23 15:05:13,863] m-LoRA: Adapter lora_gsm8k_75 epoch: 1/1 iteration: 96/128 step: 13
[2025-12-23 15:05:14,010] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 6/128 step: 4
[2025-12-23 15:05:14,115] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 102/128 step: 52
[2025-12-23 15:05:15,281] m-LoRA: Adapter lora_gsm8k_77 epoch: 1/1 iteration: 24/128 step: 4
[2025-12-23 15:05:15,403] m-LoRA: Adapter lora_gsm8k_75 loss: 1.0072882175445557
[2025-12-23 15:05:15,556] m-LoRA: Adapter lora_gsm8k_76 loss: 1.125794529914856
[2025-12-23 15:05:15,559] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 58/128 step: 30
[2025-12-23 15:05:15,732] m-LoRA: Adapter lora_gsm8k_69 loss: 0.5609334707260132
[2025-12-23 15:05:16,431] m-LoRA: Adapter lora_gsm8k_77 loss: 1.3893146514892578
[2025-12-23 15:05:16,665] m-LoRA: Adapter lora_gsm8k_72 loss: 0.5490682125091553
[2025-12-23 15:05:16,838] m-LoRA: Adapter lora_gsm8k_75 epoch: 1/1 iteration: 104/128 step: 14
[2025-12-23 15:05:16,956] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 8/128 step: 5
[2025-12-23 15:05:17,052] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 104/128 step: 53
[2025-12-23 15:05:18,161] m-LoRA: Adapter lora_gsm8k_77 epoch: 1/1 iteration: 32/128 step: 5
[2025-12-23 15:05:18,283] m-LoRA: Adapter lora_gsm8k_75 loss: 1.232521653175354
[2025-12-23 15:05:18,290] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 60/128 step: 31
[2025-12-23 15:05:18,339] m-LoRA: Adapter lora_gsm8k_76 loss: 1.5322588682174683
[2025-12-23 15:05:18,342] m-LoRA: Adapter lora_gsm8k_69 loss: 0.5424140691757202
[2025-12-23 15:05:19,231] m-LoRA: Adapter lora_gsm8k_77 loss: 1.3771568536758423
[2025-12-23 15:05:19,426] m-LoRA: Adapter lora_gsm8k_72 loss: 0.5532926917076111
[2025-12-23 15:05:19,542] m-LoRA: Adapter lora_gsm8k_75 epoch: 1/1 iteration: 112/128 step: 15
[2025-12-23 15:05:19,658] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 10/128 step: 6
[2025-12-23 15:05:19,750] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 106/128 step: 54
[2025-12-23 15:05:21,024] m-LoRA: Adapter lora_gsm8k_75 loss: 0.9105616807937622
[2025-12-23 15:05:21,032] m-LoRA: Adapter lora_gsm8k_77 epoch: 1/1 iteration: 40/128 step: 6
[2025-12-23 15:05:21,289] m-LoRA: Adapter lora_gsm8k_76 loss: 1.6587790250778198
[2025-12-23 15:05:21,291] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 62/128 step: 32
[2025-12-23 15:05:21,338] m-LoRA: Adapter lora_gsm8k_69 loss: 0.5538061261177063
[2025-12-23 15:05:22,994] m-LoRA: Adapter lora_gsm8k_77 loss: 1.2489815950393677
[2025-12-23 15:05:23,001] m-LoRA: Adapter lora_gsm8k_75 epoch: 1/1 iteration: 120/128 step: 16
[2025-12-23 15:05:23,206] m-LoRA: Adapter lora_gsm8k_72 loss: 0.7877809405326843
[2025-12-23 15:05:23,208] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 12/128 step: 7
[2025-12-23 15:05:23,333] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 108/128 step: 55
[2025-12-23 15:05:24,724] m-LoRA: Adapter lora_gsm8k_77 epoch: 1/1 iteration: 48/128 step: 7
[2025-12-23 15:05:24,877] m-LoRA: Adapter lora_gsm8k_75 loss: 1.1298466920852661
[2025-12-23 15:05:24,883] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 64/128 step: 33
[2025-12-23 15:05:24,939] m-LoRA: Adapter lora_gsm8k_76 loss: 1.2890678644180298
[2025-12-23 15:05:24,944] m-LoRA: Adapter lora_gsm8k_69 loss: 0.8490374088287354
[2025-12-23 15:05:25,875] m-LoRA: Adapter lora_gsm8k_77 loss: 1.2765851020812988
[2025-12-23 15:05:26,045] m-LoRA: Adapter lora_gsm8k_72 loss: 0.754141628742218
[2025-12-23 15:05:26,142] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_75']
[2025-12-23 15:05:26,308] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_78']
[2025-12-23 15:05:26,476] m-LoRA: Adapter lora_gsm8k_78 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 15:05:26,599] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 14/128 step: 8
[2025-12-23 15:05:26,673] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 110/128 step: 56
[2025-12-23 15:05:27,834] m-LoRA: Adapter lora_gsm8k_78 loss: 2.560546875
[2025-12-23 15:05:27,840] m-LoRA: Adapter lora_gsm8k_77 epoch: 1/1 iteration: 56/128 step: 8
[2025-12-23 15:05:28,006] m-LoRA: Adapter lora_gsm8k_76 loss: 1.6405285596847534
[2025-12-23 15:05:28,009] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 66/128 step: 34
[2025-12-23 15:05:28,073] m-LoRA: Adapter lora_gsm8k_69 loss: 0.4416297376155853
[2025-12-23 15:05:29,515] m-LoRA: Adapter lora_gsm8k_77 loss: 1.137600302696228
[2025-12-23 15:05:29,521] m-LoRA: Adapter lora_gsm8k_78 epoch: 1/1 iteration: 8/128 step: 2
[2025-12-23 15:05:29,680] m-LoRA: Adapter lora_gsm8k_72 loss: 0.6040819883346558
[2025-12-23 15:05:29,682] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 112/128 step: 57
[2025-12-23 15:05:29,780] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 16/128 step: 9
[2025-12-23 15:05:31,069] m-LoRA: Adapter lora_gsm8k_77 epoch: 1/1 iteration: 64/128 step: 9
[2025-12-23 15:05:31,216] m-LoRA: Adapter lora_gsm8k_78 loss: 1.6022439002990723
[2025-12-23 15:05:31,222] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 68/128 step: 35
[2025-12-23 15:05:31,245] m-LoRA: Adapter lora_gsm8k_76 loss: 1.1776076555252075
[2025-12-23 15:05:31,340] m-LoRA: Adapter lora_gsm8k_69 loss: 0.5962161421775818
[2025-12-23 15:05:32,594] m-LoRA: Adapter lora_gsm8k_77 loss: 0.810785710811615
[2025-12-23 15:05:32,775] m-LoRA: Adapter lora_gsm8k_72 loss: 0.5779328346252441
[2025-12-23 15:05:32,777] m-LoRA: Adapter lora_gsm8k_78 epoch: 1/1 iteration: 16/128 step: 3
[2025-12-23 15:05:32,958] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 18/128 step: 10
[2025-12-23 15:05:33,055] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 114/128 step: 58
[2025-12-23 15:05:34,741] m-LoRA: Adapter lora_gsm8k_78 loss: 1.1084561347961426
[2025-12-23 15:05:34,749] m-LoRA: Adapter lora_gsm8k_77 epoch: 1/1 iteration: 72/128 step: 10
[2025-12-23 15:05:35,023] m-LoRA: Adapter lora_gsm8k_76 loss: 1.2359427213668823
[2025-12-23 15:05:35,026] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 70/128 step: 36
[2025-12-23 15:05:35,059] m-LoRA: Adapter lora_gsm8k_69 loss: 0.5167229175567627
[2025-12-23 15:05:36,815] m-LoRA: Adapter lora_gsm8k_77 loss: 0.7939929366111755
[2025-12-23 15:05:36,823] m-LoRA: Adapter lora_gsm8k_78 epoch: 1/1 iteration: 24/128 step: 4
[2025-12-23 15:05:37,000] m-LoRA: Adapter lora_gsm8k_72 loss: 0.7010469436645508
[2025-12-23 15:05:37,003] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 20/128 step: 11
[2025-12-23 15:05:37,104] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 116/128 step: 59
[2025-12-23 15:05:38,735] m-LoRA: Adapter lora_gsm8k_78 loss: 1.2649487257003784
[2025-12-23 15:05:38,741] m-LoRA: Adapter lora_gsm8k_77 epoch: 1/1 iteration: 80/128 step: 11
[2025-12-23 15:05:38,971] m-LoRA: Adapter lora_gsm8k_76 loss: 1.1385679244995117
[2025-12-23 15:05:38,974] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 72/128 step: 37
[2025-12-23 15:05:39,027] m-LoRA: Adapter lora_gsm8k_69 loss: 0.8085035681724548
[2025-12-23 15:05:40,386] m-LoRA: Adapter lora_gsm8k_77 loss: 0.9649516940116882
[2025-12-23 15:05:40,392] m-LoRA: Adapter lora_gsm8k_78 epoch: 1/1 iteration: 32/128 step: 5
[2025-12-23 15:05:40,606] m-LoRA: Adapter lora_gsm8k_72 loss: 0.618442952632904
[2025-12-23 15:05:40,609] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 22/128 step: 12
[2025-12-23 15:05:40,696] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 118/128 step: 60
[2025-12-23 15:05:41,948] m-LoRA: Adapter lora_gsm8k_77 epoch: 1/1 iteration: 88/128 step: 12
[2025-12-23 15:05:42,120] m-LoRA: Adapter lora_gsm8k_78 loss: 1.2049797773361206
[2025-12-23 15:05:42,126] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 74/128 step: 38
[2025-12-23 15:05:42,170] m-LoRA: Adapter lora_gsm8k_76 loss: 0.9539031982421875
[2025-12-23 15:05:42,174] m-LoRA: Adapter lora_gsm8k_69 loss: 0.6839243769645691
[2025-12-23 15:05:43,202] m-LoRA: Adapter lora_gsm8k_77 loss: 0.6955538988113403
[2025-12-23 15:05:43,349] m-LoRA: Adapter lora_gsm8k_72 loss: 0.455620676279068
[2025-12-23 15:05:43,484] m-LoRA: Adapter lora_gsm8k_78 epoch: 1/1 iteration: 40/128 step: 6
[2025-12-23 15:05:43,610] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 24/128 step: 13
[2025-12-23 15:05:43,673] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 120/128 step: 61
[2025-12-23 15:05:44,882] m-LoRA: Adapter lora_gsm8k_77 epoch: 1/1 iteration: 96/128 step: 13
[2025-12-23 15:05:45,007] m-LoRA: Adapter lora_gsm8k_78 loss: 1.1091376543045044
[2025-12-23 15:05:45,014] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 76/128 step: 39
[2025-12-23 15:05:45,038] m-LoRA: Adapter lora_gsm8k_76 loss: 0.8846701979637146
[2025-12-23 15:05:45,187] m-LoRA: Adapter lora_gsm8k_69 loss: 0.6071501970291138
[2025-12-23 15:05:45,952] m-LoRA: Adapter lora_gsm8k_77 loss: 0.8573737144470215
[2025-12-23 15:05:46,133] m-LoRA: Adapter lora_gsm8k_72 loss: 0.52675861120224
[2025-12-23 15:05:46,347] m-LoRA: Adapter lora_gsm8k_78 epoch: 1/1 iteration: 48/128 step: 7
[2025-12-23 15:05:46,492] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 26/128 step: 14
[2025-12-23 15:05:46,581] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 122/128 step: 62
[2025-12-23 15:05:47,640] m-LoRA: Adapter lora_gsm8k_78 loss: 0.9042762517929077
[2025-12-23 15:05:47,646] m-LoRA: Adapter lora_gsm8k_77 epoch: 1/1 iteration: 104/128 step: 14
[2025-12-23 15:05:47,902] m-LoRA: Adapter lora_gsm8k_76 loss: 0.7347609996795654
[2025-12-23 15:05:47,904] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 78/128 step: 40
[2025-12-23 15:05:47,963] m-LoRA: Adapter lora_gsm8k_69 loss: 0.6641131639480591
[2025-12-23 15:05:49,410] m-LoRA: Adapter lora_gsm8k_77 loss: 0.6628885269165039
[2025-12-23 15:05:49,417] m-LoRA: Adapter lora_gsm8k_78 epoch: 1/1 iteration: 56/128 step: 8
[2025-12-23 15:05:49,636] m-LoRA: Adapter lora_gsm8k_72 loss: 0.6399900317192078
[2025-12-23 15:05:49,640] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 28/128 step: 15
[2025-12-23 15:05:49,771] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 124/128 step: 63
[2025-12-23 15:05:51,083] m-LoRA: Adapter lora_gsm8k_77 epoch: 1/1 iteration: 112/128 step: 15
[2025-12-23 15:05:51,235] m-LoRA: Adapter lora_gsm8k_78 loss: 0.924314558506012
[2025-12-23 15:05:51,242] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 80/128 step: 41
[2025-12-23 15:05:51,292] m-LoRA: Adapter lora_gsm8k_76 loss: 0.800642192363739
[2025-12-23 15:05:51,296] m-LoRA: Adapter lora_gsm8k_69 loss: 0.5639258623123169
[2025-12-23 15:05:52,053] m-LoRA: Adapter lora_gsm8k_77 loss: 0.6808537840843201
[2025-12-23 15:05:52,283] m-LoRA: Adapter lora_gsm8k_72 loss: 0.7459145188331604
[2025-12-23 15:05:52,649] m-LoRA: Adapter lora_gsm8k_78 epoch: 1/1 iteration: 64/128 step: 9
[2025-12-23 15:05:52,800] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 30/128 step: 16
[2025-12-23 15:05:52,904] m-LoRA: Adapter lora_gsm8k_69 epoch: 1/1 iteration: 126/128 step: 64
[2025-12-23 15:05:53,342] m-LoRA: Adapter lora_gsm8k_77 epoch: 1/1 iteration: 120/128 step: 16
[2025-12-23 15:05:54,015] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 82/128 step: 42
[2025-12-23 15:05:54,058] m-LoRA: Adapter lora_gsm8k_78 loss: 0.7777771949768066
[2025-12-23 15:05:54,204] m-LoRA: Adapter lora_gsm8k_76 loss: 0.6689114570617676
[2025-12-23 15:05:54,456] m-LoRA: Adapter lora_gsm8k_69 loss: 0.7248470783233643
[2025-12-23 15:05:55,034] m-LoRA: Adapter lora_gsm8k_77 loss: 0.6136419773101807
[2025-12-23 15:05:55,324] m-LoRA: Adapter lora_gsm8k_72 loss: 0.6346271634101868
[2025-12-23 15:05:55,461] m-LoRA: Adapter lora_gsm8k_78 epoch: 1/1 iteration: 72/128 step: 10
[2025-12-23 15:05:55,606] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 32/128 step: 17
[2025-12-23 15:05:55,700] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_69']
[2025-12-23 15:05:56,028] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_79']
[2025-12-23 15:05:56,070] m-LoRA: Adapter lora_gsm8k_79 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 15:05:56,895] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_77']
[2025-12-23 15:05:57,133] m-LoRA: Adapter lora_gsm8k_78 loss: 0.6230508089065552
[2025-12-23 15:05:57,139] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_80']
[2025-12-23 15:05:57,370] m-LoRA: Adapter lora_gsm8k_80 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 15:05:57,473] m-LoRA: Adapter lora_gsm8k_76 loss: 0.7084099650382996
[2025-12-23 15:05:57,475] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 84/128 step: 43
[2025-12-23 15:05:57,682] m-LoRA: Adapter lora_gsm8k_79 loss: 2.3332924842834473
[2025-12-23 15:05:58,946] m-LoRA: Adapter lora_gsm8k_80 loss: 2.5503129959106445
[2025-12-23 15:05:58,952] m-LoRA: Adapter lora_gsm8k_78 epoch: 1/1 iteration: 80/128 step: 11
[2025-12-23 15:05:59,242] m-LoRA: Adapter lora_gsm8k_72 loss: 0.5218585729598999
[2025-12-23 15:05:59,245] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 34/128 step: 18
[2025-12-23 15:05:59,375] m-LoRA: Adapter lora_gsm8k_79 epoch: 1/1 iteration: 8/128 step: 2
[2025-12-23 15:06:00,406] m-LoRA: Adapter lora_gsm8k_78 loss: 0.6988363265991211
[2025-12-23 15:06:00,411] m-LoRA: Adapter lora_gsm8k_80 epoch: 1/1 iteration: 8/128 step: 2
[2025-12-23 15:06:00,608] m-LoRA: Adapter lora_gsm8k_76 loss: 0.6400884389877319
[2025-12-23 15:06:00,610] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 86/128 step: 44
[2025-12-23 15:06:01,093] m-LoRA: Adapter lora_gsm8k_79 loss: 1.954001784324646
[2025-12-23 15:06:01,909] m-LoRA: Adapter lora_gsm8k_80 loss: 1.7941157817840576
[2025-12-23 15:06:01,914] m-LoRA: Adapter lora_gsm8k_78 epoch: 1/1 iteration: 88/128 step: 12
[2025-12-23 15:06:02,099] m-LoRA: Adapter lora_gsm8k_72 loss: 0.6248083114624023
[2025-12-23 15:06:02,101] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 36/128 step: 19
[2025-12-23 15:06:02,885] m-LoRA: Adapter lora_gsm8k_79 epoch: 1/1 iteration: 16/128 step: 3
[2025-12-23 15:06:02,963] m-LoRA: Adapter lora_gsm8k_78 loss: 0.5793601274490356
[2025-12-23 15:06:03,113] m-LoRA: Adapter lora_gsm8k_76 loss: 0.7104663848876953
[2025-12-23 15:06:03,204] m-LoRA: Adapter lora_gsm8k_80 epoch: 1/1 iteration: 16/128 step: 3
[2025-12-23 15:06:03,327] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 88/128 step: 45
[2025-12-23 15:06:04,581] m-LoRA: Adapter lora_gsm8k_78 epoch: 1/1 iteration: 96/128 step: 13
[2025-12-23 15:06:04,671] m-LoRA: Adapter lora_gsm8k_79 loss: 1.5069469213485718
[2025-12-23 15:06:05,307] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 38/128 step: 20
[2025-12-23 15:06:05,366] m-LoRA: Adapter lora_gsm8k_80 loss: 1.281293272972107
[2025-12-23 15:06:05,520] m-LoRA: Adapter lora_gsm8k_72 loss: 0.7370412349700928
[2025-12-23 15:06:06,275] m-LoRA: Adapter lora_gsm8k_78 loss: 0.598022997379303
[2025-12-23 15:06:06,282] m-LoRA: Adapter lora_gsm8k_79 epoch: 1/1 iteration: 24/128 step: 4
[2025-12-23 15:06:06,335] m-LoRA: Adapter lora_gsm8k_76 loss: 0.7378714680671692
[2025-12-23 15:06:06,997] m-LoRA: Adapter lora_gsm8k_80 epoch: 1/1 iteration: 24/128 step: 4
[2025-12-23 15:06:07,195] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 90/128 step: 46
[2025-12-23 15:06:08,012] m-LoRA: Adapter lora_gsm8k_78 epoch: 1/1 iteration: 104/128 step: 14
[2025-12-23 15:06:08,113] m-LoRA: Adapter lora_gsm8k_79 loss: 1.4865812063217163
[2025-12-23 15:06:08,720] m-LoRA: Adapter lora_gsm8k_80 loss: 1.3164280652999878
[2025-12-23 15:06:08,727] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 40/128 step: 21
[2025-12-23 15:06:08,768] m-LoRA: Adapter lora_gsm8k_72 loss: 0.48229044675827026
[2025-12-23 15:06:09,944] m-LoRA: Adapter lora_gsm8k_79 epoch: 1/1 iteration: 32/128 step: 5
[2025-12-23 15:06:10,009] m-LoRA: Adapter lora_gsm8k_78 loss: 0.6329535841941833
[2025-12-23 15:06:10,018] m-LoRA: Adapter lora_gsm8k_76 loss: 0.6378366351127625
[2025-12-23 15:06:10,250] m-LoRA: Adapter lora_gsm8k_80 epoch: 1/1 iteration: 32/128 step: 5
[2025-12-23 15:06:10,371] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 92/128 step: 47
[2025-12-23 15:06:11,056] m-LoRA: Adapter lora_gsm8k_79 loss: 1.4581493139266968
[2025-12-23 15:06:11,882] m-LoRA: Adapter lora_gsm8k_78 epoch: 1/1 iteration: 112/128 step: 15
[2025-12-23 15:06:12,039] m-LoRA: Adapter lora_gsm8k_80 loss: 1.4157949686050415
[2025-12-23 15:06:12,045] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 42/128 step: 22
[2025-12-23 15:06:12,089] m-LoRA: Adapter lora_gsm8k_72 loss: 0.6241652965545654
[2025-12-23 15:06:12,295] m-LoRA: Adapter lora_gsm8k_79 epoch: 1/1 iteration: 40/128 step: 6
[2025-12-23 15:06:13,493] m-LoRA: Adapter lora_gsm8k_78 loss: 0.3996378481388092
[2025-12-23 15:06:13,663] m-LoRA: Adapter lora_gsm8k_76 loss: 0.5820116996765137
[2025-12-23 15:06:13,665] m-LoRA: Adapter lora_gsm8k_80 epoch: 1/1 iteration: 40/128 step: 6
[2025-12-23 15:06:14,269] m-LoRA: Adapter lora_gsm8k_79 loss: 1.3504718542099
[2025-12-23 15:06:14,276] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 94/128 step: 48
[2025-12-23 15:06:15,498] m-LoRA: Adapter lora_gsm8k_80 loss: 1.44477117061615
[2025-12-23 15:06:15,504] m-LoRA: Adapter lora_gsm8k_78 epoch: 1/1 iteration: 120/128 step: 16
[2025-12-23 15:06:15,739] m-LoRA: Adapter lora_gsm8k_72 loss: 0.519487202167511
[2025-12-23 15:06:15,741] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 44/128 step: 23
[2025-12-23 15:06:15,855] m-LoRA: Adapter lora_gsm8k_79 epoch: 1/1 iteration: 48/128 step: 7
[2025-12-23 15:06:17,360] m-LoRA: Adapter lora_gsm8k_78 loss: 0.44850361347198486
[2025-12-23 15:06:17,367] m-LoRA: Adapter lora_gsm8k_80 epoch: 1/1 iteration: 48/128 step: 7
[2025-12-23 15:06:17,523] m-LoRA: Adapter lora_gsm8k_76 loss: 0.6284133195877075
[2025-12-23 15:06:17,526] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 96/128 step: 49
[2025-12-23 15:06:18,060] m-LoRA: Adapter lora_gsm8k_79 loss: 1.315761685371399
[2025-12-23 15:06:19,236] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_78']
[2025-12-23 15:06:19,916] m-LoRA: Adapter lora_gsm8k_80 loss: 1.2466214895248413
[2025-12-23 15:06:19,922] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_81']
[2025-12-23 15:06:20,075] m-LoRA: Adapter lora_gsm8k_81 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 15:06:20,201] m-LoRA: Adapter lora_gsm8k_72 loss: 0.5849951505661011
[2025-12-23 15:06:20,203] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 46/128 step: 24
[2025-12-23 15:06:20,290] m-LoRA: Adapter lora_gsm8k_79 epoch: 1/1 iteration: 56/128 step: 8
[2025-12-23 15:06:21,564] m-LoRA: Adapter lora_gsm8k_81 loss: 2.30832576751709
[2025-12-23 15:06:21,570] m-LoRA: Adapter lora_gsm8k_80 epoch: 1/1 iteration: 56/128 step: 8
[2025-12-23 15:06:21,773] m-LoRA: Adapter lora_gsm8k_76 loss: 0.4023488163948059
[2025-12-23 15:06:21,776] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 98/128 step: 50
[2025-12-23 15:06:22,242] m-LoRA: Adapter lora_gsm8k_79 loss: 1.5134268999099731
[2025-12-23 15:06:23,442] m-LoRA: Adapter lora_gsm8k_80 loss: 0.9500541090965271
[2025-12-23 15:06:23,449] m-LoRA: Adapter lora_gsm8k_81 epoch: 1/1 iteration: 8/128 step: 2
[2025-12-23 15:06:23,643] m-LoRA: Adapter lora_gsm8k_72 loss: 0.6719961166381836
[2025-12-23 15:06:23,646] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 48/128 step: 25
[2025-12-23 15:06:23,751] m-LoRA: Adapter lora_gsm8k_79 epoch: 1/1 iteration: 64/128 step: 9
[2025-12-23 15:06:24,397] m-LoRA: Adapter lora_gsm8k_81 loss: 2.206040859222412
[2025-12-23 15:06:24,731] m-LoRA: Adapter lora_gsm8k_76 loss: 0.7588004469871521
[2025-12-23 15:06:24,814] m-LoRA: Adapter lora_gsm8k_80 epoch: 1/1 iteration: 64/128 step: 9
[2025-12-23 15:06:24,937] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 100/128 step: 51
[2025-12-23 15:06:25,712] m-LoRA: Adapter lora_gsm8k_81 epoch: 1/1 iteration: 16/128 step: 3
[2025-12-23 15:06:25,776] m-LoRA: Adapter lora_gsm8k_79 loss: 0.9908174872398376
[2025-12-23 15:06:26,259] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 50/128 step: 26
[2025-12-23 15:06:26,327] m-LoRA: Adapter lora_gsm8k_80 loss: 1.155822515487671
[2025-12-23 15:06:26,334] m-LoRA: Adapter lora_gsm8k_72 loss: 0.6911799311637878
[2025-12-23 15:06:26,957] m-LoRA: Adapter lora_gsm8k_81 loss: 1.6952213048934937
[2025-12-23 15:06:27,290] m-LoRA: Adapter lora_gsm8k_76 loss: 0.7187190055847168
[2025-12-23 15:06:27,520] m-LoRA: Adapter lora_gsm8k_79 epoch: 1/1 iteration: 72/128 step: 10
[2025-12-23 15:06:28,029] m-LoRA: Adapter lora_gsm8k_80 epoch: 1/1 iteration: 72/128 step: 10
[2025-12-23 15:06:28,155] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 102/128 step: 52
[2025-12-23 15:06:28,464] m-LoRA: Adapter lora_gsm8k_81 epoch: 1/1 iteration: 24/128 step: 4
[2025-12-23 15:06:29,075] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 52/128 step: 27
[2025-12-23 15:06:29,120] m-LoRA: Adapter lora_gsm8k_79 loss: 1.3272680044174194
[2025-12-23 15:06:29,701] m-LoRA: Adapter lora_gsm8k_80 loss: 0.9188786149024963
[2025-12-23 15:06:29,719] m-LoRA: Adapter lora_gsm8k_72 loss: 0.7393321990966797
[2025-12-23 15:06:30,540] m-LoRA: Adapter lora_gsm8k_81 loss: 1.3610283136367798
[2025-12-23 15:06:30,546] m-LoRA: Adapter lora_gsm8k_79 epoch: 1/1 iteration: 80/128 step: 11
[2025-12-23 15:06:30,610] m-LoRA: Adapter lora_gsm8k_76 loss: 0.6418773531913757
[2025-12-23 15:06:30,967] m-LoRA: Adapter lora_gsm8k_80 epoch: 1/1 iteration: 80/128 step: 11
[2025-12-23 15:06:31,095] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 104/128 step: 53
[2025-12-23 15:06:32,210] m-LoRA: Adapter lora_gsm8k_81 epoch: 1/1 iteration: 32/128 step: 5
[2025-12-23 15:06:32,298] m-LoRA: Adapter lora_gsm8k_79 loss: 1.2474589347839355
[2025-12-23 15:06:32,976] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 54/128 step: 28
[2025-12-23 15:06:33,093] m-LoRA: Adapter lora_gsm8k_80 loss: 0.6289811134338379
[2025-12-23 15:06:33,102] m-LoRA: Adapter lora_gsm8k_72 loss: 0.39145010709762573
[2025-12-23 15:06:33,856] m-LoRA: Adapter lora_gsm8k_79 epoch: 1/1 iteration: 88/128 step: 12
[2025-12-23 15:06:33,951] m-LoRA: Adapter lora_gsm8k_81 loss: 1.3696001768112183
[2025-12-23 15:06:33,959] m-LoRA: Adapter lora_gsm8k_76 loss: 0.4858147203922272
[2025-12-23 15:06:35,192] m-LoRA: Adapter lora_gsm8k_79 loss: 1.2728995084762573
[2025-12-23 15:06:35,198] m-LoRA: Adapter lora_gsm8k_80 epoch: 1/1 iteration: 88/128 step: 12
[2025-12-23 15:06:35,368] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 106/128 step: 54
[2025-12-23 15:06:35,609] m-LoRA: Adapter lora_gsm8k_81 epoch: 1/1 iteration: 40/128 step: 6
[2025-12-23 15:06:35,740] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 56/128 step: 29
[2025-12-23 15:06:36,985] m-LoRA: Adapter lora_gsm8k_79 epoch: 1/1 iteration: 96/128 step: 13
[2025-12-23 15:06:37,076] m-LoRA: Adapter lora_gsm8k_80 loss: 0.743354320526123
[2025-12-23 15:06:37,084] m-LoRA: Adapter lora_gsm8k_72 loss: 0.4232636094093323
[2025-12-23 15:06:38,006] m-LoRA: Adapter lora_gsm8k_81 loss: 1.0092337131500244
[2025-12-23 15:06:38,110] m-LoRA: Adapter lora_gsm8k_76 loss: 0.5411496758460999
[2025-12-23 15:06:38,789] m-LoRA: Adapter lora_gsm8k_80 epoch: 1/1 iteration: 96/128 step: 13
[2025-12-23 15:06:38,919] m-LoRA: Adapter lora_gsm8k_79 loss: 1.226218342781067
[2025-12-23 15:06:38,926] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 108/128 step: 55
[2025-12-23 15:06:40,164] m-LoRA: Adapter lora_gsm8k_81 epoch: 1/1 iteration: 48/128 step: 7
[2025-12-23 15:06:40,254] m-LoRA: Adapter lora_gsm8k_80 loss: 0.5379326343536377
[2025-12-23 15:06:40,464] m-LoRA: Adapter lora_gsm8k_72 loss: 0.7228150963783264
[2025-12-23 15:06:40,466] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 58/128 step: 30
[2025-12-23 15:06:40,572] m-LoRA: Adapter lora_gsm8k_79 epoch: 1/1 iteration: 104/128 step: 14
[2025-12-23 15:06:41,337] m-LoRA: Adapter lora_gsm8k_81 loss: 1.389478325843811
[2025-12-23 15:06:41,543] m-LoRA: Adapter lora_gsm8k_76 loss: 0.6621826887130737
[2025-12-23 15:06:42,161] m-LoRA: Adapter lora_gsm8k_79 loss: 1.3328871726989746
[2025-12-23 15:06:42,166] m-LoRA: Adapter lora_gsm8k_80 epoch: 1/1 iteration: 104/128 step: 14
[2025-12-23 15:06:42,328] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 110/128 step: 56
[2025-12-23 15:06:42,596] m-LoRA: Adapter lora_gsm8k_81 epoch: 1/1 iteration: 56/128 step: 8
[2025-12-23 15:06:42,716] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 60/128 step: 31
[2025-12-23 15:06:44,070] m-LoRA: Adapter lora_gsm8k_80 loss: 0.5409566164016724
[2025-12-23 15:06:44,079] m-LoRA: Adapter lora_gsm8k_79 epoch: 1/1 iteration: 112/128 step: 15
[2025-12-23 15:06:44,134] m-LoRA: Adapter lora_gsm8k_72 loss: 0.6276869177818298
[2025-12-23 15:06:44,844] m-LoRA: Adapter lora_gsm8k_81 loss: 1.3502230644226074
[2025-12-23 15:06:44,861] m-LoRA: Adapter lora_gsm8k_76 loss: 0.6202008724212646
[2025-12-23 15:06:45,794] m-LoRA: Adapter lora_gsm8k_79 loss: 0.8982503414154053
[2025-12-23 15:06:46,015] m-LoRA: Adapter lora_gsm8k_80 epoch: 1/1 iteration: 112/128 step: 15
[2025-12-23 15:06:46,150] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 112/128 step: 57
[2025-12-23 15:06:46,521] m-LoRA: Adapter lora_gsm8k_81 epoch: 1/1 iteration: 64/128 step: 9
[2025-12-23 15:06:46,637] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 62/128 step: 32
[2025-12-23 15:06:47,933] m-LoRA: Adapter lora_gsm8k_80 loss: 0.581412136554718
[2025-12-23 15:06:47,940] m-LoRA: Adapter lora_gsm8k_79 epoch: 1/1 iteration: 120/128 step: 16
[2025-12-23 15:06:48,057] m-LoRA: Adapter lora_gsm8k_72 loss: 0.5031628608703613
[2025-12-23 15:06:48,716] m-LoRA: Adapter lora_gsm8k_81 loss: 1.165439486503601
[2025-12-23 15:06:48,732] m-LoRA: Adapter lora_gsm8k_76 loss: 0.7732651829719543
[2025-12-23 15:06:50,053] m-LoRA: Adapter lora_gsm8k_79 loss: 1.0106879472732544
[2025-12-23 15:06:50,061] m-LoRA: Adapter lora_gsm8k_80 epoch: 1/1 iteration: 120/128 step: 16
[2025-12-23 15:06:50,278] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 114/128 step: 58
[2025-12-23 15:06:50,387] m-LoRA: Adapter lora_gsm8k_81 epoch: 1/1 iteration: 72/128 step: 10
[2025-12-23 15:06:50,503] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 64/128 step: 33
[2025-12-23 15:06:51,860] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_79']
[2025-12-23 15:06:52,229] m-LoRA: Adapter lora_gsm8k_80 loss: 0.5861467123031616
[2025-12-23 15:06:52,236] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_82']
[2025-12-23 15:06:52,482] m-LoRA: Adapter lora_gsm8k_82 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 15:06:52,510] m-LoRA: Adapter lora_gsm8k_72 loss: 0.6550040245056152
[2025-12-23 15:06:52,687] m-LoRA: Adapter lora_gsm8k_81 loss: 1.4317457675933838
[2025-12-23 15:06:52,705] m-LoRA: Adapter lora_gsm8k_76 loss: 0.5312806367874146
[2025-12-23 15:06:53,608] m-LoRA: Adapter lora_gsm8k_82 loss: 2.420999765396118
[2025-12-23 15:06:53,733] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_80']
[2025-12-23 15:06:54,048] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_83']
[2025-12-23 15:06:54,235] m-LoRA: Adapter lora_gsm8k_83 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 15:06:54,359] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 116/128 step: 59
[2025-12-23 15:06:54,537] m-LoRA: Adapter lora_gsm8k_81 epoch: 1/1 iteration: 80/128 step: 11
[2025-12-23 15:06:55,410] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 66/128 step: 34
[2025-12-23 15:06:55,541] m-LoRA: Adapter lora_gsm8k_83 loss: 2.5351815223693848
[2025-12-23 15:06:55,548] m-LoRA: Adapter lora_gsm8k_82 epoch: 1/1 iteration: 4/128 step: 2
[2025-12-23 15:06:55,580] m-LoRA: Adapter lora_gsm8k_72 loss: 0.6420086026191711
[2025-12-23 15:06:56,531] m-LoRA: Adapter lora_gsm8k_81 loss: 0.9711354970932007
[2025-12-23 15:06:56,708] m-LoRA: Adapter lora_gsm8k_76 loss: 0.6399903297424316
[2025-12-23 15:06:57,102] m-LoRA: Adapter lora_gsm8k_82 loss: 2.476562976837158
[2025-12-23 15:06:57,106] m-LoRA: Adapter lora_gsm8k_83 epoch: 1/1 iteration: 8/128 step: 2
[2025-12-23 15:06:57,246] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 118/128 step: 60
[2025-12-23 15:06:58,611] m-LoRA: Adapter lora_gsm8k_81 epoch: 1/1 iteration: 88/128 step: 12
[2025-12-23 15:06:58,766] m-LoRA: Adapter lora_gsm8k_83 loss: 1.2680357694625854
[2025-12-23 15:06:58,774] m-LoRA: Adapter lora_gsm8k_82 epoch: 1/1 iteration: 8/128 step: 3
[2025-12-23 15:06:58,956] m-LoRA: Adapter lora_gsm8k_72 loss: 0.6933992505073547
[2025-12-23 15:06:58,959] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 68/128 step: 35
[2025-12-23 15:06:59,605] m-LoRA: Adapter lora_gsm8k_81 loss: 1.3265513181686401
[2025-12-23 15:07:00,005] m-LoRA: Adapter lora_gsm8k_82 loss: 2.4079957008361816
[2025-12-23 15:07:00,142] m-LoRA: Adapter lora_gsm8k_76 loss: 0.560279905796051
[2025-12-23 15:07:00,379] m-LoRA: Adapter lora_gsm8k_83 epoch: 1/1 iteration: 16/128 step: 3
[2025-12-23 15:07:00,542] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 120/128 step: 61
[2025-12-23 15:07:00,876] m-LoRA: Adapter lora_gsm8k_81 epoch: 1/1 iteration: 96/128 step: 13
[2025-12-23 15:07:01,744] m-LoRA: Adapter lora_gsm8k_83 loss: 1.2556613683700562
[2025-12-23 15:07:01,751] m-LoRA: Adapter lora_gsm8k_82 epoch: 1/1 iteration: 12/128 step: 4
[2025-12-23 15:07:01,920] m-LoRA: Adapter lora_gsm8k_72 loss: 0.6403307914733887
[2025-12-23 15:07:01,922] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 70/128 step: 36
[2025-12-23 15:07:02,505] m-LoRA: Adapter lora_gsm8k_81 loss: 1.1342586278915405
[2025-12-23 15:07:02,764] m-LoRA: Adapter lora_gsm8k_82 loss: 2.352908134460449
[2025-12-23 15:07:02,994] m-LoRA: Adapter lora_gsm8k_76 loss: 0.5115448236465454
[2025-12-23 15:07:03,379] m-LoRA: Adapter lora_gsm8k_83 epoch: 1/1 iteration: 24/128 step: 4
[2025-12-23 15:07:03,529] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 122/128 step: 62
[2025-12-23 15:07:04,422] m-LoRA: Adapter lora_gsm8k_81 epoch: 1/1 iteration: 104/128 step: 14
[2025-12-23 15:07:04,558] m-LoRA: Adapter lora_gsm8k_83 loss: 1.4197715520858765
[2025-12-23 15:07:04,564] m-LoRA: Adapter lora_gsm8k_82 epoch: 1/1 iteration: 16/128 step: 5
[2025-12-23 15:07:04,709] m-LoRA: Adapter lora_gsm8k_72 loss: 0.610066831111908
[2025-12-23 15:07:04,712] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 72/128 step: 37
[2025-12-23 15:07:05,710] m-LoRA: Adapter lora_gsm8k_81 loss: 0.9319843053817749
[2025-12-23 15:07:05,999] m-LoRA: Adapter lora_gsm8k_82 loss: 1.9299557209014893
[2025-12-23 15:07:06,003] m-LoRA: Adapter lora_gsm8k_83 epoch: 1/1 iteration: 32/128 step: 5
[2025-12-23 15:07:06,174] m-LoRA: Adapter lora_gsm8k_76 loss: 0.6922877430915833
[2025-12-23 15:07:06,176] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 124/128 step: 63
[2025-12-23 15:07:07,816] m-LoRA: Adapter lora_gsm8k_83 loss: 0.9404519200325012
[2025-12-23 15:07:07,824] m-LoRA: Adapter lora_gsm8k_81 epoch: 1/1 iteration: 112/128 step: 15
[2025-12-23 15:07:08,059] m-LoRA: Adapter lora_gsm8k_72 loss: 0.6506643891334534
[2025-12-23 15:07:08,061] m-LoRA: Adapter lora_gsm8k_82 epoch: 1/1 iteration: 20/128 step: 6
[2025-12-23 15:07:08,141] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 74/128 step: 38
[2025-12-23 15:07:08,888] m-LoRA: Adapter lora_gsm8k_81 loss: 1.195899248123169
[2025-12-23 15:07:09,401] m-LoRA: Adapter lora_gsm8k_82 loss: 1.9783310890197754
[2025-12-23 15:07:09,606] m-LoRA: Adapter lora_gsm8k_76 loss: 0.5955326557159424
[2025-12-23 15:07:09,609] m-LoRA: Adapter lora_gsm8k_83 epoch: 1/1 iteration: 40/128 step: 6
[2025-12-23 15:07:09,771] m-LoRA: Adapter lora_gsm8k_72 epoch: 1/1 iteration: 126/128 step: 64
[2025-12-23 15:07:10,100] m-LoRA: Adapter lora_gsm8k_81 epoch: 1/1 iteration: 120/128 step: 16
[2025-12-23 15:07:10,703] m-LoRA: Adapter lora_gsm8k_82 epoch: 1/1 iteration: 24/128 step: 7
[2025-12-23 15:07:10,753] m-LoRA: Adapter lora_gsm8k_83 loss: 1.2081478834152222
[2025-12-23 15:07:10,928] m-LoRA: Adapter lora_gsm8k_72 loss: 0.5816479325294495
[2025-12-23 15:07:10,931] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 76/128 step: 39
[2025-12-23 15:07:11,521] m-LoRA: Adapter lora_gsm8k_81 loss: 1.0633459091186523
[2025-12-23 15:07:11,860] m-LoRA: Adapter lora_gsm8k_82 loss: 1.862993597984314
[2025-12-23 15:07:12,006] m-LoRA: Adapter lora_gsm8k_76 loss: 0.8621364235877991
[2025-12-23 15:07:12,130] m-LoRA: Adapter lora_gsm8k_83 epoch: 1/1 iteration: 48/128 step: 7
[2025-12-23 15:07:12,261] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_72']
[2025-12-23 15:07:12,826] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_84']
[2025-12-23 15:07:12,904] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 15:07:13,330] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_81']
[2025-12-23 15:07:13,479] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_85']
[2025-12-23 15:07:13,631] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 15:07:14,061] m-LoRA: Adapter lora_gsm8k_82 epoch: 1/1 iteration: 28/128 step: 8
[2025-12-23 15:07:14,112] m-LoRA: Adapter lora_gsm8k_83 loss: 0.9398413896560669
[2025-12-23 15:07:14,221] m-LoRA: Adapter lora_gsm8k_84 loss: 2.3225247859954834
[2025-12-23 15:07:14,224] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 78/128 step: 40
[2025-12-23 15:07:14,373] m-LoRA: Adapter lora_gsm8k_85 loss: 2.614488124847412
[2025-12-23 15:07:14,566] m-LoRA: Adapter lora_gsm8k_82 loss: 2.040895462036133
[2025-12-23 15:07:15,442] m-LoRA: Adapter lora_gsm8k_76 loss: 0.5176020264625549
[2025-12-23 15:07:15,707] m-LoRA: Adapter lora_gsm8k_83 epoch: 1/1 iteration: 56/128 step: 8
[2025-12-23 15:07:15,877] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 2/128 step: 2
[2025-12-23 15:07:15,979] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 2/128 step: 2
[2025-12-23 15:07:16,065] m-LoRA: Adapter lora_gsm8k_82 epoch: 1/1 iteration: 32/128 step: 9
[2025-12-23 15:07:16,123] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 80/128 step: 41
[2025-12-23 15:07:16,921] m-LoRA: Adapter lora_gsm8k_83 loss: 0.8808344006538391
[2025-12-23 15:07:16,938] m-LoRA: Adapter lora_gsm8k_84 loss: 2.309364080429077
[2025-12-23 15:07:17,160] m-LoRA: Adapter lora_gsm8k_85 loss: 1.4871017932891846
[2025-12-23 15:07:17,399] m-LoRA: Adapter lora_gsm8k_82 loss: 1.7661088705062866
[2025-12-23 15:07:17,521] m-LoRA: Adapter lora_gsm8k_76 loss: 0.8135131001472473
[2025-12-23 15:07:18,321] m-LoRA: Adapter lora_gsm8k_83 epoch: 1/1 iteration: 64/128 step: 9
[2025-12-23 15:07:18,454] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 4/128 step: 3
[2025-12-23 15:07:18,536] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 4/128 step: 3
[2025-12-23 15:07:18,628] m-LoRA: Adapter lora_gsm8k_82 epoch: 1/1 iteration: 36/128 step: 10
[2025-12-23 15:07:18,695] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 82/128 step: 42
[2025-12-23 15:07:19,575] m-LoRA: Adapter lora_gsm8k_83 loss: 0.731080174446106
[2025-12-23 15:07:19,627] m-LoRA: Adapter lora_gsm8k_84 loss: 2.071702003479004
[2025-12-23 15:07:19,723] m-LoRA: Adapter lora_gsm8k_85 loss: 2.017944812774658
[2025-12-23 15:07:20,109] m-LoRA: Adapter lora_gsm8k_82 loss: 1.5041602849960327
[2025-12-23 15:07:20,190] m-LoRA: Adapter lora_gsm8k_76 loss: 0.40295496582984924
[2025-12-23 15:07:21,003] m-LoRA: Adapter lora_gsm8k_83 epoch: 1/1 iteration: 72/128 step: 10
[2025-12-23 15:07:21,137] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 6/128 step: 4
[2025-12-23 15:07:21,218] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 6/128 step: 4
[2025-12-23 15:07:21,301] m-LoRA: Adapter lora_gsm8k_82 epoch: 1/1 iteration: 40/128 step: 11
[2025-12-23 15:07:21,381] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 84/128 step: 43
[2025-12-23 15:07:22,199] m-LoRA: Adapter lora_gsm8k_83 loss: 0.6789273023605347
[2025-12-23 15:07:22,214] m-LoRA: Adapter lora_gsm8k_84 loss: 2.6126577854156494
[2025-12-23 15:07:22,427] m-LoRA: Adapter lora_gsm8k_85 loss: 1.663976788520813
[2025-12-23 15:07:22,744] m-LoRA: Adapter lora_gsm8k_82 loss: 1.5467908382415771
[2025-12-23 15:07:22,852] m-LoRA: Adapter lora_gsm8k_76 loss: 0.5787496566772461
[2025-12-23 15:07:23,611] m-LoRA: Adapter lora_gsm8k_83 epoch: 1/1 iteration: 80/128 step: 11
[2025-12-23 15:07:23,741] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 8/128 step: 5
[2025-12-23 15:07:23,819] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 8/128 step: 5
[2025-12-23 15:07:23,911] m-LoRA: Adapter lora_gsm8k_82 epoch: 1/1 iteration: 44/128 step: 12
[2025-12-23 15:07:24,001] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 86/128 step: 44
[2025-12-23 15:07:24,800] m-LoRA: Adapter lora_gsm8k_83 loss: 0.6305146813392639
[2025-12-23 15:07:24,818] m-LoRA: Adapter lora_gsm8k_84 loss: 2.1522886753082275
[2025-12-23 15:07:25,007] m-LoRA: Adapter lora_gsm8k_85 loss: 1.4493646621704102
[2025-12-23 15:07:25,290] m-LoRA: Adapter lora_gsm8k_82 loss: 1.6765187978744507
[2025-12-23 15:07:25,425] m-LoRA: Adapter lora_gsm8k_76 loss: 0.4675372540950775
[2025-12-23 15:07:26,224] m-LoRA: Adapter lora_gsm8k_83 epoch: 1/1 iteration: 88/128 step: 12
[2025-12-23 15:07:26,364] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 10/128 step: 6
[2025-12-23 15:07:26,440] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 10/128 step: 6
[2025-12-23 15:07:26,528] m-LoRA: Adapter lora_gsm8k_82 epoch: 1/1 iteration: 48/128 step: 13
[2025-12-23 15:07:26,614] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 88/128 step: 45
[2025-12-23 15:07:27,538] m-LoRA: Adapter lora_gsm8k_83 loss: 0.5605005621910095
[2025-12-23 15:07:27,556] m-LoRA: Adapter lora_gsm8k_84 loss: 1.8726229667663574
[2025-12-23 15:07:27,790] m-LoRA: Adapter lora_gsm8k_85 loss: 1.2341387271881104
[2025-12-23 15:07:28,227] m-LoRA: Adapter lora_gsm8k_82 loss: 1.039910078048706
[2025-12-23 15:07:28,338] m-LoRA: Adapter lora_gsm8k_76 loss: 0.6340348124504089
[2025-12-23 15:07:29,135] m-LoRA: Adapter lora_gsm8k_83 epoch: 1/1 iteration: 96/128 step: 13
[2025-12-23 15:07:29,272] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 12/128 step: 7
[2025-12-23 15:07:29,361] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 12/128 step: 7
[2025-12-23 15:07:29,515] m-LoRA: Adapter lora_gsm8k_82 epoch: 1/1 iteration: 52/128 step: 14
[2025-12-23 15:07:29,617] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 90/128 step: 46
[2025-12-23 15:07:30,312] m-LoRA: Adapter lora_gsm8k_83 loss: 0.5057230591773987
[2025-12-23 15:07:30,328] m-LoRA: Adapter lora_gsm8k_84 loss: 2.4607629776000977
[2025-12-23 15:07:30,532] m-LoRA: Adapter lora_gsm8k_85 loss: 1.0420798063278198
[2025-12-23 15:07:30,845] m-LoRA: Adapter lora_gsm8k_82 loss: 1.5615402460098267
[2025-12-23 15:07:30,969] m-LoRA: Adapter lora_gsm8k_76 loss: 0.6382043361663818
[2025-12-23 15:07:31,669] m-LoRA: Adapter lora_gsm8k_83 epoch: 1/1 iteration: 104/128 step: 14
[2025-12-23 15:07:31,822] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 14/128 step: 8
[2025-12-23 15:07:31,920] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 14/128 step: 8
[2025-12-23 15:07:32,020] m-LoRA: Adapter lora_gsm8k_82 epoch: 1/1 iteration: 56/128 step: 15
[2025-12-23 15:07:32,106] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 92/128 step: 47
[2025-12-23 15:07:33,224] m-LoRA: Adapter lora_gsm8k_83 loss: 0.44078344106674194
[2025-12-23 15:07:33,243] m-LoRA: Adapter lora_gsm8k_84 loss: 1.7737812995910645
[2025-12-23 15:07:33,441] m-LoRA: Adapter lora_gsm8k_85 loss: 0.9815153479576111
[2025-12-23 15:07:33,746] m-LoRA: Adapter lora_gsm8k_82 loss: 1.5896791219711304
[2025-12-23 15:07:33,883] m-LoRA: Adapter lora_gsm8k_76 loss: 0.636782705783844
[2025-12-23 15:07:35,092] m-LoRA: Adapter lora_gsm8k_83 epoch: 1/1 iteration: 112/128 step: 15
[2025-12-23 15:07:35,270] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 16/128 step: 9
[2025-12-23 15:07:35,338] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 16/128 step: 9
[2025-12-23 15:07:35,412] m-LoRA: Adapter lora_gsm8k_82 epoch: 1/1 iteration: 60/128 step: 16
[2025-12-23 15:07:35,484] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 94/128 step: 48
[2025-12-23 15:07:36,243] m-LoRA: Adapter lora_gsm8k_83 loss: 0.6533523201942444
[2025-12-23 15:07:36,382] m-LoRA: Adapter lora_gsm8k_84 loss: 1.6123623847961426
[2025-12-23 15:07:36,513] m-LoRA: Adapter lora_gsm8k_85 loss: 0.7590639591217041
[2025-12-23 15:07:36,808] m-LoRA: Adapter lora_gsm8k_82 loss: 1.739309549331665
[2025-12-23 15:07:36,894] m-LoRA: Adapter lora_gsm8k_76 loss: 0.5454742908477783
[2025-12-23 15:07:37,528] m-LoRA: Adapter lora_gsm8k_83 epoch: 1/1 iteration: 120/128 step: 16
[2025-12-23 15:07:37,664] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 18/128 step: 10
[2025-12-23 15:07:37,747] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 18/128 step: 10
[2025-12-23 15:07:37,826] m-LoRA: Adapter lora_gsm8k_82 epoch: 1/1 iteration: 64/128 step: 17
[2025-12-23 15:07:37,914] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 96/128 step: 49
[2025-12-23 15:07:38,754] m-LoRA: Adapter lora_gsm8k_83 loss: 0.5574472546577454
[2025-12-23 15:07:38,776] m-LoRA: Adapter lora_gsm8k_84 loss: 1.5037425756454468
[2025-12-23 15:07:38,972] m-LoRA: Adapter lora_gsm8k_85 loss: 0.6930354237556458
[2025-12-23 15:07:39,231] m-LoRA: Adapter lora_gsm8k_82 loss: 1.6125684976577759
[2025-12-23 15:07:39,405] m-LoRA: Adapter lora_gsm8k_76 loss: 0.6273287534713745
[2025-12-23 15:07:40,144] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_83']
[2025-12-23 15:07:40,790] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_86']
[2025-12-23 15:07:40,831] m-LoRA: Adapter lora_gsm8k_86 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 15:07:41,016] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 20/128 step: 11
[2025-12-23 15:07:41,085] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 20/128 step: 11
[2025-12-23 15:07:41,173] m-LoRA: Adapter lora_gsm8k_82 epoch: 1/1 iteration: 68/128 step: 18
[2025-12-23 15:07:41,246] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 98/128 step: 50
[2025-12-23 15:07:43,381] m-LoRA: Adapter lora_gsm8k_86 loss: 2.2951202392578125
[2025-12-23 15:07:43,399] m-LoRA: Adapter lora_gsm8k_84 loss: 1.6088671684265137
[2025-12-23 15:07:43,426] m-LoRA: Adapter lora_gsm8k_85 loss: 0.7390168905258179
[2025-12-23 15:07:43,811] m-LoRA: Adapter lora_gsm8k_82 loss: 1.4734861850738525
[2025-12-23 15:07:43,923] m-LoRA: Adapter lora_gsm8k_76 loss: 0.6730640530586243
[2025-12-23 15:07:46,322] m-LoRA: Adapter lora_gsm8k_86 epoch: 1/1 iteration: 16/128 step: 2
[2025-12-23 15:07:46,554] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 22/128 step: 12
[2025-12-23 15:07:46,620] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 22/128 step: 12
[2025-12-23 15:07:46,690] m-LoRA: Adapter lora_gsm8k_82 epoch: 1/1 iteration: 72/128 step: 19
[2025-12-23 15:07:46,780] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 100/128 step: 51
[2025-12-23 15:07:48,610] m-LoRA: Adapter lora_gsm8k_86 loss: 1.8350563049316406
[2025-12-23 15:07:48,631] m-LoRA: Adapter lora_gsm8k_84 loss: 1.5285264253616333
[2025-12-23 15:07:48,651] m-LoRA: Adapter lora_gsm8k_85 loss: 0.6583760976791382
[2025-12-23 15:07:49,029] m-LoRA: Adapter lora_gsm8k_82 loss: 1.3404921293258667
[2025-12-23 15:07:49,136] m-LoRA: Adapter lora_gsm8k_76 loss: 0.7771958112716675
[2025-12-23 15:07:51,114] m-LoRA: Adapter lora_gsm8k_86 epoch: 1/1 iteration: 32/128 step: 3
[2025-12-23 15:07:51,346] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 24/128 step: 13
[2025-12-23 15:07:51,446] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 24/128 step: 13
[2025-12-23 15:07:51,521] m-LoRA: Adapter lora_gsm8k_82 epoch: 1/1 iteration: 76/128 step: 20
[2025-12-23 15:07:51,604] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 102/128 step: 52
[2025-12-23 15:07:53,513] m-LoRA: Adapter lora_gsm8k_86 loss: 1.3354426622390747
[2025-12-23 15:07:53,536] m-LoRA: Adapter lora_gsm8k_84 loss: 1.3863502740859985
[2025-12-23 15:07:53,804] m-LoRA: Adapter lora_gsm8k_85 loss: 0.5182257294654846
[2025-12-23 15:07:53,968] m-LoRA: Adapter lora_gsm8k_82 loss: 1.8032863140106201
[2025-12-23 15:07:54,103] m-LoRA: Adapter lora_gsm8k_76 loss: 0.6066752076148987
[2025-12-23 15:07:56,199] m-LoRA: Adapter lora_gsm8k_86 epoch: 1/1 iteration: 48/128 step: 4
[2025-12-23 15:07:56,431] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 26/128 step: 14
[2025-12-23 15:07:56,523] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 26/128 step: 14
[2025-12-23 15:07:56,609] m-LoRA: Adapter lora_gsm8k_82 epoch: 1/1 iteration: 80/128 step: 21
[2025-12-23 15:07:56,698] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 104/128 step: 53
[2025-12-23 15:07:58,531] m-LoRA: Adapter lora_gsm8k_86 loss: 1.25522780418396
[2025-12-23 15:07:58,557] m-LoRA: Adapter lora_gsm8k_84 loss: 1.5703048706054688
[2025-12-23 15:07:58,583] m-LoRA: Adapter lora_gsm8k_85 loss: 0.8142980933189392
[2025-12-23 15:07:58,963] m-LoRA: Adapter lora_gsm8k_82 loss: 1.3309094905853271
[2025-12-23 15:07:59,086] m-LoRA: Adapter lora_gsm8k_76 loss: 0.540941059589386
[2025-12-23 15:08:01,179] m-LoRA: Adapter lora_gsm8k_86 epoch: 1/1 iteration: 64/128 step: 5
[2025-12-23 15:08:01,402] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 28/128 step: 15
[2025-12-23 15:08:01,489] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 28/128 step: 15
[2025-12-23 15:08:01,582] m-LoRA: Adapter lora_gsm8k_82 epoch: 1/1 iteration: 84/128 step: 22
[2025-12-23 15:08:01,681] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 106/128 step: 54
[2025-12-23 15:08:03,317] m-LoRA: Adapter lora_gsm8k_86 loss: 1.4290083646774292
[2025-12-23 15:08:03,347] m-LoRA: Adapter lora_gsm8k_84 loss: 1.9207921028137207
[2025-12-23 15:08:03,391] m-LoRA: Adapter lora_gsm8k_85 loss: 0.5996121764183044
[2025-12-23 15:08:03,864] m-LoRA: Adapter lora_gsm8k_82 loss: 1.1835212707519531
[2025-12-23 15:08:03,958] m-LoRA: Adapter lora_gsm8k_76 loss: 0.5298748016357422
[2025-12-23 15:08:05,662] m-LoRA: Adapter lora_gsm8k_86 epoch: 1/1 iteration: 80/128 step: 6
[2025-12-23 15:08:05,877] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 30/128 step: 16
[2025-12-23 15:08:05,965] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 30/128 step: 16
[2025-12-23 15:08:06,038] m-LoRA: Adapter lora_gsm8k_82 epoch: 1/1 iteration: 88/128 step: 23
[2025-12-23 15:08:06,129] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 108/128 step: 55
[2025-12-23 15:08:06,394] m-LoRA: Adapter lora_gsm8k_84 loss: 1.847020149230957
[2025-12-23 15:08:06,712] m-LoRA: Adapter lora_gsm8k_85 loss: 0.5502820014953613
[2025-12-23 15:08:08,466] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 32/128 step: 17
[2025-12-23 15:08:08,749] m-LoRA: Adapter lora_gsm8k_86 loss: 1.004752516746521
[2025-12-23 15:08:08,764] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 32/128 step: 17
[2025-12-23 15:08:08,811] m-LoRA: Adapter lora_gsm8k_82 loss: 1.4761147499084473
[2025-12-23 15:08:08,815] m-LoRA: Adapter lora_gsm8k_76 loss: 0.6297316551208496
[2025-12-23 15:08:09,091] m-LoRA: Adapter lora_gsm8k_84 loss: 1.4930294752120972
[2025-12-23 15:08:09,179] m-LoRA: Adapter lora_gsm8k_85 loss: 0.6985591053962708
[2025-12-23 15:08:12,039] m-LoRA: Adapter lora_gsm8k_86 epoch: 1/1 iteration: 96/128 step: 7
[2025-12-23 15:08:12,301] m-LoRA: Adapter lora_gsm8k_82 epoch: 1/1 iteration: 92/128 step: 24
[2025-12-23 15:08:12,403] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 110/128 step: 56
[2025-12-23 15:08:12,494] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 34/128 step: 18
[2025-12-23 15:08:12,568] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 34/128 step: 18
[2025-12-23 15:08:14,531] m-LoRA: Adapter lora_gsm8k_86 loss: 1.1601706743240356
[2025-12-23 15:08:14,572] m-LoRA: Adapter lora_gsm8k_82 loss: 1.3825149536132812
[2025-12-23 15:08:14,851] m-LoRA: Adapter lora_gsm8k_76 loss: 0.5845944881439209
[2025-12-23 15:08:14,855] m-LoRA: Adapter lora_gsm8k_84 loss: 2.163576126098633
[2025-12-23 15:08:15,078] m-LoRA: Adapter lora_gsm8k_85 loss: 0.6995850205421448
[2025-12-23 15:08:17,337] m-LoRA: Adapter lora_gsm8k_86 epoch: 1/1 iteration: 112/128 step: 8
[2025-12-23 15:08:17,544] m-LoRA: Adapter lora_gsm8k_82 epoch: 1/1 iteration: 96/128 step: 25
[2025-12-23 15:08:17,630] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 112/128 step: 57
[2025-12-23 15:08:17,713] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 36/128 step: 19
[2025-12-23 15:08:17,784] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 36/128 step: 19
[2025-12-23 15:08:19,726] m-LoRA: Adapter lora_gsm8k_86 loss: 1.098503589630127
[2025-12-23 15:08:19,757] m-LoRA: Adapter lora_gsm8k_82 loss: 1.5176196098327637
[2025-12-23 15:08:20,040] m-LoRA: Adapter lora_gsm8k_76 loss: 0.6981357336044312
[2025-12-23 15:08:20,234] m-LoRA: Adapter lora_gsm8k_84 loss: 1.3361921310424805
[2025-12-23 15:08:20,359] m-LoRA: Adapter lora_gsm8k_85 loss: 0.795380711555481
[2025-12-23 15:08:22,436] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_86']
[2025-12-23 15:08:22,685] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_87']
[2025-12-23 15:08:22,756] m-LoRA: Adapter lora_gsm8k_87 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 15:08:22,853] m-LoRA: Adapter lora_gsm8k_82 epoch: 1/1 iteration: 100/128 step: 26
[2025-12-23 15:08:22,937] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 114/128 step: 58
[2025-12-23 15:08:22,997] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 38/128 step: 20
[2025-12-23 15:08:23,070] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 38/128 step: 20
[2025-12-23 15:08:24,003] m-LoRA: Adapter lora_gsm8k_87 loss: 2.2918970584869385
[2025-12-23 15:08:24,182] m-LoRA: Adapter lora_gsm8k_82 loss: 1.411421775817871
[2025-12-23 15:08:24,354] m-LoRA: Adapter lora_gsm8k_76 loss: 0.5718722343444824
[2025-12-23 15:08:24,456] m-LoRA: Adapter lora_gsm8k_84 loss: 1.8800835609436035
[2025-12-23 15:08:24,602] m-LoRA: Adapter lora_gsm8k_85 loss: 0.5482926964759827
[2025-12-23 15:08:25,507] m-LoRA: Adapter lora_gsm8k_87 epoch: 1/1 iteration: 8/128 step: 2
[2025-12-23 15:08:25,608] m-LoRA: Adapter lora_gsm8k_82 epoch: 1/1 iteration: 104/128 step: 27
[2025-12-23 15:08:25,688] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 116/128 step: 59
[2025-12-23 15:08:25,766] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 40/128 step: 21
[2025-12-23 15:08:25,849] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 40/128 step: 21
[2025-12-23 15:08:26,633] m-LoRA: Adapter lora_gsm8k_87 loss: 1.5574522018432617
[2025-12-23 15:08:26,958] m-LoRA: Adapter lora_gsm8k_82 loss: 1.2532230615615845
[2025-12-23 15:08:27,080] m-LoRA: Adapter lora_gsm8k_76 loss: 0.6278102397918701
[2025-12-23 15:08:27,178] m-LoRA: Adapter lora_gsm8k_84 loss: 1.8055328130722046
[2025-12-23 15:08:27,336] m-LoRA: Adapter lora_gsm8k_85 loss: 0.5957373976707458
[2025-12-23 15:08:27,961] m-LoRA: Adapter lora_gsm8k_87 epoch: 1/1 iteration: 16/128 step: 3
[2025-12-23 15:08:28,145] m-LoRA: Adapter lora_gsm8k_82 epoch: 1/1 iteration: 108/128 step: 28
[2025-12-23 15:08:28,250] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 118/128 step: 60
[2025-12-23 15:08:28,346] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 42/128 step: 22
[2025-12-23 15:08:28,438] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 42/128 step: 22
[2025-12-23 15:08:29,157] m-LoRA: Adapter lora_gsm8k_87 loss: 1.4394229650497437
[2025-12-23 15:08:29,465] m-LoRA: Adapter lora_gsm8k_82 loss: 1.2706422805786133
[2025-12-23 15:08:29,557] m-LoRA: Adapter lora_gsm8k_76 loss: 0.5123792290687561
[2025-12-23 15:08:29,706] m-LoRA: Adapter lora_gsm8k_84 loss: 1.6126773357391357
[2025-12-23 15:08:29,856] m-LoRA: Adapter lora_gsm8k_85 loss: 0.7093945741653442
[2025-12-23 15:08:30,481] m-LoRA: Adapter lora_gsm8k_87 epoch: 1/1 iteration: 24/128 step: 4
[2025-12-23 15:08:30,603] m-LoRA: Adapter lora_gsm8k_82 epoch: 1/1 iteration: 112/128 step: 29
[2025-12-23 15:08:30,693] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 120/128 step: 61
[2025-12-23 15:08:30,780] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 44/128 step: 23
[2025-12-23 15:08:30,898] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 44/128 step: 23
[2025-12-23 15:08:32,042] m-LoRA: Adapter lora_gsm8k_87 loss: 0.9630163311958313
[2025-12-23 15:08:32,311] m-LoRA: Adapter lora_gsm8k_82 loss: 1.3076714277267456
[2025-12-23 15:08:32,389] m-LoRA: Adapter lora_gsm8k_76 loss: 0.5819342136383057
[2025-12-23 15:08:32,497] m-LoRA: Adapter lora_gsm8k_84 loss: 1.7914135456085205
[2025-12-23 15:08:32,610] m-LoRA: Adapter lora_gsm8k_85 loss: 0.5958879590034485
[2025-12-23 15:08:33,829] m-LoRA: Adapter lora_gsm8k_87 epoch: 1/1 iteration: 32/128 step: 5
[2025-12-23 15:08:33,994] m-LoRA: Adapter lora_gsm8k_82 epoch: 1/1 iteration: 116/128 step: 30
[2025-12-23 15:08:34,076] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 122/128 step: 62
[2025-12-23 15:08:34,149] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 46/128 step: 24
[2025-12-23 15:08:34,220] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 46/128 step: 24
[2025-12-23 15:08:34,997] m-LoRA: Adapter lora_gsm8k_87 loss: 1.2322118282318115
[2025-12-23 15:08:35,298] m-LoRA: Adapter lora_gsm8k_82 loss: 1.271405816078186
[2025-12-23 15:08:35,389] m-LoRA: Adapter lora_gsm8k_76 loss: 0.5281176567077637
[2025-12-23 15:08:35,547] m-LoRA: Adapter lora_gsm8k_84 loss: 1.6476176977157593
[2025-12-23 15:08:35,655] m-LoRA: Adapter lora_gsm8k_85 loss: 0.6494084000587463
[2025-12-23 15:08:36,314] m-LoRA: Adapter lora_gsm8k_87 epoch: 1/1 iteration: 40/128 step: 6
[2025-12-23 15:08:36,442] m-LoRA: Adapter lora_gsm8k_82 epoch: 1/1 iteration: 120/128 step: 31
[2025-12-23 15:08:36,534] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 124/128 step: 63
[2025-12-23 15:08:36,609] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 48/128 step: 25
[2025-12-23 15:08:36,703] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 48/128 step: 25
[2025-12-23 15:08:37,527] m-LoRA: Adapter lora_gsm8k_87 loss: 1.1093236207962036
[2025-12-23 15:08:37,777] m-LoRA: Adapter lora_gsm8k_82 loss: 1.4167624711990356
[2025-12-23 15:08:37,912] m-LoRA: Adapter lora_gsm8k_76 loss: 0.6554818153381348
[2025-12-23 15:08:38,079] m-LoRA: Adapter lora_gsm8k_84 loss: 1.5565681457519531
[2025-12-23 15:08:38,230] m-LoRA: Adapter lora_gsm8k_85 loss: 0.6034184098243713
[2025-12-23 15:08:38,908] m-LoRA: Adapter lora_gsm8k_87 epoch: 1/1 iteration: 48/128 step: 7
[2025-12-23 15:08:39,032] m-LoRA: Adapter lora_gsm8k_82 epoch: 1/1 iteration: 124/128 step: 32
[2025-12-23 15:08:39,125] m-LoRA: Adapter lora_gsm8k_76 epoch: 1/1 iteration: 126/128 step: 64
[2025-12-23 15:08:39,200] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 50/128 step: 26
[2025-12-23 15:08:39,276] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 50/128 step: 26
[2025-12-23 15:08:40,097] m-LoRA: Adapter lora_gsm8k_87 loss: 1.1899245977401733
[2025-12-23 15:08:40,383] m-LoRA: Adapter lora_gsm8k_82 loss: 1.2533397674560547
[2025-12-23 15:08:40,447] m-LoRA: Adapter lora_gsm8k_76 loss: 0.7757541537284851
[2025-12-23 15:08:40,601] m-LoRA: Adapter lora_gsm8k_84 loss: 1.61121666431427
[2025-12-23 15:08:40,721] m-LoRA: Adapter lora_gsm8k_85 loss: 0.5394980907440186
[2025-12-23 15:08:41,435] m-LoRA: Adapter lora_gsm8k_87 epoch: 1/1 iteration: 56/128 step: 8
[2025-12-23 15:08:41,535] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_82']
[2025-12-23 15:08:41,829] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_88']
[2025-12-23 15:08:41,905] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 15:08:41,997] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_76']
[2025-12-23 15:08:42,575] m-LoRA: Task to running, need to load adapters: ['lora_gsm8k_89']
[2025-12-23 15:08:42,844] m-LoRA: Adapter lora_gsm8k_89 epoch: 1/1 iteration: 0/128 step: 1
[2025-12-23 15:08:43,014] m-LoRA: Adapter lora_gsm8k_87 loss: 1.1766726970672607
[2025-12-23 15:08:43,021] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 52/128 step: 27
[2025-12-23 15:08:43,197] m-LoRA: Adapter lora_gsm8k_88 loss: 2.2861342430114746
[2025-12-23 15:08:43,199] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 52/128 step: 27
[2025-12-23 15:08:44,242] m-LoRA: Adapter lora_gsm8k_87 epoch: 1/1 iteration: 64/128 step: 9
[2025-12-23 15:08:44,368] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 2/128 step: 2
[2025-12-23 15:08:45,537] m-LoRA: Adapter lora_gsm8k_89 loss: 2.4260189533233643
[2025-12-23 15:08:45,550] m-LoRA: Adapter lora_gsm8k_84 loss: 1.6451842784881592
[2025-12-23 15:08:45,771] m-LoRA: Adapter lora_gsm8k_85 loss: 0.5186945199966431
[2025-12-23 15:08:46,511] m-LoRA: Adapter lora_gsm8k_87 loss: 0.9047023057937622
[2025-12-23 15:08:46,524] m-LoRA: Adapter lora_gsm8k_88 loss: 2.3227641582489014
[2025-12-23 15:08:48,048] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 54/128 step: 28
[2025-12-23 15:08:48,196] m-LoRA: Adapter lora_gsm8k_89 epoch: 1/1 iteration: 16/128 step: 2
[2025-12-23 15:08:48,417] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 54/128 step: 28
[2025-12-23 15:08:48,614] m-LoRA: Adapter lora_gsm8k_84 loss: 1.6054767370224
[2025-12-23 15:08:48,616] m-LoRA: Adapter lora_gsm8k_87 epoch: 1/1 iteration: 72/128 step: 10
[2025-12-23 15:08:48,755] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 4/128 step: 3
[2025-12-23 15:08:49,070] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 56/128 step: 29
[2025-12-23 15:08:50,420] m-LoRA: Adapter lora_gsm8k_89 loss: 1.4732645750045776
[2025-12-23 15:08:50,437] m-LoRA: Adapter lora_gsm8k_85 loss: 0.6105168461799622
[2025-12-23 15:08:51,149] m-LoRA: Adapter lora_gsm8k_87 loss: 1.1277798414230347
[2025-12-23 15:08:51,166] m-LoRA: Adapter lora_gsm8k_88 loss: 2.224971055984497
[2025-12-23 15:08:51,365] m-LoRA: Adapter lora_gsm8k_84 loss: 1.337817668914795
[2025-12-23 15:08:52,974] m-LoRA: Adapter lora_gsm8k_89 epoch: 1/1 iteration: 32/128 step: 3
[2025-12-23 15:08:53,253] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 56/128 step: 29
[2025-12-23 15:08:53,347] m-LoRA: Adapter lora_gsm8k_87 epoch: 1/1 iteration: 80/128 step: 11
[2025-12-23 15:08:53,463] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 6/128 step: 4
[2025-12-23 15:08:53,540] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 58/128 step: 30
[2025-12-23 15:08:55,996] m-LoRA: Adapter lora_gsm8k_89 loss: 0.9665597677230835
[2025-12-23 15:08:56,013] m-LoRA: Adapter lora_gsm8k_85 loss: 0.7030746936798096
[2025-12-23 15:08:56,588] m-LoRA: Adapter lora_gsm8k_87 loss: 0.9279821515083313
[2025-12-23 15:08:56,609] m-LoRA: Adapter lora_gsm8k_88 loss: 1.8550055027008057
[2025-12-23 15:08:56,777] m-LoRA: Adapter lora_gsm8k_84 loss: 1.7140154838562012
[2025-12-23 15:08:59,445] m-LoRA: Adapter lora_gsm8k_89 epoch: 1/1 iteration: 48/128 step: 4
[2025-12-23 15:08:59,731] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 58/128 step: 30
[2025-12-23 15:08:59,833] m-LoRA: Adapter lora_gsm8k_87 epoch: 1/1 iteration: 88/128 step: 12
[2025-12-23 15:08:59,986] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 8/128 step: 5
[2025-12-23 15:09:00,089] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 60/128 step: 31
[2025-12-23 15:09:01,825] m-LoRA: Adapter lora_gsm8k_89 loss: 1.1772915124893188
[2025-12-23 15:09:01,844] m-LoRA: Adapter lora_gsm8k_85 loss: 0.6691365838050842
[2025-12-23 15:09:02,748] m-LoRA: Adapter lora_gsm8k_87 loss: 0.7012775540351868
[2025-12-23 15:09:02,767] m-LoRA: Adapter lora_gsm8k_88 loss: 1.7832542657852173
[2025-12-23 15:09:02,920] m-LoRA: Adapter lora_gsm8k_84 loss: 1.4865636825561523
[2025-12-23 15:09:04,559] m-LoRA: Adapter lora_gsm8k_89 epoch: 1/1 iteration: 64/128 step: 5
[2025-12-23 15:09:04,823] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 60/128 step: 31
[2025-12-23 15:09:04,940] m-LoRA: Adapter lora_gsm8k_87 epoch: 1/1 iteration: 96/128 step: 13
[2025-12-23 15:09:05,135] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 10/128 step: 6
[2025-12-23 15:09:05,257] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 62/128 step: 32
[2025-12-23 15:09:07,084] m-LoRA: Adapter lora_gsm8k_89 loss: 1.179007649421692
[2025-12-23 15:09:07,101] m-LoRA: Adapter lora_gsm8k_85 loss: 0.6120874285697937
[2025-12-23 15:09:07,945] m-LoRA: Adapter lora_gsm8k_87 loss: 0.8135672807693481
[2025-12-23 15:09:07,957] m-LoRA: Adapter lora_gsm8k_88 loss: 2.2026309967041016
[2025-12-23 15:09:08,118] m-LoRA: Adapter lora_gsm8k_84 loss: 1.482256531715393
[2025-12-23 15:09:09,989] m-LoRA: Adapter lora_gsm8k_89 epoch: 1/1 iteration: 80/128 step: 6
[2025-12-23 15:09:10,244] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 62/128 step: 32
[2025-12-23 15:09:10,342] m-LoRA: Adapter lora_gsm8k_87 epoch: 1/1 iteration: 104/128 step: 14
[2025-12-23 15:09:10,476] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 12/128 step: 7
[2025-12-23 15:09:10,559] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 64/128 step: 33
[2025-12-23 15:09:12,749] m-LoRA: Adapter lora_gsm8k_89 loss: 0.9940493106842041
[2025-12-23 15:09:12,771] m-LoRA: Adapter lora_gsm8k_85 loss: 0.6033687591552734
[2025-12-23 15:09:13,485] m-LoRA: Adapter lora_gsm8k_87 loss: 0.6731980443000793
[2025-12-23 15:09:13,500] m-LoRA: Adapter lora_gsm8k_88 loss: 1.9293301105499268
[2025-12-23 15:09:13,693] m-LoRA: Adapter lora_gsm8k_84 loss: 1.5341447591781616
[2025-12-23 15:09:15,915] m-LoRA: Adapter lora_gsm8k_89 epoch: 1/1 iteration: 96/128 step: 7
[2025-12-23 15:09:16,161] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 64/128 step: 33
[2025-12-23 15:09:16,230] m-LoRA: Adapter lora_gsm8k_87 epoch: 1/1 iteration: 112/128 step: 15
[2025-12-23 15:09:16,349] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 14/128 step: 8
[2025-12-23 15:09:16,423] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 66/128 step: 34
[2025-12-23 15:09:18,501] m-LoRA: Adapter lora_gsm8k_89 loss: 1.0459963083267212
[2025-12-23 15:09:18,521] m-LoRA: Adapter lora_gsm8k_85 loss: 0.5279647707939148
[2025-12-23 15:09:19,274] m-LoRA: Adapter lora_gsm8k_87 loss: 0.6131737232208252
[2025-12-23 15:09:19,292] m-LoRA: Adapter lora_gsm8k_88 loss: 1.8197935819625854
[2025-12-23 15:09:19,494] m-LoRA: Adapter lora_gsm8k_84 loss: 1.4356448650360107
[2025-12-23 15:09:21,475] m-LoRA: Adapter lora_gsm8k_89 epoch: 1/1 iteration: 112/128 step: 8
[2025-12-23 15:09:21,738] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 66/128 step: 34
[2025-12-23 15:09:21,825] m-LoRA: Adapter lora_gsm8k_87 epoch: 1/1 iteration: 120/128 step: 16
[2025-12-23 15:09:21,971] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 16/128 step: 9
[2025-12-23 15:09:22,050] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 68/128 step: 35
[2025-12-23 15:09:23,746] m-LoRA: Adapter lora_gsm8k_89 loss: 1.0882623195648193
[2025-12-23 15:09:23,765] m-LoRA: Adapter lora_gsm8k_85 loss: 0.5929593443870544
[2025-12-23 15:09:24,432] m-LoRA: Adapter lora_gsm8k_87 loss: 0.6355346441268921
[2025-12-23 15:09:24,445] m-LoRA: Adapter lora_gsm8k_88 loss: 2.1323862075805664
[2025-12-23 15:09:24,612] m-LoRA: Adapter lora_gsm8k_84 loss: 1.5893349647521973
[2025-12-23 15:09:26,425] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_89']
[2025-12-23 15:09:27,124] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 68/128 step: 35
[2025-12-23 15:09:27,188] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_87']
[2025-12-23 15:09:27,582] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 18/128 step: 10
[2025-12-23 15:09:27,942] m-LoRA: Adapter lora_gsm8k_85 loss: 0.6992311477661133
[2025-12-23 15:09:27,945] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 70/128 step: 36
[2025-12-23 15:09:28,066] m-LoRA: Adapter lora_gsm8k_88 loss: 1.5694851875305176
[2025-12-23 15:09:28,540] m-LoRA: Adapter lora_gsm8k_84 loss: 1.3615896701812744
[2025-12-23 15:09:28,542] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 70/128 step: 36
[2025-12-23 15:09:28,652] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 20/128 step: 11
[2025-12-23 15:09:29,000] m-LoRA: Adapter lora_gsm8k_85 loss: 0.5764075517654419
[2025-12-23 15:09:29,106] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 72/128 step: 37
[2025-12-23 15:09:29,177] m-LoRA: Adapter lora_gsm8k_88 loss: 1.6631264686584473
[2025-12-23 15:09:29,506] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 72/128 step: 37
[2025-12-23 15:09:29,735] m-LoRA: Adapter lora_gsm8k_84 loss: 1.0915136337280273
[2025-12-23 15:09:29,738] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 22/128 step: 12
[2025-12-23 15:09:29,947] m-LoRA: Adapter lora_gsm8k_85 loss: 0.5514793992042542
[2025-12-23 15:09:30,238] m-LoRA: Adapter lora_gsm8k_88 loss: 1.9528603553771973
[2025-12-23 15:09:30,349] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 74/128 step: 38
[2025-12-23 15:09:30,424] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 74/128 step: 38
[2025-12-23 15:09:30,622] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 24/128 step: 13
[2025-12-23 15:09:30,790] m-LoRA: Adapter lora_gsm8k_84 loss: 1.4798170328140259
[2025-12-23 15:09:30,921] m-LoRA: Adapter lora_gsm8k_85 loss: 0.4929674565792084
[2025-12-23 15:09:31,071] m-LoRA: Adapter lora_gsm8k_88 loss: 2.0361952781677246
[2025-12-23 15:09:31,363] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 76/128 step: 39
[2025-12-23 15:09:31,417] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 76/128 step: 39
[2025-12-23 15:09:31,513] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 26/128 step: 14
[2025-12-23 15:09:31,730] m-LoRA: Adapter lora_gsm8k_84 loss: 1.4457873106002808
[2025-12-23 15:09:31,917] m-LoRA: Adapter lora_gsm8k_85 loss: 0.8903408050537109
[2025-12-23 15:09:32,104] m-LoRA: Adapter lora_gsm8k_88 loss: 1.5034366846084595
[2025-12-23 15:09:32,204] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 78/128 step: 40
[2025-12-23 15:09:32,427] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 78/128 step: 40
[2025-12-23 15:09:32,572] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 28/128 step: 15
[2025-12-23 15:09:32,728] m-LoRA: Adapter lora_gsm8k_84 loss: 1.357121467590332
[2025-12-23 15:09:32,903] m-LoRA: Adapter lora_gsm8k_85 loss: 0.802064836025238
[2025-12-23 15:09:33,153] m-LoRA: Adapter lora_gsm8k_88 loss: 1.129957914352417
[2025-12-23 15:09:33,302] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 80/128 step: 41
[2025-12-23 15:09:33,405] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 80/128 step: 41
[2025-12-23 15:09:33,831] m-LoRA: Adapter lora_gsm8k_84 loss: 1.5037230253219604
[2025-12-23 15:09:33,834] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 30/128 step: 16
[2025-12-23 15:09:34,011] m-LoRA: Adapter lora_gsm8k_85 loss: 0.551605224609375
[2025-12-23 15:09:34,472] m-LoRA: Adapter lora_gsm8k_88 loss: 1.2863329648971558
[2025-12-23 15:09:34,476] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 82/128 step: 42
[2025-12-23 15:09:34,644] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 82/128 step: 42
[2025-12-23 15:09:35,041] m-LoRA: Adapter lora_gsm8k_84 loss: 1.8054593801498413
[2025-12-23 15:09:35,226] m-LoRA: Adapter lora_gsm8k_85 loss: 0.6417003870010376
[2025-12-23 15:09:35,228] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 32/128 step: 17
[2025-12-23 15:09:35,508] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 84/128 step: 43
[2025-12-23 15:09:35,679] m-LoRA: Adapter lora_gsm8k_88 loss: 1.8975125551223755
[2025-12-23 15:09:35,817] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 84/128 step: 43
[2025-12-23 15:09:35,883] m-LoRA: Adapter lora_gsm8k_84 loss: 1.654649019241333
[2025-12-23 15:09:36,204] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 34/128 step: 18
[2025-12-23 15:09:36,326] m-LoRA: Adapter lora_gsm8k_85 loss: 0.6458456516265869
[2025-12-23 15:09:36,467] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 86/128 step: 44
[2025-12-23 15:09:36,565] m-LoRA: Adapter lora_gsm8k_88 loss: 2.0981991291046143
[2025-12-23 15:09:36,944] m-LoRA: Adapter lora_gsm8k_84 loss: 1.505494236946106
[2025-12-23 15:09:36,946] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 86/128 step: 44
[2025-12-23 15:09:37,041] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 36/128 step: 19
[2025-12-23 15:09:37,481] m-LoRA: Adapter lora_gsm8k_85 loss: 0.5392720103263855
[2025-12-23 15:09:37,484] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 88/128 step: 45
[2025-12-23 15:09:37,604] m-LoRA: Adapter lora_gsm8k_88 loss: 2.01908278465271
[2025-12-23 15:09:38,005] m-LoRA: Adapter lora_gsm8k_84 loss: 1.4956597089767456
[2025-12-23 15:09:38,120] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 88/128 step: 45
[2025-12-23 15:09:38,213] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 38/128 step: 20
[2025-12-23 15:09:38,583] m-LoRA: Adapter lora_gsm8k_85 loss: 0.6124628782272339
[2025-12-23 15:09:38,586] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 90/128 step: 46
[2025-12-23 15:09:38,770] m-LoRA: Adapter lora_gsm8k_88 loss: 1.4989817142486572
[2025-12-23 15:09:39,198] m-LoRA: Adapter lora_gsm8k_84 loss: 1.125800371170044
[2025-12-23 15:09:39,201] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 90/128 step: 46
[2025-12-23 15:09:39,458] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 40/128 step: 21
[2025-12-23 15:09:39,733] m-LoRA: Adapter lora_gsm8k_85 loss: 0.5678054094314575
[2025-12-23 15:09:39,906] m-LoRA: Adapter lora_gsm8k_88 loss: 1.9763240814208984
[2025-12-23 15:09:39,908] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 92/128 step: 47
[2025-12-23 15:09:40,359] m-LoRA: Adapter lora_gsm8k_84 loss: 1.2831460237503052
[2025-12-23 15:09:40,362] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 92/128 step: 47
[2025-12-23 15:09:40,486] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 42/128 step: 22
[2025-12-23 15:09:40,793] m-LoRA: Adapter lora_gsm8k_85 loss: 0.5644856095314026
[2025-12-23 15:09:40,897] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 94/128 step: 48
[2025-12-23 15:09:40,999] m-LoRA: Adapter lora_gsm8k_88 loss: 1.440532922744751
[2025-12-23 15:09:41,264] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 94/128 step: 48
[2025-12-23 15:09:41,380] m-LoRA: Adapter lora_gsm8k_84 loss: 1.318608283996582
[2025-12-23 15:09:41,552] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 44/128 step: 23
[2025-12-23 15:09:41,707] m-LoRA: Adapter lora_gsm8k_85 loss: 0.6791157126426697
[2025-12-23 15:09:41,884] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 96/128 step: 49
[2025-12-23 15:09:41,944] m-LoRA: Adapter lora_gsm8k_88 loss: 1.723624348640442
[2025-12-23 15:09:42,240] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 96/128 step: 49
[2025-12-23 15:09:42,421] m-LoRA: Adapter lora_gsm8k_84 loss: 1.1640548706054688
[2025-12-23 15:09:42,423] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 46/128 step: 24
[2025-12-23 15:09:42,669] m-LoRA: Adapter lora_gsm8k_85 loss: 0.46657595038414
[2025-12-23 15:09:43,011] m-LoRA: Adapter lora_gsm8k_88 loss: 1.5162441730499268
[2025-12-23 15:09:43,015] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 98/128 step: 50
[2025-12-23 15:09:43,184] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 98/128 step: 50
[2025-12-23 15:09:43,571] m-LoRA: Adapter lora_gsm8k_84 loss: 1.2236013412475586
[2025-12-23 15:09:43,757] m-LoRA: Adapter lora_gsm8k_85 loss: 0.5592535138130188
[2025-12-23 15:09:43,759] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 48/128 step: 25
[2025-12-23 15:09:44,277] m-LoRA: Adapter lora_gsm8k_88 loss: 1.7788033485412598
[2025-12-23 15:09:44,279] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 100/128 step: 51
[2025-12-23 15:09:44,403] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 100/128 step: 51
[2025-12-23 15:09:44,739] m-LoRA: Adapter lora_gsm8k_84 loss: 1.0699535608291626
[2025-12-23 15:09:44,916] m-LoRA: Adapter lora_gsm8k_85 loss: 0.5302773714065552
[2025-12-23 15:09:44,918] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 50/128 step: 26
[2025-12-23 15:09:45,285] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 102/128 step: 52
[2025-12-23 15:09:45,433] m-LoRA: Adapter lora_gsm8k_88 loss: 1.3121808767318726
[2025-12-23 15:09:45,436] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 102/128 step: 52
[2025-12-23 15:09:45,771] m-LoRA: Adapter lora_gsm8k_84 loss: 1.1657346487045288
[2025-12-23 15:09:45,936] m-LoRA: Adapter lora_gsm8k_85 loss: 0.637434720993042
[2025-12-23 15:09:46,045] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 52/128 step: 27
[2025-12-23 15:09:46,259] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 104/128 step: 53
[2025-12-23 15:09:46,354] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 104/128 step: 53
[2025-12-23 15:09:46,462] m-LoRA: Adapter lora_gsm8k_88 loss: 1.6735996007919312
[2025-12-23 15:09:46,725] m-LoRA: Adapter lora_gsm8k_84 loss: 1.298682689666748
[2025-12-23 15:09:46,927] m-LoRA: Adapter lora_gsm8k_85 loss: 0.4155535399913788
[2025-12-23 15:09:46,930] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 54/128 step: 28
[2025-12-23 15:09:47,239] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 106/128 step: 54
[2025-12-23 15:09:47,346] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 106/128 step: 54
[2025-12-23 15:09:47,559] m-LoRA: Adapter lora_gsm8k_88 loss: 1.4479159116744995
[2025-12-23 15:09:47,681] m-LoRA: Adapter lora_gsm8k_84 loss: 1.2249408960342407
[2025-12-23 15:09:47,803] m-LoRA: Adapter lora_gsm8k_85 loss: 0.535447895526886
[2025-12-23 15:09:48,114] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 56/128 step: 29
[2025-12-23 15:09:48,219] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 108/128 step: 55
[2025-12-23 15:09:48,322] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 108/128 step: 55
[2025-12-23 15:09:48,577] m-LoRA: Adapter lora_gsm8k_88 loss: 1.326280951499939
[2025-12-23 15:09:48,726] m-LoRA: Adapter lora_gsm8k_84 loss: 1.0682315826416016
[2025-12-23 15:09:48,859] m-LoRA: Adapter lora_gsm8k_85 loss: 0.5555421710014343
[2025-12-23 15:09:49,096] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 58/128 step: 30
[2025-12-23 15:09:49,240] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 110/128 step: 56
[2025-12-23 15:09:49,334] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 110/128 step: 56
[2025-12-23 15:09:49,626] m-LoRA: Adapter lora_gsm8k_88 loss: 1.3899518251419067
[2025-12-23 15:09:49,910] m-LoRA: Adapter lora_gsm8k_84 loss: 0.9412537813186646
[2025-12-23 15:09:49,982] m-LoRA: Adapter lora_gsm8k_85 loss: 0.4308440387248993
[2025-12-23 15:09:50,140] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 60/128 step: 31
[2025-12-23 15:09:50,503] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 112/128 step: 57
[2025-12-23 15:09:50,581] m-LoRA: Adapter lora_gsm8k_88 loss: 1.8255772590637207
[2025-12-23 15:09:50,732] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 112/128 step: 57
[2025-12-23 15:09:50,919] m-LoRA: Adapter lora_gsm8k_84 loss: 1.0915799140930176
[2025-12-23 15:09:51,025] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 62/128 step: 32
[2025-12-23 15:09:51,174] m-LoRA: Adapter lora_gsm8k_85 loss: 0.6590413451194763
[2025-12-23 15:09:51,517] m-LoRA: Adapter lora_gsm8k_88 loss: 1.5488862991333008
[2025-12-23 15:09:51,520] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 114/128 step: 58
[2025-12-23 15:09:51,654] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 114/128 step: 58
[2025-12-23 15:09:51,936] m-LoRA: Adapter lora_gsm8k_84 loss: 1.106031060218811
[2025-12-23 15:09:52,098] m-LoRA: Adapter lora_gsm8k_85 loss: 0.6181360483169556
[2025-12-23 15:09:52,100] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 64/128 step: 33
[2025-12-23 15:09:52,322] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 116/128 step: 59
[2025-12-23 15:09:52,550] m-LoRA: Adapter lora_gsm8k_88 loss: 1.5154080390930176
[2025-12-23 15:09:52,551] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 116/128 step: 59
[2025-12-23 15:09:52,752] m-LoRA: Adapter lora_gsm8k_84 loss: 1.038429617881775
[2025-12-23 15:09:53,041] m-LoRA: Adapter lora_gsm8k_85 loss: 0.5411680340766907
[2025-12-23 15:09:53,044] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 66/128 step: 34
[2025-12-23 15:09:53,188] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 118/128 step: 60
[2025-12-23 15:09:53,570] m-LoRA: Adapter lora_gsm8k_88 loss: 1.338071584701538
[2025-12-23 15:09:53,573] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 118/128 step: 60
[2025-12-23 15:09:53,705] m-LoRA: Adapter lora_gsm8k_84 loss: 1.0312868356704712
[2025-12-23 15:09:54,216] m-LoRA: Adapter lora_gsm8k_85 loss: 0.6210941076278687
[2025-12-23 15:09:54,219] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 68/128 step: 35
[2025-12-23 15:09:54,377] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 120/128 step: 61
[2025-12-23 15:09:54,795] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 120/128 step: 61
[2025-12-23 15:09:54,832] m-LoRA: Adapter lora_gsm8k_88 loss: 1.5231657028198242
[2025-12-23 15:09:55,007] m-LoRA: Adapter lora_gsm8k_84 loss: 0.9703818559646606
[2025-12-23 15:09:55,460] m-LoRA: Adapter lora_gsm8k_85 loss: 0.4367009103298187
[2025-12-23 15:09:55,463] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 70/128 step: 36
[2025-12-23 15:09:55,617] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 122/128 step: 62
[2025-12-23 15:09:55,971] m-LoRA: Adapter lora_gsm8k_88 loss: 1.3699769973754883
[2025-12-23 15:09:56,180] m-LoRA: Adapter lora_gsm8k_84 loss: 0.7967444658279419
[2025-12-23 15:09:56,183] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 122/128 step: 62
[2025-12-23 15:09:56,433] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 72/128 step: 37
[2025-12-23 15:09:56,726] m-LoRA: Adapter lora_gsm8k_85 loss: 0.5501636862754822
[2025-12-23 15:09:56,730] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 124/128 step: 63
[2025-12-23 15:09:57,022] m-LoRA: Adapter lora_gsm8k_88 loss: 1.2869822978973389
[2025-12-23 15:09:57,390] m-LoRA: Adapter lora_gsm8k_84 loss: 0.8618919849395752
[2025-12-23 15:09:57,394] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 124/128 step: 63
[2025-12-23 15:09:57,579] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 74/128 step: 38
[2025-12-23 15:09:58,041] m-LoRA: Adapter lora_gsm8k_85 loss: 0.5970563292503357
[2025-12-23 15:09:58,044] m-LoRA: Adapter lora_gsm8k_84 epoch: 1/1 iteration: 126/128 step: 64
[2025-12-23 15:09:58,166] m-LoRA: Adapter lora_gsm8k_88 loss: 1.3253849744796753
[2025-12-23 15:09:58,608] m-LoRA: Adapter lora_gsm8k_84 loss: 0.9402452111244202
[2025-12-23 15:09:58,716] m-LoRA: Adapter lora_gsm8k_85 epoch: 1/1 iteration: 126/128 step: 64
[2025-12-23 15:09:58,796] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 76/128 step: 39
[2025-12-23 15:09:59,062] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_84']
[2025-12-23 15:09:59,382] m-LoRA: Adapter lora_gsm8k_85 loss: 0.5325537323951721
[2025-12-23 15:09:59,533] m-LoRA: Adapter lora_gsm8k_88 loss: 1.1341333389282227
[2025-12-23 15:10:00,066] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_85']
[2025-12-23 15:10:00,667] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 78/128 step: 40
[2025-12-23 15:10:01,800] m-LoRA: Adapter lora_gsm8k_88 loss: 1.3077549934387207
[2025-12-23 15:10:02,293] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 80/128 step: 41
[2025-12-23 15:10:02,697] m-LoRA: Adapter lora_gsm8k_88 loss: 1.3754065036773682
[2025-12-23 15:10:03,145] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 82/128 step: 42
[2025-12-23 15:10:03,585] m-LoRA: Adapter lora_gsm8k_88 loss: 1.1068644523620605
[2025-12-23 15:10:04,114] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 84/128 step: 43
[2025-12-23 15:10:04,553] m-LoRA: Adapter lora_gsm8k_88 loss: 1.3789467811584473
[2025-12-23 15:10:05,039] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 86/128 step: 44
[2025-12-23 15:10:05,498] m-LoRA: Adapter lora_gsm8k_88 loss: 1.1072595119476318
[2025-12-23 15:10:05,991] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 88/128 step: 45
[2025-12-23 15:10:06,382] m-LoRA: Adapter lora_gsm8k_88 loss: 1.3802669048309326
[2025-12-23 15:10:06,840] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 90/128 step: 46
[2025-12-23 15:10:07,260] m-LoRA: Adapter lora_gsm8k_88 loss: 1.2286573648452759
[2025-12-23 15:10:07,772] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 92/128 step: 47
[2025-12-23 15:10:08,202] m-LoRA: Adapter lora_gsm8k_88 loss: 1.264545202255249
[2025-12-23 15:10:08,704] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 94/128 step: 48
[2025-12-23 15:10:09,139] m-LoRA: Adapter lora_gsm8k_88 loss: 1.120815396308899
[2025-12-23 15:10:09,606] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 96/128 step: 49
[2025-12-23 15:10:10,038] m-LoRA: Adapter lora_gsm8k_88 loss: 1.0359400510787964
[2025-12-23 15:10:10,495] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 98/128 step: 50
[2025-12-23 15:10:10,866] m-LoRA: Adapter lora_gsm8k_88 loss: 1.2961257696151733
[2025-12-23 15:10:11,302] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 100/128 step: 51
[2025-12-23 15:10:11,765] m-LoRA: Adapter lora_gsm8k_88 loss: 1.148742914199829
[2025-12-23 15:10:12,298] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 102/128 step: 52
[2025-12-23 15:10:12,725] m-LoRA: Adapter lora_gsm8k_88 loss: 1.164220929145813
[2025-12-23 15:10:13,181] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 104/128 step: 53
[2025-12-23 15:10:13,553] m-LoRA: Adapter lora_gsm8k_88 loss: 1.2681658267974854
[2025-12-23 15:10:13,969] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 106/128 step: 54
[2025-12-23 15:10:14,413] m-LoRA: Adapter lora_gsm8k_88 loss: 0.9192482829093933
[2025-12-23 15:10:14,895] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 108/128 step: 55
[2025-12-23 15:10:15,306] m-LoRA: Adapter lora_gsm8k_88 loss: 1.1329432725906372
[2025-12-23 15:10:15,757] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 110/128 step: 56
[2025-12-23 15:10:16,200] m-LoRA: Adapter lora_gsm8k_88 loss: 0.9058536887168884
[2025-12-23 15:10:16,683] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 112/128 step: 57
[2025-12-23 15:10:17,181] m-LoRA: Adapter lora_gsm8k_88 loss: 0.7599747776985168
[2025-12-23 15:10:17,734] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 114/128 step: 58
[2025-12-23 15:10:18,114] m-LoRA: Adapter lora_gsm8k_88 loss: 1.1192214488983154
[2025-12-23 15:10:18,550] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 116/128 step: 59
[2025-12-23 15:10:18,965] m-LoRA: Adapter lora_gsm8k_88 loss: 1.0156601667404175
[2025-12-23 15:10:19,469] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 118/128 step: 60
[2025-12-23 15:10:19,893] m-LoRA: Adapter lora_gsm8k_88 loss: 0.8700568675994873
[2025-12-23 15:10:20,384] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 120/128 step: 61
[2025-12-23 15:10:20,850] m-LoRA: Adapter lora_gsm8k_88 loss: 0.7698204517364502
[2025-12-23 15:10:21,380] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 122/128 step: 62
[2025-12-23 15:10:21,817] m-LoRA: Adapter lora_gsm8k_88 loss: 0.7073426842689514
[2025-12-23 15:10:22,296] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 124/128 step: 63
[2025-12-23 15:10:22,740] m-LoRA: Adapter lora_gsm8k_88 loss: 0.9228525161743164
[2025-12-23 15:10:23,227] m-LoRA: Adapter lora_gsm8k_88 epoch: 1/1 iteration: 126/128 step: 64
[2025-12-23 15:10:23,618] m-LoRA: Adapter lora_gsm8k_88 loss: 0.7648189663887024
[2025-12-23 15:10:24,046] m-LoRA: Finish and base model offload adapter - ['lora_gsm8k_88']
